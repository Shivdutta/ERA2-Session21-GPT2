{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmQVqmwOYA9W",
        "outputId": "cfef78ad-9f12-40e0-9ad4-b123c1a239b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab_Notebooks/Session21\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab_Notebooks/Session21"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjp2M80qaS5w",
        "outputId": "2fbe6d95-97fe-4351-b323-6e3d9008b947"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-3 Paper\n",
        "# add cosing delay\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "TIm5m3fLaS-P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True) # Flash attention\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50304 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# STOP\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B*T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "# CHANGES IN CURRENT CODE\n",
        "torch.set_float32_matmul_precision('high')\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "# model = torch.compile(model)\n",
        "\n",
        "# CODE UPDATE HERE\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 5000\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <=1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "train_loader = DataLoaderLite(B = 8, T = 1024)\n",
        "\n",
        "import time\n",
        "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    # NEW CODE ADDED HERE\n",
        "    with torch.autocast(device_type='cpu', dtype=torch.bfloat16):\n",
        "        logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
        "    # NEW CODE\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = (t1 - t0) * 1000\n",
        "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
        "    print(f'step{step} | loss: {loss.item()} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec: .2f} | norm: {norm:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyqo7BUKbbBO",
        "outputId": "554a4b9e-f856-435a-99d0-196fd0101709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 41 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-529846064895>:292: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step0 | loss: 10.957502365112305 | dt: 3636.90ms | tok/sec:  2252.47 | norm: 7.38\n",
            "step1 | loss: 9.85181999206543 | dt: 2300.94ms | tok/sec:  3560.28 | norm: 4.93\n",
            "step2 | loss: 9.092519760131836 | dt: 2290.87ms | tok/sec:  3575.94 | norm: 5.98\n",
            "step3 | loss: 9.113883972167969 | dt: 2318.00ms | tok/sec:  3534.08 | norm: 4.87\n",
            "step4 | loss: 8.779541969299316 | dt: 2343.55ms | tok/sec:  3495.55 | norm: 3.79\n",
            "step5 | loss: 8.616558074951172 | dt: 2335.92ms | tok/sec:  3506.98 | norm: 2.09\n",
            "step6 | loss: 8.51376724243164 | dt: 2357.85ms | tok/sec:  3474.36 | norm: 2.25\n",
            "step7 | loss: 8.101021766662598 | dt: 2401.67ms | tok/sec:  3410.96 | norm: 2.59\n",
            "step8 | loss: 7.663261413574219 | dt: 2446.81ms | tok/sec:  3348.03 | norm: 2.63\n",
            "step9 | loss: 7.441658020019531 | dt: 2444.32ms | tok/sec:  3351.44 | norm: 2.16\n",
            "step10 | loss: 7.219490051269531 | dt: 2464.67ms | tok/sec:  3323.77 | norm: 1.61\n",
            "step11 | loss: 6.955009937286377 | dt: 2435.23ms | tok/sec:  3363.95 | norm: 1.25\n",
            "step12 | loss: 6.890677452087402 | dt: 2362.63ms | tok/sec:  3467.32 | norm: 1.77\n",
            "step13 | loss: 6.6646013259887695 | dt: 2395.56ms | tok/sec:  3419.66 | norm: 1.21\n",
            "step14 | loss: 6.54273796081543 | dt: 2335.78ms | tok/sec:  3507.18 | norm: 1.13\n",
            "step15 | loss: 6.339191436767578 | dt: 2352.52ms | tok/sec:  3482.23 | norm: 0.86\n",
            "step16 | loss: 6.440168380737305 | dt: 2340.56ms | tok/sec:  3500.02 | norm: 1.16\n",
            "step17 | loss: 6.506091117858887 | dt: 2315.77ms | tok/sec:  3537.48 | norm: 1.25\n",
            "step18 | loss: 6.4497785568237305 | dt: 2296.81ms | tok/sec:  3566.69 | norm: 1.25\n",
            "step19 | loss: 6.2405571937561035 | dt: 2284.18ms | tok/sec:  3586.41 | norm: 1.15\n",
            "step20 | loss: 6.335883617401123 | dt: 2283.05ms | tok/sec:  3588.19 | norm: 1.29\n",
            "step21 | loss: 6.188211917877197 | dt: 2270.67ms | tok/sec:  3607.75 | norm: 1.53\n",
            "step22 | loss: 6.281375408172607 | dt: 2258.23ms | tok/sec:  3627.62 | norm: 1.45\n",
            "step23 | loss: 6.114114284515381 | dt: 2249.02ms | tok/sec:  3642.48 | norm: 0.91\n",
            "step24 | loss: 6.104596138000488 | dt: 2234.13ms | tok/sec:  3666.76 | norm: 1.16\n",
            "step25 | loss: 6.135897159576416 | dt: 2241.49ms | tok/sec:  3654.71 | norm: 1.02\n",
            "step26 | loss: 6.510039329528809 | dt: 2224.00ms | tok/sec:  3683.45 | norm: 1.50\n",
            "step27 | loss: 6.3627800941467285 | dt: 2224.48ms | tok/sec:  3682.65 | norm: 2.16\n",
            "step28 | loss: 6.589535713195801 | dt: 2219.95ms | tok/sec:  3690.17 | norm: 1.11\n",
            "step29 | loss: 6.362138748168945 | dt: 2224.51ms | tok/sec:  3682.61 | norm: 2.20\n",
            "step30 | loss: 6.292539596557617 | dt: 2231.34ms | tok/sec:  3671.34 | norm: 1.82\n",
            "step31 | loss: 6.310004234313965 | dt: 2226.05ms | tok/sec:  3680.06 | norm: 1.30\n",
            "step32 | loss: 6.200471878051758 | dt: 2227.13ms | tok/sec:  3678.28 | norm: 1.36\n",
            "step33 | loss: 6.4246602058410645 | dt: 2230.68ms | tok/sec:  3672.43 | norm: 1.02\n",
            "step34 | loss: 6.283390998840332 | dt: 2243.82ms | tok/sec:  3650.92 | norm: 1.44\n",
            "step35 | loss: 6.167110919952393 | dt: 2258.14ms | tok/sec:  3627.76 | norm: 0.95\n",
            "step36 | loss: 6.219305992126465 | dt: 2257.83ms | tok/sec:  3628.26 | norm: 1.24\n",
            "step37 | loss: 6.227319240570068 | dt: 2275.64ms | tok/sec:  3599.86 | norm: 0.96\n",
            "step38 | loss: 5.954634189605713 | dt: 2287.86ms | tok/sec:  3580.64 | norm: 0.99\n",
            "step39 | loss: 5.947409152984619 | dt: 2295.33ms | tok/sec:  3568.98 | norm: 1.33\n",
            "step40 | loss: 6.338311195373535 | dt: 2300.91ms | tok/sec:  3560.33 | norm: 1.33\n",
            "step41 | loss: 6.2145609855651855 | dt: 2319.55ms | tok/sec:  3531.72 | norm: 1.44\n",
            "step42 | loss: 6.252432346343994 | dt: 2330.74ms | tok/sec:  3514.76 | norm: 1.47\n",
            "step43 | loss: 5.934846878051758 | dt: 2334.82ms | tok/sec:  3508.63 | norm: 1.41\n",
            "step44 | loss: 5.823863983154297 | dt: 2349.32ms | tok/sec:  3486.96 | norm: 1.11\n",
            "step45 | loss: 5.835160732269287 | dt: 2328.46ms | tok/sec:  3518.20 | norm: 1.03\n",
            "step46 | loss: 5.9648637771606445 | dt: 2325.55ms | tok/sec:  3522.61 | norm: 0.79\n",
            "step47 | loss: 6.064889430999756 | dt: 2324.08ms | tok/sec:  3524.83 | norm: 1.64\n",
            "step48 | loss: 5.918643951416016 | dt: 2330.03ms | tok/sec:  3515.83 | norm: 1.43\n",
            "step49 | loss: 5.771398544311523 | dt: 2326.08ms | tok/sec:  3521.81 | norm: 1.65\n",
            "step50 | loss: 5.973570823669434 | dt: 2326.50ms | tok/sec:  3521.16 | norm: 1.15\n",
            "step51 | loss: 6.047374725341797 | dt: 2329.57ms | tok/sec:  3516.53 | norm: 1.29\n",
            "step52 | loss: 6.126014709472656 | dt: 2315.82ms | tok/sec:  3537.41 | norm: 1.11\n",
            "step53 | loss: 6.197587490081787 | dt: 2305.94ms | tok/sec:  3552.57 | norm: 1.22\n",
            "step54 | loss: 6.042303562164307 | dt: 2315.90ms | tok/sec:  3537.28 | norm: 0.83\n",
            "step55 | loss: 6.0097246170043945 | dt: 2309.38ms | tok/sec:  3547.28 | norm: 1.08\n",
            "step56 | loss: 5.833901882171631 | dt: 2301.94ms | tok/sec:  3558.73 | norm: 1.10\n",
            "step57 | loss: 5.860937595367432 | dt: 2290.10ms | tok/sec:  3577.13 | norm: 1.06\n",
            "step58 | loss: 5.988315105438232 | dt: 2283.69ms | tok/sec:  3587.18 | norm: 1.44\n",
            "step59 | loss: 5.903310298919678 | dt: 2285.57ms | tok/sec:  3584.23 | norm: 1.20\n",
            "step60 | loss: 5.752288341522217 | dt: 2283.51ms | tok/sec:  3587.46 | norm: 1.21\n",
            "step61 | loss: 5.837345123291016 | dt: 2284.55ms | tok/sec:  3585.83 | norm: 0.96\n",
            "step62 | loss: 5.708353519439697 | dt: 2285.97ms | tok/sec:  3583.60 | norm: 1.20\n",
            "step63 | loss: 5.903595924377441 | dt: 2286.59ms | tok/sec:  3582.63 | norm: 0.99\n",
            "step64 | loss: 5.677953720092773 | dt: 2277.99ms | tok/sec:  3596.15 | norm: 0.88\n",
            "step65 | loss: 5.671082019805908 | dt: 2272.31ms | tok/sec:  3605.14 | norm: 0.87\n",
            "step66 | loss: 5.752283573150635 | dt: 2282.55ms | tok/sec:  3588.96 | norm: 1.21\n",
            "step67 | loss: 6.088681697845459 | dt: 2290.39ms | tok/sec:  3576.68 | norm: 1.19\n",
            "step68 | loss: 5.931972026824951 | dt: 2290.48ms | tok/sec:  3576.54 | norm: 1.18\n",
            "step69 | loss: 6.216938018798828 | dt: 2286.63ms | tok/sec:  3582.56 | norm: 1.07\n",
            "step70 | loss: 5.935596942901611 | dt: 2290.17ms | tok/sec:  3577.03 | norm: 1.07\n",
            "step71 | loss: 5.883135795593262 | dt: 2282.19ms | tok/sec:  3589.54 | norm: 1.00\n",
            "step72 | loss: 5.804743766784668 | dt: 2290.41ms | tok/sec:  3576.66 | norm: 1.30\n",
            "step73 | loss: 5.6665730476379395 | dt: 2290.71ms | tok/sec:  3576.18 | norm: 1.16\n",
            "step74 | loss: 5.878129482269287 | dt: 2309.51ms | tok/sec:  3547.08 | norm: 1.41\n",
            "step75 | loss: 5.751673221588135 | dt: 2304.04ms | tok/sec:  3555.49 | norm: 1.39\n",
            "step76 | loss: 5.740673065185547 | dt: 2295.18ms | tok/sec:  3569.23 | norm: 1.03\n",
            "step77 | loss: 5.694212913513184 | dt: 2306.30ms | tok/sec:  3552.01 | norm: 1.12\n",
            "step78 | loss: 5.720193862915039 | dt: 2307.17ms | tok/sec:  3550.68 | norm: 1.07\n",
            "step79 | loss: 5.471050262451172 | dt: 2311.51ms | tok/sec:  3544.00 | norm: 1.04\n",
            "step80 | loss: 5.407823085784912 | dt: 2310.46ms | tok/sec:  3545.62 | norm: 1.11\n",
            "step81 | loss: 5.936132907867432 | dt: 2303.16ms | tok/sec:  3556.85 | norm: 1.54\n",
            "step82 | loss: 5.873926162719727 | dt: 2305.71ms | tok/sec:  3552.92 | norm: 1.40\n",
            "step83 | loss: 5.933490753173828 | dt: 2310.92ms | tok/sec:  3544.91 | norm: 1.42\n",
            "step84 | loss: 5.620535373687744 | dt: 2307.14ms | tok/sec:  3550.71 | norm: 1.41\n",
            "step85 | loss: 5.497847557067871 | dt: 2305.11ms | tok/sec:  3553.85 | norm: 1.19\n",
            "step86 | loss: 5.544733047485352 | dt: 2303.57ms | tok/sec:  3556.23 | norm: 1.34\n",
            "step87 | loss: 5.666781425476074 | dt: 2310.70ms | tok/sec:  3545.25 | norm: 1.02\n",
            "step88 | loss: 5.708705425262451 | dt: 2310.50ms | tok/sec:  3545.56 | norm: 1.34\n",
            "step89 | loss: 5.61589241027832 | dt: 2308.23ms | tok/sec:  3549.04 | norm: 1.25\n",
            "step90 | loss: 5.400691986083984 | dt: 2296.95ms | tok/sec:  3566.46 | norm: 1.31\n",
            "step91 | loss: 5.605989456176758 | dt: 2311.21ms | tok/sec:  3544.46 | norm: 1.21\n",
            "step92 | loss: 5.6850762367248535 | dt: 2303.43ms | tok/sec:  3556.44 | norm: 1.30\n",
            "step93 | loss: 5.7979631423950195 | dt: 2305.45ms | tok/sec:  3553.32 | norm: 0.87\n",
            "step94 | loss: 5.866434574127197 | dt: 2306.74ms | tok/sec:  3551.33 | norm: 0.90\n",
            "step95 | loss: 5.724168300628662 | dt: 2300.54ms | tok/sec:  3560.90 | norm: 0.91\n",
            "step96 | loss: 5.699787616729736 | dt: 2307.39ms | tok/sec:  3550.33 | norm: 0.92\n",
            "step97 | loss: 5.5280442237854 | dt: 2310.65ms | tok/sec:  3545.33 | norm: 0.96\n",
            "step98 | loss: 5.577280044555664 | dt: 2315.37ms | tok/sec:  3538.10 | norm: 0.83\n",
            "step99 | loss: 5.699481010437012 | dt: 2297.93ms | tok/sec:  3564.94 | norm: 0.85\n",
            "step100 | loss: 5.592121124267578 | dt: 2305.44ms | tok/sec:  3553.34 | norm: 0.90\n",
            "step101 | loss: 5.436394214630127 | dt: 2308.76ms | tok/sec:  3548.22 | norm: 0.90\n",
            "step102 | loss: 5.5472564697265625 | dt: 2300.25ms | tok/sec:  3561.35 | norm: 0.79\n",
            "step103 | loss: 5.351930141448975 | dt: 2299.23ms | tok/sec:  3562.93 | norm: 0.85\n",
            "step104 | loss: 5.581873416900635 | dt: 2305.00ms | tok/sec:  3554.01 | norm: 0.84\n",
            "step105 | loss: 5.338691234588623 | dt: 2301.57ms | tok/sec:  3559.31 | norm: 0.87\n",
            "step106 | loss: 5.3244948387146 | dt: 2314.23ms | tok/sec:  3539.85 | norm: 0.80\n",
            "step107 | loss: 5.375544548034668 | dt: 2302.46ms | tok/sec:  3557.93 | norm: 0.82\n",
            "step108 | loss: 5.705663681030273 | dt: 2305.69ms | tok/sec:  3552.94 | norm: 1.28\n",
            "step109 | loss: 5.5850958824157715 | dt: 2307.96ms | tok/sec:  3549.45 | norm: 1.15\n",
            "step110 | loss: 5.908847808837891 | dt: 2308.45ms | tok/sec:  3548.69 | norm: 1.20\n",
            "step111 | loss: 5.573326110839844 | dt: 2308.77ms | tok/sec:  3548.20 | norm: 1.29\n",
            "step112 | loss: 5.53872537612915 | dt: 2305.01ms | tok/sec:  3554.00 | norm: 1.15\n",
            "step113 | loss: 5.450270175933838 | dt: 2303.29ms | tok/sec:  3556.66 | norm: 1.37\n",
            "step114 | loss: 5.350772380828857 | dt: 2307.57ms | tok/sec:  3550.06 | norm: 1.43\n",
            "step115 | loss: 5.522515773773193 | dt: 2294.53ms | tok/sec:  3570.23 | norm: 1.42\n",
            "step116 | loss: 5.359874248504639 | dt: 2301.05ms | tok/sec:  3560.11 | norm: 1.14\n",
            "step117 | loss: 5.423978328704834 | dt: 2305.09ms | tok/sec:  3553.87 | norm: 1.12\n",
            "step118 | loss: 5.34649133682251 | dt: 2307.96ms | tok/sec:  3549.45 | norm: 1.04\n",
            "step119 | loss: 5.335850715637207 | dt: 2303.78ms | tok/sec:  3555.89 | norm: 0.91\n",
            "step120 | loss: 5.123157024383545 | dt: 2303.62ms | tok/sec:  3556.14 | norm: 0.91\n",
            "step121 | loss: 5.035571098327637 | dt: 2293.67ms | tok/sec:  3571.57 | norm: 1.02\n",
            "step122 | loss: 5.674404144287109 | dt: 2302.81ms | tok/sec:  3557.40 | norm: 1.35\n",
            "step123 | loss: 5.570218563079834 | dt: 2300.86ms | tok/sec:  3560.41 | norm: 1.14\n",
            "step124 | loss: 5.6194329261779785 | dt: 2310.57ms | tok/sec:  3545.45 | norm: 1.29\n",
            "step125 | loss: 5.266046524047852 | dt: 2306.30ms | tok/sec:  3552.01 | norm: 1.40\n",
            "step126 | loss: 5.154872894287109 | dt: 2305.59ms | tok/sec:  3553.10 | norm: 0.98\n",
            "step127 | loss: 5.244863986968994 | dt: 2309.11ms | tok/sec:  3547.68 | norm: 0.89\n",
            "step128 | loss: 5.383363246917725 | dt: 2302.19ms | tok/sec:  3558.35 | norm: 0.88\n",
            "step129 | loss: 5.360595703125 | dt: 2310.08ms | tok/sec:  3546.20 | norm: 1.22\n",
            "step130 | loss: 5.306070327758789 | dt: 2309.21ms | tok/sec:  3547.53 | norm: 0.90\n",
            "step131 | loss: 5.070909023284912 | dt: 2307.40ms | tok/sec:  3550.31 | norm: 0.93\n",
            "step132 | loss: 5.287195682525635 | dt: 2312.09ms | tok/sec:  3543.11 | norm: 0.98\n",
            "step133 | loss: 5.297784328460693 | dt: 2307.11ms | tok/sec:  3550.76 | norm: 1.14\n",
            "step134 | loss: 5.492014408111572 | dt: 2311.88ms | tok/sec:  3543.44 | norm: 0.88\n",
            "step135 | loss: 5.573202610015869 | dt: 2313.45ms | tok/sec:  3541.03 | norm: 0.89\n",
            "step136 | loss: 5.414155960083008 | dt: 2301.58ms | tok/sec:  3559.30 | norm: 0.93\n",
            "step137 | loss: 5.393551349639893 | dt: 2313.20ms | tok/sec:  3541.41 | norm: 0.87\n",
            "step138 | loss: 5.193724632263184 | dt: 2306.92ms | tok/sec:  3551.05 | norm: 0.94\n",
            "step139 | loss: 5.285552978515625 | dt: 2317.25ms | tok/sec:  3535.22 | norm: 0.83\n",
            "step140 | loss: 5.391749382019043 | dt: 2307.51ms | tok/sec:  3550.14 | norm: 0.92\n",
            "step141 | loss: 5.323830604553223 | dt: 2315.02ms | tok/sec:  3538.63 | norm: 0.97\n",
            "step142 | loss: 5.158593654632568 | dt: 2303.96ms | tok/sec:  3555.62 | norm: 0.94\n",
            "step143 | loss: 5.285514831542969 | dt: 2307.12ms | tok/sec:  3550.75 | norm: 0.87\n",
            "step144 | loss: 5.094153881072998 | dt: 2318.50ms | tok/sec:  3533.32 | norm: 0.83\n",
            "step145 | loss: 5.336436748504639 | dt: 2309.28ms | tok/sec:  3547.42 | norm: 0.88\n",
            "step146 | loss: 5.087306022644043 | dt: 2316.56ms | tok/sec:  3536.28 | norm: 0.79\n",
            "step147 | loss: 5.106851100921631 | dt: 2296.21ms | tok/sec:  3567.62 | norm: 0.84\n",
            "step148 | loss: 5.125030517578125 | dt: 2305.40ms | tok/sec:  3553.39 | norm: 0.73\n",
            "step149 | loss: 5.504721164703369 | dt: 2301.16ms | tok/sec:  3559.94 | norm: 1.07\n",
            "step150 | loss: 5.385162830352783 | dt: 2310.80ms | tok/sec:  3545.09 | norm: 1.04\n",
            "step151 | loss: 5.732105731964111 | dt: 2309.94ms | tok/sec:  3546.41 | norm: 0.97\n",
            "step152 | loss: 5.371829032897949 | dt: 2305.80ms | tok/sec:  3552.78 | norm: 1.08\n",
            "step153 | loss: 5.379671096801758 | dt: 2302.28ms | tok/sec:  3558.22 | norm: 1.05\n",
            "step154 | loss: 5.195221424102783 | dt: 2314.81ms | tok/sec:  3538.95 | norm: 1.19\n",
            "step155 | loss: 5.13270378112793 | dt: 2303.10ms | tok/sec:  3556.95 | norm: 1.17\n",
            "step156 | loss: 5.300270080566406 | dt: 2307.70ms | tok/sec:  3549.85 | norm: 1.17\n",
            "step157 | loss: 5.104602336883545 | dt: 2300.12ms | tok/sec:  3561.55 | norm: 1.15\n",
            "step158 | loss: 5.193417072296143 | dt: 2305.12ms | tok/sec:  3553.83 | norm: 1.07\n",
            "step159 | loss: 5.120999336242676 | dt: 2293.85ms | tok/sec:  3571.28 | norm: 1.09\n",
            "step160 | loss: 5.126312255859375 | dt: 2307.66ms | tok/sec:  3549.92 | norm: 1.01\n",
            "step161 | loss: 4.9353814125061035 | dt: 2307.21ms | tok/sec:  3550.60 | norm: 1.02\n",
            "step162 | loss: 4.81960916519165 | dt: 2304.86ms | tok/sec:  3554.23 | norm: 0.89\n",
            "step163 | loss: 5.460670471191406 | dt: 2296.04ms | tok/sec:  3567.89 | norm: 1.48\n",
            "step164 | loss: 5.334466934204102 | dt: 2299.24ms | tok/sec:  3562.92 | norm: 1.17\n",
            "step165 | loss: 5.365325450897217 | dt: 2287.89ms | tok/sec:  3580.59 | norm: 1.03\n",
            "step166 | loss: 5.017235279083252 | dt: 2292.22ms | tok/sec:  3573.83 | norm: 1.22\n",
            "step167 | loss: 4.931551933288574 | dt: 2302.94ms | tok/sec:  3557.20 | norm: 1.06\n",
            "step168 | loss: 5.010552883148193 | dt: 2305.01ms | tok/sec:  3553.99 | norm: 1.01\n",
            "step169 | loss: 5.173160076141357 | dt: 2292.08ms | tok/sec:  3574.05 | norm: 0.88\n",
            "step170 | loss: 5.193162441253662 | dt: 2302.78ms | tok/sec:  3557.43 | norm: 1.38\n",
            "step171 | loss: 5.128174781799316 | dt: 2286.96ms | tok/sec:  3582.05 | norm: 1.02\n",
            "step172 | loss: 4.883674621582031 | dt: 2291.88ms | tok/sec:  3574.35 | norm: 1.07\n",
            "step173 | loss: 5.068671703338623 | dt: 2300.34ms | tok/sec:  3561.22 | norm: 0.92\n",
            "step174 | loss: 5.095041275024414 | dt: 2286.92ms | tok/sec:  3582.11 | norm: 1.14\n",
            "step175 | loss: 5.295699119567871 | dt: 2289.60ms | tok/sec:  3577.91 | norm: 0.94\n",
            "step176 | loss: 5.3808274269104 | dt: 2305.06ms | tok/sec:  3553.92 | norm: 0.85\n",
            "step177 | loss: 5.23065185546875 | dt: 2289.13ms | tok/sec:  3578.66 | norm: 0.86\n",
            "step178 | loss: 5.222563743591309 | dt: 2290.28ms | tok/sec:  3576.86 | norm: 0.87\n",
            "step179 | loss: 4.993645668029785 | dt: 2296.15ms | tok/sec:  3567.70 | norm: 0.92\n",
            "step180 | loss: 5.113619804382324 | dt: 2292.18ms | tok/sec:  3573.88 | norm: 0.82\n",
            "step181 | loss: 5.238149642944336 | dt: 2298.80ms | tok/sec:  3563.59 | norm: 0.89\n",
            "step182 | loss: 5.147368907928467 | dt: 2286.94ms | tok/sec:  3582.09 | norm: 0.90\n",
            "step183 | loss: 5.001948356628418 | dt: 2302.91ms | tok/sec:  3557.24 | norm: 0.94\n",
            "step184 | loss: 5.108400821685791 | dt: 2286.76ms | tok/sec:  3582.36 | norm: 0.89\n",
            "step185 | loss: 4.925244331359863 | dt: 2287.32ms | tok/sec:  3581.48 | norm: 0.89\n",
            "step186 | loss: 5.172433853149414 | dt: 2305.18ms | tok/sec:  3553.74 | norm: 0.80\n",
            "step187 | loss: 4.927345275878906 | dt: 2286.04ms | tok/sec:  3583.49 | norm: 0.86\n",
            "step188 | loss: 4.950865745544434 | dt: 2289.67ms | tok/sec:  3577.81 | norm: 0.93\n",
            "step189 | loss: 4.976717472076416 | dt: 2305.63ms | tok/sec:  3553.04 | norm: 0.98\n",
            "step190 | loss: 5.343790054321289 | dt: 2286.52ms | tok/sec:  3582.74 | norm: 1.20\n",
            "step191 | loss: 5.221378803253174 | dt: 2295.33ms | tok/sec:  3568.99 | norm: 1.09\n",
            "step192 | loss: 5.553328990936279 | dt: 2291.48ms | tok/sec:  3574.98 | norm: 1.03\n",
            "step193 | loss: 5.199703216552734 | dt: 2300.78ms | tok/sec:  3560.54 | norm: 0.96\n",
            "step194 | loss: 5.232889652252197 | dt: 2288.06ms | tok/sec:  3580.32 | norm: 0.89\n",
            "step195 | loss: 5.053099632263184 | dt: 2291.04ms | tok/sec:  3575.67 | norm: 1.27\n",
            "step196 | loss: 4.950209617614746 | dt: 2304.58ms | tok/sec:  3554.67 | norm: 1.28\n",
            "step197 | loss: 5.07380485534668 | dt: 2286.79ms | tok/sec:  3582.31 | norm: 1.03\n",
            "step198 | loss: 4.8946309089660645 | dt: 2297.69ms | tok/sec:  3565.31 | norm: 0.88\n",
            "step199 | loss: 5.01742696762085 | dt: 2295.41ms | tok/sec:  3568.87 | norm: 1.03\n",
            "step200 | loss: 4.973816394805908 | dt: 2294.74ms | tok/sec:  3569.90 | norm: 1.30\n",
            "step201 | loss: 4.969235897064209 | dt: 2290.19ms | tok/sec:  3576.99 | norm: 1.11\n",
            "step202 | loss: 4.817770004272461 | dt: 2293.03ms | tok/sec:  3572.57 | norm: 0.98\n",
            "step203 | loss: 4.677842617034912 | dt: 2290.90ms | tok/sec:  3575.89 | norm: 0.89\n",
            "step204 | loss: 5.34591817855835 | dt: 2298.88ms | tok/sec:  3563.48 | norm: 1.48\n",
            "step205 | loss: 5.221866130828857 | dt: 2290.01ms | tok/sec:  3577.27 | norm: 1.28\n",
            "step206 | loss: 5.224706172943115 | dt: 2301.19ms | tok/sec:  3559.89 | norm: 1.20\n",
            "step207 | loss: 4.880803108215332 | dt: 2286.56ms | tok/sec:  3582.67 | norm: 1.16\n",
            "step208 | loss: 4.80810022354126 | dt: 2286.54ms | tok/sec:  3582.70 | norm: 1.04\n",
            "step209 | loss: 4.890408515930176 | dt: 2287.02ms | tok/sec:  3581.95 | norm: 0.95\n",
            "step210 | loss: 5.04163122177124 | dt: 2295.56ms | tok/sec:  3568.62 | norm: 0.70\n",
            "step211 | loss: 5.046813011169434 | dt: 2294.43ms | tok/sec:  3570.39 | norm: 1.23\n",
            "step212 | loss: 4.946155071258545 | dt: 2307.83ms | tok/sec:  3549.65 | norm: 0.97\n",
            "step213 | loss: 4.720420837402344 | dt: 2287.74ms | tok/sec:  3580.82 | norm: 0.94\n",
            "step214 | loss: 4.898014545440674 | dt: 2286.09ms | tok/sec:  3583.41 | norm: 0.87\n",
            "step215 | loss: 4.872725009918213 | dt: 2290.63ms | tok/sec:  3576.32 | norm: 1.09\n",
            "step216 | loss: 5.122313976287842 | dt: 2293.69ms | tok/sec:  3571.54 | norm: 0.82\n",
            "step217 | loss: 5.218537330627441 | dt: 2298.31ms | tok/sec:  3564.35 | norm: 0.76\n",
            "step218 | loss: 5.0804266929626465 | dt: 2297.86ms | tok/sec:  3565.06 | norm: 0.76\n",
            "step219 | loss: 5.075785160064697 | dt: 2300.71ms | tok/sec:  3560.64 | norm: 0.80\n",
            "step220 | loss: 4.866796970367432 | dt: 2307.65ms | tok/sec:  3549.94 | norm: 0.95\n",
            "step221 | loss: 5.018865585327148 | dt: 2291.67ms | tok/sec:  3574.69 | norm: 0.90\n",
            "step222 | loss: 5.130330562591553 | dt: 2307.69ms | tok/sec:  3549.86 | norm: 0.87\n",
            "step223 | loss: 5.034974575042725 | dt: 2301.81ms | tok/sec:  3558.95 | norm: 0.88\n",
            "step224 | loss: 4.874654293060303 | dt: 2306.24ms | tok/sec:  3552.10 | norm: 0.87\n",
            "step225 | loss: 4.973630905151367 | dt: 2306.11ms | tok/sec:  3552.31 | norm: 0.88\n",
            "step226 | loss: 4.804208755493164 | dt: 2292.30ms | tok/sec:  3573.70 | norm: 0.94\n",
            "step227 | loss: 5.080491542816162 | dt: 2302.39ms | tok/sec:  3558.04 | norm: 1.06\n",
            "step228 | loss: 4.799396514892578 | dt: 2316.71ms | tok/sec:  3536.06 | norm: 0.90\n",
            "step229 | loss: 4.828204154968262 | dt: 2296.61ms | tok/sec:  3566.99 | norm: 0.86\n",
            "step230 | loss: 4.839694976806641 | dt: 2310.92ms | tok/sec:  3544.91 | norm: 0.93\n",
            "step231 | loss: 5.227551460266113 | dt: 2306.84ms | tok/sec:  3551.18 | norm: 1.18\n",
            "step232 | loss: 5.1378984451293945 | dt: 2305.23ms | tok/sec:  3553.66 | norm: 1.10\n",
            "step233 | loss: 5.411684989929199 | dt: 2303.82ms | tok/sec:  3555.83 | norm: 1.02\n",
            "step234 | loss: 5.063236236572266 | dt: 2308.79ms | tok/sec:  3548.18 | norm: 0.91\n",
            "step235 | loss: 5.122341632843018 | dt: 2301.02ms | tok/sec:  3560.16 | norm: 0.96\n",
            "step236 | loss: 4.930838108062744 | dt: 2306.10ms | tok/sec:  3552.32 | norm: 1.21\n",
            "step237 | loss: 4.849892616271973 | dt: 2306.13ms | tok/sec:  3552.27 | norm: 1.31\n",
            "step238 | loss: 4.9495744705200195 | dt: 2297.11ms | tok/sec:  3566.22 | norm: 1.15\n",
            "step239 | loss: 4.754099369049072 | dt: 2302.88ms | tok/sec:  3557.28 | norm: 0.89\n",
            "step240 | loss: 4.902632713317871 | dt: 2301.55ms | tok/sec:  3559.33 | norm: 0.84\n",
            "step241 | loss: 4.87207555770874 | dt: 2306.87ms | tok/sec:  3551.13 | norm: 1.10\n",
            "step242 | loss: 4.845419406890869 | dt: 2307.42ms | tok/sec:  3550.29 | norm: 1.09\n",
            "step243 | loss: 4.695091724395752 | dt: 2307.40ms | tok/sec:  3550.32 | norm: 0.99\n",
            "step244 | loss: 4.568676471710205 | dt: 2303.11ms | tok/sec:  3556.92 | norm: 0.86\n",
            "step245 | loss: 5.19868278503418 | dt: 2303.20ms | tok/sec:  3556.80 | norm: 1.18\n",
            "step246 | loss: 5.103373050689697 | dt: 2304.14ms | tok/sec:  3555.34 | norm: 1.01\n",
            "step247 | loss: 5.1138105392456055 | dt: 2307.05ms | tok/sec:  3550.86 | norm: 1.16\n",
            "step248 | loss: 4.753859043121338 | dt: 2310.53ms | tok/sec:  3545.51 | norm: 1.11\n",
            "step249 | loss: 4.678009986877441 | dt: 2308.37ms | tok/sec:  3548.83 | norm: 0.86\n",
            "step250 | loss: 4.764225482940674 | dt: 2310.14ms | tok/sec:  3546.10 | norm: 0.92\n",
            "step251 | loss: 4.931800842285156 | dt: 2309.04ms | tok/sec:  3547.79 | norm: 0.66\n",
            "step252 | loss: 4.968630313873291 | dt: 2307.17ms | tok/sec:  3550.66 | norm: 1.26\n",
            "step253 | loss: 4.879737854003906 | dt: 2309.82ms | tok/sec:  3546.60 | norm: 1.00\n",
            "step254 | loss: 4.643530368804932 | dt: 2308.90ms | tok/sec:  3548.01 | norm: 1.02\n",
            "step255 | loss: 4.821850299835205 | dt: 2308.26ms | tok/sec:  3548.99 | norm: 0.89\n",
            "step256 | loss: 4.762956142425537 | dt: 2311.91ms | tok/sec:  3543.40 | norm: 1.14\n",
            "step257 | loss: 5.026092529296875 | dt: 2303.66ms | tok/sec:  3556.08 | norm: 0.91\n",
            "step258 | loss: 5.094743728637695 | dt: 2305.64ms | tok/sec:  3553.03 | norm: 0.83\n",
            "step259 | loss: 4.9746270179748535 | dt: 2302.49ms | tok/sec:  3557.89 | norm: 0.74\n",
            "step260 | loss: 4.973072528839111 | dt: 2311.37ms | tok/sec:  3544.22 | norm: 0.79\n",
            "step261 | loss: 4.7693705558776855 | dt: 2300.85ms | tok/sec:  3560.43 | norm: 0.93\n",
            "step262 | loss: 4.933743476867676 | dt: 2305.69ms | tok/sec:  3552.96 | norm: 0.93\n",
            "step263 | loss: 5.049465656280518 | dt: 2306.72ms | tok/sec:  3551.36 | norm: 0.94\n",
            "step264 | loss: 4.95932674407959 | dt: 2303.45ms | tok/sec:  3556.40 | norm: 0.92\n",
            "step265 | loss: 4.800985336303711 | dt: 2306.65ms | tok/sec:  3551.48 | norm: 0.89\n",
            "step266 | loss: 4.908570766448975 | dt: 2308.00ms | tok/sec:  3549.40 | norm: 0.99\n",
            "step267 | loss: 4.768200874328613 | dt: 2308.19ms | tok/sec:  3549.10 | norm: 1.15\n",
            "step268 | loss: 5.033626079559326 | dt: 2301.54ms | tok/sec:  3559.35 | norm: 1.12\n",
            "step269 | loss: 4.72609281539917 | dt: 2307.26ms | tok/sec:  3550.54 | norm: 0.94\n",
            "step270 | loss: 4.7720232009887695 | dt: 2309.83ms | tok/sec:  3546.58 | norm: 0.90\n",
            "step271 | loss: 4.776083469390869 | dt: 2305.54ms | tok/sec:  3553.18 | norm: 0.98\n",
            "step272 | loss: 5.14692497253418 | dt: 2300.82ms | tok/sec:  3560.48 | norm: 1.26\n",
            "step273 | loss: 5.055416584014893 | dt: 2305.54ms | tok/sec:  3553.18 | norm: 1.14\n",
            "step274 | loss: 5.327505588531494 | dt: 2306.19ms | tok/sec:  3552.19 | norm: 1.12\n",
            "step275 | loss: 4.988885879516602 | dt: 2313.27ms | tok/sec:  3541.31 | norm: 0.99\n",
            "step276 | loss: 5.027335166931152 | dt: 2300.62ms | tok/sec:  3560.77 | norm: 0.87\n",
            "step277 | loss: 4.830191135406494 | dt: 2292.28ms | tok/sec:  3573.73 | norm: 1.12\n",
            "step278 | loss: 4.7660722732543945 | dt: 2301.44ms | tok/sec:  3559.52 | norm: 1.24\n",
            "step279 | loss: 4.88258171081543 | dt: 2288.96ms | tok/sec:  3578.91 | norm: 1.06\n",
            "step280 | loss: 4.6982903480529785 | dt: 2293.97ms | tok/sec:  3571.11 | norm: 0.98\n",
            "step281 | loss: 4.824255466461182 | dt: 2303.92ms | tok/sec:  3555.67 | norm: 0.86\n",
            "step282 | loss: 4.748541831970215 | dt: 2305.65ms | tok/sec:  3553.02 | norm: 0.93\n",
            "step283 | loss: 4.75164270401001 | dt: 2294.49ms | tok/sec:  3570.30 | norm: 0.98\n",
            "step284 | loss: 4.600333213806152 | dt: 2298.27ms | tok/sec:  3564.42 | norm: 1.09\n",
            "step285 | loss: 4.47504186630249 | dt: 2289.43ms | tok/sec:  3578.18 | norm: 1.06\n",
            "step286 | loss: 5.134123802185059 | dt: 2285.67ms | tok/sec:  3584.07 | norm: 1.12\n",
            "step287 | loss: 5.022860050201416 | dt: 2290.82ms | tok/sec:  3576.01 | norm: 0.91\n",
            "step288 | loss: 5.04923677444458 | dt: 2301.97ms | tok/sec:  3558.69 | norm: 1.14\n",
            "step289 | loss: 4.693843841552734 | dt: 2304.67ms | tok/sec:  3554.52 | norm: 1.27\n",
            "step290 | loss: 4.6197896003723145 | dt: 2290.67ms | tok/sec:  3576.25 | norm: 1.01\n",
            "step291 | loss: 4.701066493988037 | dt: 2293.19ms | tok/sec:  3572.32 | norm: 0.91\n",
            "step292 | loss: 4.857021331787109 | dt: 2298.09ms | tok/sec:  3564.71 | norm: 0.81\n",
            "step293 | loss: 4.803430080413818 | dt: 2297.22ms | tok/sec:  3566.05 | norm: 1.26\n",
            "step294 | loss: 4.7346086502075195 | dt: 2296.28ms | tok/sec:  3567.51 | norm: 0.94\n",
            "step295 | loss: 4.502872943878174 | dt: 2289.19ms | tok/sec:  3578.57 | norm: 0.99\n",
            "step296 | loss: 4.723245143890381 | dt: 2308.01ms | tok/sec:  3549.38 | norm: 0.92\n",
            "step297 | loss: 4.707447528839111 | dt: 2303.04ms | tok/sec:  3557.04 | norm: 1.24\n",
            "step298 | loss: 4.9691572189331055 | dt: 2291.15ms | tok/sec:  3575.50 | norm: 0.96\n",
            "step299 | loss: 5.038112163543701 | dt: 2301.52ms | tok/sec:  3559.39 | norm: 0.85\n",
            "step300 | loss: 4.933690071105957 | dt: 2304.18ms | tok/sec:  3555.28 | norm: 1.00\n",
            "step301 | loss: 4.92457914352417 | dt: 2300.05ms | tok/sec:  3561.66 | norm: 1.00\n",
            "step302 | loss: 4.6928205490112305 | dt: 2296.09ms | tok/sec:  3567.80 | norm: 0.95\n",
            "step303 | loss: 4.863173484802246 | dt: 2304.70ms | tok/sec:  3554.48 | norm: 0.94\n",
            "step304 | loss: 4.999427795410156 | dt: 2302.90ms | tok/sec:  3557.25 | norm: 1.04\n",
            "step305 | loss: 4.908647537231445 | dt: 2304.01ms | tok/sec:  3555.55 | norm: 1.02\n",
            "step306 | loss: 4.71389102935791 | dt: 2293.26ms | tok/sec:  3572.21 | norm: 0.81\n",
            "step307 | loss: 4.811988830566406 | dt: 2306.66ms | tok/sec:  3551.45 | norm: 0.71\n",
            "step308 | loss: 4.649221897125244 | dt: 2300.56ms | tok/sec:  3560.88 | norm: 0.81\n",
            "step309 | loss: 4.940822124481201 | dt: 2295.85ms | tok/sec:  3568.17 | norm: 0.97\n",
            "step310 | loss: 4.6372904777526855 | dt: 2291.78ms | tok/sec:  3574.51 | norm: 0.85\n",
            "step311 | loss: 4.66695499420166 | dt: 2303.93ms | tok/sec:  3555.66 | norm: 0.83\n",
            "step312 | loss: 4.667023181915283 | dt: 2287.47ms | tok/sec:  3581.25 | norm: 0.77\n",
            "step313 | loss: 5.027268409729004 | dt: 2288.93ms | tok/sec:  3578.96 | norm: 1.12\n",
            "step314 | loss: 4.953195571899414 | dt: 2289.12ms | tok/sec:  3578.66 | norm: 1.13\n",
            "step315 | loss: 5.232649326324463 | dt: 2287.12ms | tok/sec:  3581.80 | norm: 1.23\n",
            "step316 | loss: 4.872283458709717 | dt: 2283.51ms | tok/sec:  3587.46 | norm: 1.07\n",
            "step317 | loss: 4.938229560852051 | dt: 2292.47ms | tok/sec:  3573.44 | norm: 0.79\n",
            "step318 | loss: 4.737282752990723 | dt: 2304.21ms | tok/sec:  3555.23 | norm: 1.18\n",
            "step319 | loss: 4.71567964553833 | dt: 2285.91ms | tok/sec:  3583.69 | norm: 1.28\n",
            "step320 | loss: 4.764925003051758 | dt: 2286.94ms | tok/sec:  3582.09 | norm: 1.07\n",
            "step321 | loss: 4.61186408996582 | dt: 2288.80ms | tok/sec:  3579.17 | norm: 1.02\n",
            "step322 | loss: 4.768373489379883 | dt: 2305.77ms | tok/sec:  3552.82 | norm: 0.87\n",
            "step323 | loss: 4.70844030380249 | dt: 2299.07ms | tok/sec:  3563.18 | norm: 1.06\n",
            "step324 | loss: 4.668079376220703 | dt: 2291.27ms | tok/sec:  3575.31 | norm: 0.95\n",
            "step325 | loss: 4.542832851409912 | dt: 2300.42ms | tok/sec:  3561.10 | norm: 0.97\n",
            "step326 | loss: 4.401703834533691 | dt: 2288.21ms | tok/sec:  3580.09 | norm: 1.23\n",
            "step327 | loss: 5.072498321533203 | dt: 2299.01ms | tok/sec:  3563.27 | norm: 1.43\n",
            "step328 | loss: 4.992977142333984 | dt: 2293.09ms | tok/sec:  3572.47 | norm: 1.14\n",
            "step329 | loss: 5.019753456115723 | dt: 2294.80ms | tok/sec:  3569.80 | norm: 1.12\n",
            "step330 | loss: 4.64914608001709 | dt: 2297.16ms | tok/sec:  3566.14 | norm: 1.19\n",
            "step331 | loss: 4.549032211303711 | dt: 2292.17ms | tok/sec:  3573.91 | norm: 1.04\n",
            "step332 | loss: 4.6545867919921875 | dt: 2306.56ms | tok/sec:  3551.61 | norm: 1.02\n",
            "step333 | loss: 4.808984756469727 | dt: 2305.11ms | tok/sec:  3553.84 | norm: 0.87\n",
            "step334 | loss: 4.726726055145264 | dt: 2300.99ms | tok/sec:  3560.21 | norm: 1.29\n",
            "step335 | loss: 4.700722694396973 | dt: 2307.03ms | tok/sec:  3550.88 | norm: 1.02\n",
            "step336 | loss: 4.460609436035156 | dt: 2289.60ms | tok/sec:  3577.91 | norm: 1.06\n",
            "step337 | loss: 4.667068004608154 | dt: 2290.31ms | tok/sec:  3576.80 | norm: 0.90\n",
            "step338 | loss: 4.592923164367676 | dt: 2307.82ms | tok/sec:  3549.67 | norm: 1.09\n",
            "step339 | loss: 4.877613544464111 | dt: 2292.26ms | tok/sec:  3573.77 | norm: 0.85\n",
            "step340 | loss: 4.9500322341918945 | dt: 2304.22ms | tok/sec:  3555.22 | norm: 0.82\n",
            "step341 | loss: 4.818554878234863 | dt: 2293.80ms | tok/sec:  3571.36 | norm: 0.82\n",
            "step342 | loss: 4.834928035736084 | dt: 2301.49ms | tok/sec:  3559.43 | norm: 0.82\n",
            "step343 | loss: 4.601043224334717 | dt: 2290.99ms | tok/sec:  3575.75 | norm: 0.86\n",
            "step344 | loss: 4.798501491546631 | dt: 2302.04ms | tok/sec:  3558.58 | norm: 0.92\n",
            "step345 | loss: 4.877887725830078 | dt: 2299.15ms | tok/sec:  3563.05 | norm: 0.90\n",
            "step346 | loss: 4.803046703338623 | dt: 2297.74ms | tok/sec:  3565.25 | norm: 0.89\n",
            "step347 | loss: 4.657588005065918 | dt: 2305.79ms | tok/sec:  3552.80 | norm: 0.87\n",
            "step348 | loss: 4.773932456970215 | dt: 2307.81ms | tok/sec:  3549.69 | norm: 0.86\n",
            "step349 | loss: 4.641410827636719 | dt: 2308.43ms | tok/sec:  3548.73 | norm: 1.09\n",
            "step350 | loss: 4.877414703369141 | dt: 2309.30ms | tok/sec:  3547.39 | norm: 1.06\n",
            "step351 | loss: 4.607907295227051 | dt: 2305.94ms | tok/sec:  3552.56 | norm: 1.06\n",
            "step352 | loss: 4.641172409057617 | dt: 2302.38ms | tok/sec:  3558.05 | norm: 1.01\n",
            "step353 | loss: 4.639081954956055 | dt: 2304.11ms | tok/sec:  3555.39 | norm: 0.85\n",
            "step354 | loss: 5.011447906494141 | dt: 2308.81ms | tok/sec:  3548.14 | norm: 1.09\n",
            "step355 | loss: 4.931183338165283 | dt: 2304.28ms | tok/sec:  3555.13 | norm: 1.08\n",
            "step356 | loss: 5.236813068389893 | dt: 2291.24ms | tok/sec:  3575.36 | norm: 1.34\n",
            "step357 | loss: 4.882075309753418 | dt: 2306.04ms | tok/sec:  3552.41 | norm: 1.30\n",
            "step358 | loss: 4.9320502281188965 | dt: 2301.23ms | tok/sec:  3559.84 | norm: 1.12\n",
            "step359 | loss: 4.726041316986084 | dt: 2305.65ms | tok/sec:  3553.01 | norm: 1.25\n",
            "step360 | loss: 4.639966011047363 | dt: 2309.94ms | tok/sec:  3546.41 | norm: 1.24\n",
            "step361 | loss: 4.728344440460205 | dt: 2306.57ms | tok/sec:  3551.59 | norm: 1.23\n",
            "step362 | loss: 4.5474138259887695 | dt: 2306.21ms | tok/sec:  3552.15 | norm: 1.19\n",
            "step363 | loss: 4.698692321777344 | dt: 2302.99ms | tok/sec:  3557.11 | norm: 1.08\n",
            "step364 | loss: 4.622720718383789 | dt: 2313.19ms | tok/sec:  3541.43 | norm: 1.02\n",
            "step365 | loss: 4.57321834564209 | dt: 2302.59ms | tok/sec:  3557.74 | norm: 0.99\n",
            "step366 | loss: 4.454977989196777 | dt: 2309.78ms | tok/sec:  3546.66 | norm: 1.07\n",
            "step367 | loss: 4.339471817016602 | dt: 2305.01ms | tok/sec:  3553.99 | norm: 0.94\n",
            "step368 | loss: 4.976034641265869 | dt: 2308.85ms | tok/sec:  3548.08 | norm: 1.10\n",
            "step369 | loss: 4.83394718170166 | dt: 2306.01ms | tok/sec:  3552.45 | norm: 1.09\n",
            "step370 | loss: 4.860768795013428 | dt: 2313.12ms | tok/sec:  3541.53 | norm: 1.08\n",
            "step371 | loss: 4.520817279815674 | dt: 2306.09ms | tok/sec:  3552.34 | norm: 1.09\n",
            "step372 | loss: 4.465671539306641 | dt: 2313.27ms | tok/sec:  3541.31 | norm: 0.93\n",
            "step373 | loss: 4.5716423988342285 | dt: 2305.18ms | tok/sec:  3553.73 | norm: 0.98\n",
            "step374 | loss: 4.734773635864258 | dt: 2309.20ms | tok/sec:  3547.55 | norm: 0.90\n",
            "step375 | loss: 4.644841194152832 | dt: 2307.08ms | tok/sec:  3550.80 | norm: 1.22\n",
            "step376 | loss: 4.643272399902344 | dt: 2304.76ms | tok/sec:  3554.38 | norm: 0.91\n",
            "step377 | loss: 4.361111640930176 | dt: 2307.71ms | tok/sec:  3549.84 | norm: 0.89\n",
            "step378 | loss: 4.5618157386779785 | dt: 2305.56ms | tok/sec:  3553.14 | norm: 0.89\n",
            "step379 | loss: 4.5023932456970215 | dt: 2313.00ms | tok/sec:  3541.72 | norm: 1.09\n",
            "step380 | loss: 4.818605899810791 | dt: 2299.12ms | tok/sec:  3563.11 | norm: 0.84\n",
            "step381 | loss: 4.886672496795654 | dt: 2309.82ms | tok/sec:  3546.59 | norm: 0.85\n",
            "step382 | loss: 4.7573747634887695 | dt: 2301.20ms | tok/sec:  3559.88 | norm: 0.76\n",
            "step383 | loss: 4.773007869720459 | dt: 2306.25ms | tok/sec:  3552.08 | norm: 0.72\n",
            "step384 | loss: 4.550174713134766 | dt: 2307.04ms | tok/sec:  3550.87 | norm: 0.84\n",
            "step385 | loss: 4.722808837890625 | dt: 2306.49ms | tok/sec:  3551.72 | norm: 0.88\n",
            "step386 | loss: 4.818273544311523 | dt: 2307.53ms | tok/sec:  3550.12 | norm: 0.82\n",
            "step387 | loss: 4.741825580596924 | dt: 2304.28ms | tok/sec:  3555.13 | norm: 0.92\n",
            "step388 | loss: 4.583401203155518 | dt: 2307.72ms | tok/sec:  3549.82 | norm: 1.00\n",
            "step389 | loss: 4.702856063842773 | dt: 2304.91ms | tok/sec:  3554.16 | norm: 0.84\n",
            "step390 | loss: 4.551120281219482 | dt: 2311.71ms | tok/sec:  3543.69 | norm: 0.82\n",
            "step391 | loss: 4.809020042419434 | dt: 2301.58ms | tok/sec:  3559.29 | norm: 0.88\n",
            "step392 | loss: 4.532141208648682 | dt: 2307.49ms | tok/sec:  3550.18 | norm: 0.95\n",
            "step393 | loss: 4.556522846221924 | dt: 2307.93ms | tok/sec:  3549.51 | norm: 1.05\n",
            "step394 | loss: 4.537905693054199 | dt: 2307.74ms | tok/sec:  3549.79 | norm: 0.79\n",
            "step395 | loss: 4.889418601989746 | dt: 2309.11ms | tok/sec:  3547.69 | norm: 0.97\n",
            "step396 | loss: 4.839014530181885 | dt: 2304.01ms | tok/sec:  3555.54 | norm: 0.96\n",
            "step397 | loss: 5.159203052520752 | dt: 2310.49ms | tok/sec:  3545.57 | norm: 1.15\n",
            "step398 | loss: 4.79980993270874 | dt: 2306.54ms | tok/sec:  3551.65 | norm: 1.11\n",
            "step399 | loss: 4.861387729644775 | dt: 2306.57ms | tok/sec:  3551.60 | norm: 1.05\n",
            "step400 | loss: 4.668987274169922 | dt: 2308.05ms | tok/sec:  3549.31 | norm: 1.19\n",
            "step401 | loss: 4.569786071777344 | dt: 2307.47ms | tok/sec:  3550.21 | norm: 1.07\n",
            "step402 | loss: 4.635849475860596 | dt: 2310.64ms | tok/sec:  3545.33 | norm: 1.04\n",
            "step403 | loss: 4.453954696655273 | dt: 2315.09ms | tok/sec:  3538.52 | norm: 1.03\n",
            "step404 | loss: 4.627358913421631 | dt: 2304.24ms | tok/sec:  3555.18 | norm: 1.07\n",
            "step405 | loss: 4.555994033813477 | dt: 2301.51ms | tok/sec:  3559.40 | norm: 1.12\n",
            "step406 | loss: 4.531641960144043 | dt: 2305.22ms | tok/sec:  3553.68 | norm: 1.04\n",
            "step407 | loss: 4.394992351531982 | dt: 2307.13ms | tok/sec:  3550.73 | norm: 0.92\n",
            "step408 | loss: 4.246201515197754 | dt: 2306.86ms | tok/sec:  3551.14 | norm: 0.90\n",
            "step409 | loss: 4.905420303344727 | dt: 2304.44ms | tok/sec:  3554.88 | norm: 1.18\n",
            "step410 | loss: 4.804412364959717 | dt: 2308.31ms | tok/sec:  3548.92 | norm: 0.92\n",
            "step411 | loss: 4.783878803253174 | dt: 2313.81ms | tok/sec:  3540.48 | norm: 0.88\n",
            "step412 | loss: 4.460745334625244 | dt: 2292.88ms | tok/sec:  3572.79 | norm: 1.05\n",
            "step413 | loss: 4.407891273498535 | dt: 2314.11ms | tok/sec:  3540.02 | norm: 1.04\n",
            "step414 | loss: 4.512177467346191 | dt: 2303.68ms | tok/sec:  3556.05 | norm: 0.98\n",
            "step415 | loss: 4.659162998199463 | dt: 2306.45ms | tok/sec:  3551.78 | norm: 0.64\n",
            "step416 | loss: 4.574563503265381 | dt: 2303.91ms | tok/sec:  3555.69 | norm: 1.09\n",
            "step417 | loss: 4.570185661315918 | dt: 2302.01ms | tok/sec:  3558.63 | norm: 0.99\n",
            "step418 | loss: 4.290410041809082 | dt: 2313.00ms | tok/sec:  3541.72 | norm: 0.99\n",
            "step419 | loss: 4.512981414794922 | dt: 2306.76ms | tok/sec:  3551.30 | norm: 1.06\n",
            "step420 | loss: 4.436194896697998 | dt: 2304.82ms | tok/sec:  3554.29 | norm: 1.16\n",
            "step421 | loss: 4.733535289764404 | dt: 2304.39ms | tok/sec:  3554.95 | norm: 0.89\n",
            "step422 | loss: 4.804035663604736 | dt: 2312.92ms | tok/sec:  3541.84 | norm: 0.80\n",
            "step423 | loss: 4.695369720458984 | dt: 2310.59ms | tok/sec:  3545.42 | norm: 0.74\n",
            "step424 | loss: 4.705554008483887 | dt: 2298.45ms | tok/sec:  3564.14 | norm: 0.76\n",
            "step425 | loss: 4.4556756019592285 | dt: 2308.61ms | tok/sec:  3548.46 | norm: 0.84\n",
            "step426 | loss: 4.679088115692139 | dt: 2308.83ms | tok/sec:  3548.12 | norm: 0.93\n",
            "step427 | loss: 4.7767839431762695 | dt: 2310.07ms | tok/sec:  3546.21 | norm: 0.90\n",
            "step428 | loss: 4.703556060791016 | dt: 2303.85ms | tok/sec:  3555.78 | norm: 0.93\n",
            "step429 | loss: 4.528587341308594 | dt: 2308.08ms | tok/sec:  3549.27 | norm: 0.80\n",
            "step430 | loss: 4.652236461639404 | dt: 2311.54ms | tok/sec:  3543.95 | norm: 0.76\n",
            "step431 | loss: 4.502355575561523 | dt: 2295.94ms | tok/sec:  3568.04 | norm: 0.80\n",
            "step432 | loss: 4.760213375091553 | dt: 2309.92ms | tok/sec:  3546.44 | norm: 0.91\n",
            "step433 | loss: 4.475639343261719 | dt: 2301.44ms | tok/sec:  3559.51 | norm: 0.86\n",
            "step434 | loss: 4.497068405151367 | dt: 2299.37ms | tok/sec:  3562.72 | norm: 0.88\n",
            "step435 | loss: 4.489537715911865 | dt: 2311.95ms | tok/sec:  3543.33 | norm: 0.77\n",
            "step436 | loss: 4.8393425941467285 | dt: 2303.88ms | tok/sec:  3555.74 | norm: 1.04\n",
            "step437 | loss: 4.7910943031311035 | dt: 2307.20ms | tok/sec:  3550.63 | norm: 1.02\n",
            "step438 | loss: 5.098659992218018 | dt: 2304.77ms | tok/sec:  3554.37 | norm: 0.87\n",
            "step439 | loss: 4.752406120300293 | dt: 2308.01ms | tok/sec:  3549.38 | norm: 0.91\n",
            "step440 | loss: 4.782588958740234 | dt: 2303.26ms | tok/sec:  3556.70 | norm: 0.96\n",
            "step441 | loss: 4.5848612785339355 | dt: 2305.52ms | tok/sec:  3553.21 | norm: 1.24\n",
            "step442 | loss: 4.49214506149292 | dt: 2313.53ms | tok/sec:  3540.91 | norm: 1.08\n",
            "step443 | loss: 4.581797122955322 | dt: 2303.54ms | tok/sec:  3556.27 | norm: 1.01\n",
            "step444 | loss: 4.416747570037842 | dt: 2306.00ms | tok/sec:  3552.47 | norm: 0.85\n",
            "step445 | loss: 4.573356628417969 | dt: 2304.92ms | tok/sec:  3554.14 | norm: 0.81\n",
            "step446 | loss: 4.504344463348389 | dt: 2308.36ms | tok/sec:  3548.85 | norm: 0.95\n",
            "step447 | loss: 4.459729194641113 | dt: 2308.61ms | tok/sec:  3548.45 | norm: 0.88\n",
            "step448 | loss: 4.338834762573242 | dt: 2304.39ms | tok/sec:  3554.96 | norm: 0.85\n",
            "step449 | loss: 4.1935319900512695 | dt: 2307.16ms | tok/sec:  3550.69 | norm: 0.92\n",
            "step450 | loss: 4.8705363273620605 | dt: 2307.76ms | tok/sec:  3549.76 | norm: 1.33\n",
            "step451 | loss: 4.75173282623291 | dt: 2303.70ms | tok/sec:  3556.02 | norm: 1.08\n",
            "step452 | loss: 4.769200325012207 | dt: 2307.57ms | tok/sec:  3550.05 | norm: 1.17\n",
            "step453 | loss: 4.416035175323486 | dt: 2303.22ms | tok/sec:  3556.77 | norm: 1.14\n",
            "step454 | loss: 4.373802661895752 | dt: 2306.61ms | tok/sec:  3551.54 | norm: 0.96\n",
            "step455 | loss: 4.463726997375488 | dt: 2314.28ms | tok/sec:  3539.76 | norm: 0.87\n",
            "step456 | loss: 4.635168552398682 | dt: 2305.25ms | tok/sec:  3553.63 | norm: 0.75\n",
            "step457 | loss: 4.542212009429932 | dt: 2307.73ms | tok/sec:  3549.81 | norm: 1.23\n",
            "step458 | loss: 4.525494575500488 | dt: 2314.02ms | tok/sec:  3540.16 | norm: 1.02\n",
            "step459 | loss: 4.258951663970947 | dt: 2299.06ms | tok/sec:  3563.20 | norm: 0.96\n",
            "step460 | loss: 4.466871738433838 | dt: 2305.48ms | tok/sec:  3553.27 | norm: 0.84\n",
            "step461 | loss: 4.392568111419678 | dt: 2308.97ms | tok/sec:  3547.91 | norm: 1.07\n",
            "step462 | loss: 4.704384803771973 | dt: 2308.53ms | tok/sec:  3548.58 | norm: 0.95\n",
            "step463 | loss: 4.7948737144470215 | dt: 2320.52ms | tok/sec:  3530.24 | norm: 1.01\n",
            "step464 | loss: 4.684811592102051 | dt: 2318.33ms | tok/sec:  3533.58 | norm: 0.87\n",
            "step465 | loss: 4.679356575012207 | dt: 2297.59ms | tok/sec:  3565.48 | norm: 0.81\n",
            "step466 | loss: 4.417679309844971 | dt: 2306.68ms | tok/sec:  3551.43 | norm: 0.79\n",
            "step467 | loss: 4.624238014221191 | dt: 2310.66ms | tok/sec:  3545.31 | norm: 0.87\n",
            "step468 | loss: 4.71738338470459 | dt: 2306.50ms | tok/sec:  3551.70 | norm: 0.89\n",
            "step469 | loss: 4.639471054077148 | dt: 2313.48ms | tok/sec:  3540.99 | norm: 0.99\n",
            "step470 | loss: 4.48512077331543 | dt: 2296.50ms | tok/sec:  3567.16 | norm: 0.93\n",
            "step471 | loss: 4.613563060760498 | dt: 2312.23ms | tok/sec:  3542.90 | norm: 0.85\n",
            "step472 | loss: 4.468483924865723 | dt: 2303.70ms | tok/sec:  3556.02 | norm: 0.88\n",
            "step473 | loss: 4.712873935699463 | dt: 2311.75ms | tok/sec:  3543.64 | norm: 0.89\n",
            "step474 | loss: 4.430881977081299 | dt: 2309.68ms | tok/sec:  3546.81 | norm: 0.90\n",
            "step475 | loss: 4.466791152954102 | dt: 2314.90ms | tok/sec:  3538.81 | norm: 0.94\n",
            "step476 | loss: 4.477308750152588 | dt: 2301.74ms | tok/sec:  3559.05 | norm: 0.83\n",
            "step477 | loss: 4.815132141113281 | dt: 2316.56ms | tok/sec:  3536.27 | norm: 1.06\n",
            "step478 | loss: 4.766100883483887 | dt: 2298.15ms | tok/sec:  3564.61 | norm: 1.09\n",
            "step479 | loss: 5.028670310974121 | dt: 2315.10ms | tok/sec:  3538.51 | norm: 1.12\n",
            "step480 | loss: 4.684736251831055 | dt: 2315.82ms | tok/sec:  3537.41 | norm: 1.07\n",
            "step481 | loss: 4.736457347869873 | dt: 2306.17ms | tok/sec:  3552.21 | norm: 0.77\n",
            "step482 | loss: 4.524658203125 | dt: 2308.97ms | tok/sec:  3547.90 | norm: 1.14\n",
            "step483 | loss: 4.440650463104248 | dt: 2310.60ms | tok/sec:  3545.40 | norm: 1.23\n",
            "step484 | loss: 4.506648540496826 | dt: 2307.10ms | tok/sec:  3550.78 | norm: 1.10\n",
            "step485 | loss: 4.357855319976807 | dt: 2310.48ms | tok/sec:  3545.59 | norm: 1.03\n",
            "step486 | loss: 4.523839950561523 | dt: 2304.05ms | tok/sec:  3555.49 | norm: 0.92\n",
            "step487 | loss: 4.489260196685791 | dt: 2316.56ms | tok/sec:  3536.27 | norm: 1.14\n",
            "step488 | loss: 4.429257392883301 | dt: 2310.22ms | tok/sec:  3545.98 | norm: 0.98\n",
            "step489 | loss: 4.297430992126465 | dt: 2302.45ms | tok/sec:  3557.95 | norm: 1.01\n",
            "step490 | loss: 4.13280725479126 | dt: 2305.37ms | tok/sec:  3553.45 | norm: 1.07\n",
            "step491 | loss: 4.848844528198242 | dt: 2304.48ms | tok/sec:  3554.81 | norm: 1.30\n",
            "step492 | loss: 4.687577247619629 | dt: 2312.15ms | tok/sec:  3543.02 | norm: 1.05\n",
            "step493 | loss: 4.717187881469727 | dt: 2306.44ms | tok/sec:  3551.80 | norm: 1.13\n",
            "step494 | loss: 4.375618934631348 | dt: 2303.06ms | tok/sec:  3557.01 | norm: 1.02\n",
            "step495 | loss: 4.333872318267822 | dt: 2310.75ms | tok/sec:  3545.17 | norm: 0.89\n",
            "step496 | loss: 4.430291652679443 | dt: 2303.80ms | tok/sec:  3555.87 | norm: 0.84\n",
            "step497 | loss: 4.599109649658203 | dt: 2306.86ms | tok/sec:  3551.15 | norm: 0.70\n",
            "step498 | loss: 4.488107681274414 | dt: 2306.54ms | tok/sec:  3551.64 | norm: 1.21\n",
            "step499 | loss: 4.464732646942139 | dt: 2304.86ms | tok/sec:  3554.23 | norm: 1.06\n",
            "step500 | loss: 4.216037750244141 | dt: 2305.30ms | tok/sec:  3553.55 | norm: 1.07\n",
            "step501 | loss: 4.4193291664123535 | dt: 2305.40ms | tok/sec:  3553.40 | norm: 0.84\n",
            "step502 | loss: 4.3126654624938965 | dt: 2303.08ms | tok/sec:  3556.98 | norm: 0.97\n",
            "step503 | loss: 4.615211009979248 | dt: 2308.33ms | tok/sec:  3548.88 | norm: 0.83\n",
            "step504 | loss: 4.707419395446777 | dt: 2306.62ms | tok/sec:  3551.51 | norm: 0.92\n",
            "step505 | loss: 4.614399433135986 | dt: 2297.46ms | tok/sec:  3565.68 | norm: 0.89\n",
            "step506 | loss: 4.62880277633667 | dt: 2295.73ms | tok/sec:  3568.36 | norm: 0.92\n",
            "step507 | loss: 4.386997699737549 | dt: 2309.35ms | tok/sec:  3547.32 | norm: 0.82\n",
            "step508 | loss: 4.578683853149414 | dt: 2304.20ms | tok/sec:  3555.25 | norm: 0.81\n",
            "step509 | loss: 4.6663498878479 | dt: 2307.95ms | tok/sec:  3549.48 | norm: 0.85\n",
            "step510 | loss: 4.606154441833496 | dt: 2312.09ms | tok/sec:  3543.12 | norm: 0.90\n",
            "step511 | loss: 4.430295467376709 | dt: 2303.92ms | tok/sec:  3555.68 | norm: 1.02\n",
            "step512 | loss: 4.5606489181518555 | dt: 2291.00ms | tok/sec:  3575.73 | norm: 1.04\n",
            "step513 | loss: 4.426259517669678 | dt: 2309.10ms | tok/sec:  3547.70 | norm: 1.00\n",
            "step514 | loss: 4.678666591644287 | dt: 2304.52ms | tok/sec:  3554.75 | norm: 0.81\n",
            "step515 | loss: 4.4073944091796875 | dt: 2307.67ms | tok/sec:  3549.90 | norm: 1.02\n",
            "step516 | loss: 4.427535057067871 | dt: 2292.51ms | tok/sec:  3573.38 | norm: 1.07\n",
            "step517 | loss: 4.4118170738220215 | dt: 2301.23ms | tok/sec:  3559.83 | norm: 0.92\n",
            "step518 | loss: 4.806034564971924 | dt: 2311.28ms | tok/sec:  3544.35 | norm: 1.17\n",
            "step519 | loss: 4.752878665924072 | dt: 2302.93ms | tok/sec:  3557.21 | norm: 1.13\n",
            "step520 | loss: 5.082659721374512 | dt: 2306.19ms | tok/sec:  3552.18 | norm: 1.03\n",
            "step521 | loss: 4.721841335296631 | dt: 2290.84ms | tok/sec:  3575.99 | norm: 1.01\n",
            "step522 | loss: 4.733649253845215 | dt: 2302.79ms | tok/sec:  3557.42 | norm: 0.96\n",
            "step523 | loss: 4.51903772354126 | dt: 2304.32ms | tok/sec:  3555.05 | norm: 1.24\n",
            "step524 | loss: 4.437418460845947 | dt: 2302.09ms | tok/sec:  3558.51 | norm: 1.10\n",
            "step525 | loss: 4.5034990310668945 | dt: 2292.76ms | tok/sec:  3572.99 | norm: 1.04\n",
            "step526 | loss: 4.336724281311035 | dt: 2296.13ms | tok/sec:  3567.74 | norm: 0.94\n",
            "step527 | loss: 4.48115348815918 | dt: 2298.24ms | tok/sec:  3564.47 | norm: 0.92\n",
            "step528 | loss: 4.4175214767456055 | dt: 2282.98ms | tok/sec:  3588.29 | norm: 1.22\n",
            "step529 | loss: 4.355011940002441 | dt: 2289.95ms | tok/sec:  3577.36 | norm: 1.11\n",
            "step530 | loss: 4.2433977127075195 | dt: 2296.82ms | tok/sec:  3566.67 | norm: 0.95\n",
            "step531 | loss: 4.096278667449951 | dt: 2289.49ms | tok/sec:  3578.08 | norm: 0.97\n",
            "step532 | loss: 4.799482822418213 | dt: 2294.17ms | tok/sec:  3570.79 | norm: 1.31\n",
            "step533 | loss: 4.679732799530029 | dt: 2301.08ms | tok/sec:  3560.07 | norm: 1.17\n",
            "step534 | loss: 4.687127590179443 | dt: 2291.40ms | tok/sec:  3575.11 | norm: 1.18\n",
            "step535 | loss: 4.33573055267334 | dt: 2294.19ms | tok/sec:  3570.76 | norm: 1.02\n",
            "step536 | loss: 4.263912200927734 | dt: 2306.37ms | tok/sec:  3551.90 | norm: 0.81\n",
            "step537 | loss: 4.37258243560791 | dt: 2288.98ms | tok/sec:  3578.89 | norm: 0.86\n",
            "step538 | loss: 4.552070617675781 | dt: 2302.87ms | tok/sec:  3557.30 | norm: 0.83\n",
            "step539 | loss: 4.422210216522217 | dt: 2300.36ms | tok/sec:  3561.17 | norm: 1.12\n",
            "step540 | loss: 4.402280330657959 | dt: 2295.76ms | tok/sec:  3568.32 | norm: 0.83\n",
            "step541 | loss: 4.172253608703613 | dt: 2307.38ms | tok/sec:  3550.34 | norm: 0.81\n",
            "step542 | loss: 4.366717338562012 | dt: 2288.90ms | tok/sec:  3579.00 | norm: 0.77\n",
            "step543 | loss: 4.269367218017578 | dt: 2298.31ms | tok/sec:  3564.36 | norm: 1.07\n",
            "step544 | loss: 4.592073917388916 | dt: 2296.83ms | tok/sec:  3566.65 | norm: 0.82\n",
            "step545 | loss: 4.686623573303223 | dt: 2288.47ms | tok/sec:  3579.69 | norm: 0.82\n",
            "step546 | loss: 4.565118789672852 | dt: 2289.09ms | tok/sec:  3578.71 | norm: 0.72\n",
            "step547 | loss: 4.5600056648254395 | dt: 2307.97ms | tok/sec:  3549.44 | norm: 0.77\n",
            "step548 | loss: 4.319966793060303 | dt: 2294.62ms | tok/sec:  3570.09 | norm: 0.85\n",
            "step549 | loss: 4.544116973876953 | dt: 2299.33ms | tok/sec:  3562.77 | norm: 0.97\n",
            "step550 | loss: 4.631453514099121 | dt: 2309.35ms | tok/sec:  3547.32 | norm: 0.95\n",
            "step551 | loss: 4.532726287841797 | dt: 2300.90ms | tok/sec:  3560.34 | norm: 0.90\n",
            "step552 | loss: 4.3918561935424805 | dt: 2295.32ms | tok/sec:  3569.00 | norm: 0.87\n",
            "step553 | loss: 4.513775825500488 | dt: 2295.99ms | tok/sec:  3567.95 | norm: 0.93\n",
            "step554 | loss: 4.423293113708496 | dt: 2289.17ms | tok/sec:  3578.59 | norm: 1.03\n",
            "step555 | loss: 4.670340061187744 | dt: 2301.77ms | tok/sec:  3559.00 | norm: 1.11\n",
            "step556 | loss: 4.403974533081055 | dt: 2284.64ms | tok/sec:  3585.68 | norm: 1.14\n",
            "step557 | loss: 4.421957492828369 | dt: 2303.90ms | tok/sec:  3555.71 | norm: 0.92\n",
            "step558 | loss: 4.432497978210449 | dt: 2306.78ms | tok/sec:  3551.28 | norm: 0.88\n",
            "step559 | loss: 4.8319525718688965 | dt: 2289.94ms | tok/sec:  3577.39 | norm: 1.45\n",
            "step560 | loss: 4.775783538818359 | dt: 2301.17ms | tok/sec:  3559.93 | norm: 1.55\n",
            "step561 | loss: 5.029633522033691 | dt: 2308.78ms | tok/sec:  3548.20 | norm: 1.27\n",
            "step562 | loss: 4.676906585693359 | dt: 2301.59ms | tok/sec:  3559.27 | norm: 1.18\n",
            "step563 | loss: 4.727744102478027 | dt: 2296.40ms | tok/sec:  3567.32 | norm: 1.13\n",
            "step564 | loss: 4.514012813568115 | dt: 2293.86ms | tok/sec:  3571.28 | norm: 1.09\n",
            "step565 | loss: 4.3989458084106445 | dt: 2288.67ms | tok/sec:  3579.36 | norm: 1.01\n",
            "step566 | loss: 4.468149185180664 | dt: 2301.44ms | tok/sec:  3559.51 | norm: 1.38\n",
            "step567 | loss: 4.292110443115234 | dt: 2290.87ms | tok/sec:  3575.93 | norm: 1.26\n",
            "step568 | loss: 4.451122760772705 | dt: 2306.86ms | tok/sec:  3551.14 | norm: 1.19\n",
            "step569 | loss: 4.388485908508301 | dt: 2305.71ms | tok/sec:  3552.93 | norm: 1.15\n",
            "step570 | loss: 4.326158046722412 | dt: 2307.88ms | tok/sec:  3549.57 | norm: 1.05\n",
            "step571 | loss: 4.189619541168213 | dt: 2309.68ms | tok/sec:  3546.82 | norm: 1.17\n",
            "step572 | loss: 4.061727523803711 | dt: 2304.86ms | tok/sec:  3554.23 | norm: 1.15\n",
            "step573 | loss: 4.716222763061523 | dt: 2313.47ms | tok/sec:  3541.00 | norm: 1.34\n",
            "step574 | loss: 4.595462799072266 | dt: 2301.86ms | tok/sec:  3558.86 | norm: 1.09\n",
            "step575 | loss: 4.661811351776123 | dt: 2308.89ms | tok/sec:  3548.02 | norm: 1.19\n",
            "step576 | loss: 4.313946723937988 | dt: 2309.83ms | tok/sec:  3546.58 | norm: 1.16\n",
            "step577 | loss: 4.265737056732178 | dt: 2305.51ms | tok/sec:  3553.23 | norm: 0.97\n",
            "step578 | loss: 4.362027645111084 | dt: 2304.77ms | tok/sec:  3554.36 | norm: 0.96\n",
            "step579 | loss: 4.51447868347168 | dt: 2307.76ms | tok/sec:  3549.77 | norm: 0.77\n",
            "step580 | loss: 4.37155818939209 | dt: 2307.07ms | tok/sec:  3550.82 | norm: 1.07\n",
            "step581 | loss: 4.382772922515869 | dt: 2293.60ms | tok/sec:  3571.69 | norm: 0.98\n",
            "step582 | loss: 4.111584663391113 | dt: 2299.35ms | tok/sec:  3562.75 | norm: 1.01\n",
            "step583 | loss: 4.350225925445557 | dt: 2306.37ms | tok/sec:  3551.90 | norm: 1.01\n",
            "step584 | loss: 4.227142333984375 | dt: 2301.11ms | tok/sec:  3560.01 | norm: 1.15\n",
            "step585 | loss: 4.559839725494385 | dt: 2293.47ms | tok/sec:  3571.89 | norm: 0.87\n",
            "step586 | loss: 4.618855953216553 | dt: 2301.78ms | tok/sec:  3558.98 | norm: 0.83\n",
            "step587 | loss: 4.494633674621582 | dt: 2312.62ms | tok/sec:  3542.30 | norm: 0.73\n",
            "step588 | loss: 4.49730110168457 | dt: 2303.61ms | tok/sec:  3556.16 | norm: 0.76\n",
            "step589 | loss: 4.273030757904053 | dt: 2310.94ms | tok/sec:  3544.88 | norm: 0.88\n",
            "step590 | loss: 4.472690105438232 | dt: 2310.46ms | tok/sec:  3545.61 | norm: 0.84\n",
            "step591 | loss: 4.57672119140625 | dt: 2304.18ms | tok/sec:  3555.29 | norm: 0.87\n",
            "step592 | loss: 4.50006103515625 | dt: 2304.99ms | tok/sec:  3554.03 | norm: 0.91\n",
            "step593 | loss: 4.3441386222839355 | dt: 2305.75ms | tok/sec:  3552.86 | norm: 0.85\n",
            "step594 | loss: 4.474480152130127 | dt: 2309.56ms | tok/sec:  3546.99 | norm: 0.88\n",
            "step595 | loss: 4.322170734405518 | dt: 2301.65ms | tok/sec:  3559.18 | norm: 0.94\n",
            "step596 | loss: 4.59450101852417 | dt: 2304.40ms | tok/sec:  3554.94 | norm: 0.76\n",
            "step597 | loss: 4.320400714874268 | dt: 2303.09ms | tok/sec:  3556.97 | norm: 0.79\n",
            "step598 | loss: 4.3398661613464355 | dt: 2309.30ms | tok/sec:  3547.39 | norm: 0.88\n",
            "step599 | loss: 4.358292102813721 | dt: 2307.97ms | tok/sec:  3549.44 | norm: 1.02\n",
            "step600 | loss: 4.7578535079956055 | dt: 2305.50ms | tok/sec:  3553.24 | norm: 1.23\n",
            "step601 | loss: 4.720632553100586 | dt: 2308.86ms | tok/sec:  3548.08 | norm: 1.12\n",
            "step602 | loss: 4.936197757720947 | dt: 2312.21ms | tok/sec:  3542.93 | norm: 0.99\n",
            "step603 | loss: 4.570131301879883 | dt: 2308.30ms | tok/sec:  3548.93 | norm: 0.96\n",
            "step604 | loss: 4.63919734954834 | dt: 2304.35ms | tok/sec:  3555.02 | norm: 1.00\n",
            "step605 | loss: 4.380341529846191 | dt: 2311.86ms | tok/sec:  3543.47 | norm: 0.96\n",
            "step606 | loss: 4.293466567993164 | dt: 2309.05ms | tok/sec:  3547.78 | norm: 0.89\n",
            "step607 | loss: 4.406585693359375 | dt: 2310.37ms | tok/sec:  3545.75 | norm: 0.97\n",
            "step608 | loss: 4.25646448135376 | dt: 2307.13ms | tok/sec:  3550.73 | norm: 1.02\n",
            "step609 | loss: 4.43202018737793 | dt: 2300.53ms | tok/sec:  3560.92 | norm: 1.15\n",
            "step610 | loss: 4.354238510131836 | dt: 2309.45ms | tok/sec:  3547.17 | norm: 1.15\n",
            "step611 | loss: 4.308496952056885 | dt: 2306.59ms | tok/sec:  3551.57 | norm: 1.04\n",
            "step612 | loss: 4.182501792907715 | dt: 2305.06ms | tok/sec:  3553.93 | norm: 1.00\n",
            "step613 | loss: 4.037558555603027 | dt: 2304.96ms | tok/sec:  3554.08 | norm: 0.97\n",
            "step614 | loss: 4.685565948486328 | dt: 2305.61ms | tok/sec:  3553.07 | norm: 1.29\n",
            "step615 | loss: 4.559445858001709 | dt: 2306.09ms | tok/sec:  3552.33 | norm: 1.14\n",
            "step616 | loss: 4.5726494789123535 | dt: 2308.39ms | tok/sec:  3548.79 | norm: 1.08\n",
            "step617 | loss: 4.244192123413086 | dt: 2301.98ms | tok/sec:  3558.68 | norm: 1.08\n",
            "step618 | loss: 4.19073486328125 | dt: 2307.68ms | tok/sec:  3549.88 | norm: 1.02\n",
            "step619 | loss: 4.314491271972656 | dt: 2304.73ms | tok/sec:  3554.42 | norm: 1.05\n",
            "step620 | loss: 4.490288734436035 | dt: 2306.68ms | tok/sec:  3551.42 | norm: 0.90\n",
            "step621 | loss: 4.312489986419678 | dt: 2305.70ms | tok/sec:  3552.94 | norm: 1.22\n",
            "step622 | loss: 4.372774600982666 | dt: 2313.53ms | tok/sec:  3540.92 | norm: 0.97\n",
            "step623 | loss: 4.107372760772705 | dt: 2301.87ms | tok/sec:  3558.85 | norm: 1.01\n",
            "step624 | loss: 4.3272929191589355 | dt: 2310.19ms | tok/sec:  3546.04 | norm: 0.90\n",
            "step625 | loss: 4.210611343383789 | dt: 2311.35ms | tok/sec:  3544.25 | norm: 1.05\n",
            "step626 | loss: 4.480275630950928 | dt: 2309.63ms | tok/sec:  3546.88 | norm: 0.81\n",
            "step627 | loss: 4.5713276863098145 | dt: 2310.56ms | tok/sec:  3545.46 | norm: 0.82\n",
            "step628 | loss: 4.483702182769775 | dt: 2309.21ms | tok/sec:  3547.53 | norm: 0.91\n",
            "step629 | loss: 4.48890495300293 | dt: 2308.70ms | tok/sec:  3548.31 | norm: 0.91\n",
            "step630 | loss: 4.240492820739746 | dt: 2311.81ms | tok/sec:  3543.55 | norm: 0.81\n",
            "step631 | loss: 4.443042755126953 | dt: 2317.84ms | tok/sec:  3534.33 | norm: 0.82\n",
            "step632 | loss: 4.534395217895508 | dt: 2312.77ms | tok/sec:  3542.08 | norm: 0.91\n",
            "step633 | loss: 4.471058368682861 | dt: 2312.33ms | tok/sec:  3542.74 | norm: 0.93\n",
            "step634 | loss: 4.331089973449707 | dt: 2304.43ms | tok/sec:  3554.89 | norm: 0.94\n",
            "step635 | loss: 4.47144889831543 | dt: 2307.10ms | tok/sec:  3550.77 | norm: 1.04\n",
            "step636 | loss: 4.336932182312012 | dt: 2311.71ms | tok/sec:  3543.70 | norm: 1.03\n",
            "step637 | loss: 4.567350387573242 | dt: 2297.03ms | tok/sec:  3566.35 | norm: 0.77\n",
            "step638 | loss: 4.277762413024902 | dt: 2314.31ms | tok/sec:  3539.72 | norm: 0.80\n",
            "step639 | loss: 4.321950912475586 | dt: 2299.80ms | tok/sec:  3562.05 | norm: 0.89\n",
            "step640 | loss: 4.314310073852539 | dt: 2312.83ms | tok/sec:  3541.98 | norm: 0.88\n",
            "step641 | loss: 4.700903415679932 | dt: 2298.56ms | tok/sec:  3563.97 | norm: 1.12\n",
            "step642 | loss: 4.666967391967773 | dt: 2304.65ms | tok/sec:  3554.55 | norm: 1.09\n",
            "step643 | loss: 4.88471794128418 | dt: 2306.44ms | tok/sec:  3551.79 | norm: 1.09\n",
            "step644 | loss: 4.554506301879883 | dt: 2306.51ms | tok/sec:  3551.69 | norm: 1.02\n",
            "step645 | loss: 4.621891975402832 | dt: 2306.27ms | tok/sec:  3552.06 | norm: 0.92\n",
            "step646 | loss: 4.347263336181641 | dt: 2302.22ms | tok/sec:  3558.31 | norm: 0.87\n",
            "step647 | loss: 4.286327838897705 | dt: 2309.80ms | tok/sec:  3546.62 | norm: 0.94\n",
            "step648 | loss: 4.370290756225586 | dt: 2302.42ms | tok/sec:  3557.99 | norm: 0.93\n",
            "step649 | loss: 4.201732158660889 | dt: 2300.35ms | tok/sec:  3561.20 | norm: 0.83\n",
            "step650 | loss: 4.362209796905518 | dt: 2292.05ms | tok/sec:  3574.10 | norm: 0.82\n",
            "step651 | loss: 4.282348155975342 | dt: 2306.96ms | tok/sec:  3550.99 | norm: 0.95\n",
            "step652 | loss: 4.223895072937012 | dt: 2304.35ms | tok/sec:  3555.01 | norm: 0.97\n",
            "step653 | loss: 4.108322620391846 | dt: 2290.78ms | tok/sec:  3576.08 | norm: 0.89\n",
            "step654 | loss: 3.9757883548736572 | dt: 2301.28ms | tok/sec:  3559.75 | norm: 0.85\n",
            "step655 | loss: 4.661594390869141 | dt: 2298.81ms | tok/sec:  3563.58 | norm: 1.13\n",
            "step656 | loss: 4.551987648010254 | dt: 2292.92ms | tok/sec:  3572.74 | norm: 1.01\n",
            "step657 | loss: 4.5301079750061035 | dt: 2291.67ms | tok/sec:  3574.69 | norm: 1.02\n",
            "step658 | loss: 4.21337366104126 | dt: 2301.37ms | tok/sec:  3559.62 | norm: 1.01\n",
            "step659 | loss: 4.1409220695495605 | dt: 2289.11ms | tok/sec:  3578.69 | norm: 0.80\n",
            "step660 | loss: 4.23533296585083 | dt: 2287.32ms | tok/sec:  3581.48 | norm: 0.91\n",
            "step661 | loss: 4.443839073181152 | dt: 2288.35ms | tok/sec:  3579.87 | norm: 0.93\n",
            "step662 | loss: 4.346959114074707 | dt: 2289.41ms | tok/sec:  3578.21 | norm: 1.16\n",
            "step663 | loss: 4.3753533363342285 | dt: 2286.33ms | tok/sec:  3583.03 | norm: 1.02\n",
            "step664 | loss: 4.1240153312683105 | dt: 2280.21ms | tok/sec:  3592.65 | norm: 1.37\n",
            "step665 | loss: 4.297885417938232 | dt: 2295.43ms | tok/sec:  3568.82 | norm: 0.89\n",
            "step666 | loss: 4.211956977844238 | dt: 2289.42ms | tok/sec:  3578.20 | norm: 1.05\n",
            "step667 | loss: 4.503232479095459 | dt: 2286.35ms | tok/sec:  3583.01 | norm: 0.85\n",
            "step668 | loss: 4.592016696929932 | dt: 2286.28ms | tok/sec:  3583.12 | norm: 0.87\n",
            "step669 | loss: 4.48242712020874 | dt: 2284.28ms | tok/sec:  3586.25 | norm: 0.85\n",
            "step670 | loss: 4.46895694732666 | dt: 2286.68ms | tok/sec:  3582.48 | norm: 0.82\n",
            "step671 | loss: 4.217964172363281 | dt: 2284.21ms | tok/sec:  3586.35 | norm: 0.82\n",
            "step672 | loss: 4.438268184661865 | dt: 2287.09ms | tok/sec:  3581.84 | norm: 0.82\n",
            "step673 | loss: 4.541320323944092 | dt: 2285.54ms | tok/sec:  3584.27 | norm: 0.97\n",
            "step674 | loss: 4.488913536071777 | dt: 2286.65ms | tok/sec:  3582.54 | norm: 1.09\n",
            "step675 | loss: 4.326362133026123 | dt: 2287.17ms | tok/sec:  3581.72 | norm: 1.00\n",
            "step676 | loss: 4.450571537017822 | dt: 2303.76ms | tok/sec:  3555.93 | norm: 0.79\n",
            "step677 | loss: 4.322459697723389 | dt: 2286.16ms | tok/sec:  3583.30 | norm: 0.94\n",
            "step678 | loss: 4.538873672485352 | dt: 2286.97ms | tok/sec:  3582.03 | norm: 1.00\n",
            "step679 | loss: 4.271911144256592 | dt: 2285.60ms | tok/sec:  3584.18 | norm: 1.33\n",
            "step680 | loss: 4.306692600250244 | dt: 2305.03ms | tok/sec:  3553.97 | norm: 1.29\n",
            "step681 | loss: 4.28373384475708 | dt: 2292.57ms | tok/sec:  3573.28 | norm: 0.89\n",
            "step682 | loss: 4.6697492599487305 | dt: 2299.48ms | tok/sec:  3562.54 | norm: 1.04\n",
            "step683 | loss: 4.62757682800293 | dt: 2287.65ms | tok/sec:  3580.97 | norm: 1.22\n",
            "step684 | loss: 4.8815717697143555 | dt: 2305.09ms | tok/sec:  3553.87 | norm: 1.24\n",
            "step685 | loss: 4.539928913116455 | dt: 2287.63ms | tok/sec:  3580.99 | norm: 1.21\n",
            "step686 | loss: 4.581286430358887 | dt: 2303.70ms | tok/sec:  3556.02 | norm: 1.10\n",
            "step687 | loss: 4.317100524902344 | dt: 2289.98ms | tok/sec:  3577.32 | norm: 0.95\n",
            "step688 | loss: 4.244153022766113 | dt: 2284.43ms | tok/sec:  3586.01 | norm: 0.93\n",
            "step689 | loss: 4.311146259307861 | dt: 2305.32ms | tok/sec:  3553.52 | norm: 0.95\n",
            "step690 | loss: 4.169556140899658 | dt: 2289.01ms | tok/sec:  3578.83 | norm: 0.86\n",
            "step691 | loss: 4.315413951873779 | dt: 2305.14ms | tok/sec:  3553.79 | norm: 0.80\n",
            "step692 | loss: 4.307487487792969 | dt: 2289.06ms | tok/sec:  3578.76 | norm: 1.15\n",
            "step693 | loss: 4.269674301147461 | dt: 2289.85ms | tok/sec:  3577.52 | norm: 1.17\n",
            "step694 | loss: 4.112904071807861 | dt: 2289.75ms | tok/sec:  3577.68 | norm: 1.12\n",
            "step695 | loss: 3.986088752746582 | dt: 2289.56ms | tok/sec:  3577.98 | norm: 0.95\n",
            "step696 | loss: 4.6327900886535645 | dt: 2301.93ms | tok/sec:  3558.76 | norm: 1.04\n",
            "step697 | loss: 4.525400161743164 | dt: 2289.99ms | tok/sec:  3577.31 | norm: 0.98\n",
            "step698 | loss: 4.5134382247924805 | dt: 2301.92ms | tok/sec:  3558.76 | norm: 1.01\n",
            "step699 | loss: 4.213990688323975 | dt: 2305.82ms | tok/sec:  3552.75 | norm: 1.03\n",
            "step700 | loss: 4.158049583435059 | dt: 2305.98ms | tok/sec:  3552.50 | norm: 0.88\n",
            "step701 | loss: 4.2509355545043945 | dt: 2286.60ms | tok/sec:  3582.62 | norm: 0.99\n",
            "step702 | loss: 4.431795120239258 | dt: 2292.17ms | tok/sec:  3573.91 | norm: 0.92\n",
            "step703 | loss: 4.35255765914917 | dt: 2299.77ms | tok/sec:  3562.10 | norm: 1.14\n",
            "step704 | loss: 4.319181442260742 | dt: 2286.67ms | tok/sec:  3582.51 | norm: 0.89\n",
            "step705 | loss: 4.081804275512695 | dt: 2291.28ms | tok/sec:  3575.30 | norm: 0.88\n",
            "step706 | loss: 4.24427604675293 | dt: 2301.17ms | tok/sec:  3559.93 | norm: 0.90\n",
            "step707 | loss: 4.1234893798828125 | dt: 2306.32ms | tok/sec:  3551.98 | norm: 1.01\n",
            "step708 | loss: 4.433254718780518 | dt: 2305.62ms | tok/sec:  3553.05 | norm: 0.91\n",
            "step709 | loss: 4.4977803230285645 | dt: 2305.07ms | tok/sec:  3553.90 | norm: 0.85\n",
            "step710 | loss: 4.384998798370361 | dt: 2292.56ms | tok/sec:  3573.30 | norm: 0.78\n",
            "step711 | loss: 4.360495090484619 | dt: 2301.62ms | tok/sec:  3559.23 | norm: 0.75\n",
            "step712 | loss: 4.11837100982666 | dt: 2305.48ms | tok/sec:  3553.27 | norm: 0.78\n",
            "step713 | loss: 4.353995323181152 | dt: 2300.64ms | tok/sec:  3560.75 | norm: 0.85\n",
            "step714 | loss: 4.446704864501953 | dt: 2295.73ms | tok/sec:  3568.37 | norm: 0.86\n",
            "step715 | loss: 4.368103504180908 | dt: 2298.02ms | tok/sec:  3564.81 | norm: 0.92\n",
            "step716 | loss: 4.207945823669434 | dt: 2290.26ms | tok/sec:  3576.89 | norm: 0.97\n",
            "step717 | loss: 4.329098224639893 | dt: 2307.06ms | tok/sec:  3550.84 | norm: 0.92\n",
            "step718 | loss: 4.2065815925598145 | dt: 2293.48ms | tok/sec:  3571.86 | norm: 0.91\n",
            "step719 | loss: 4.452245235443115 | dt: 2304.96ms | tok/sec:  3554.08 | norm: 0.78\n",
            "step720 | loss: 4.219259262084961 | dt: 2290.46ms | tok/sec:  3576.57 | norm: 0.88\n",
            "step721 | loss: 4.2515435218811035 | dt: 2298.11ms | tok/sec:  3564.67 | norm: 1.09\n",
            "step722 | loss: 4.231725215911865 | dt: 2303.03ms | tok/sec:  3557.05 | norm: 1.08\n",
            "step723 | loss: 4.578052043914795 | dt: 2301.59ms | tok/sec:  3559.27 | norm: 1.18\n",
            "step724 | loss: 4.568663597106934 | dt: 2306.91ms | tok/sec:  3551.07 | norm: 1.03\n",
            "step725 | loss: 4.826904773712158 | dt: 2309.36ms | tok/sec:  3547.31 | norm: 1.02\n",
            "step726 | loss: 4.5012006759643555 | dt: 2299.91ms | tok/sec:  3561.88 | norm: 1.04\n",
            "step727 | loss: 4.534636974334717 | dt: 2305.29ms | tok/sec:  3553.57 | norm: 1.00\n",
            "step728 | loss: 4.261072635650635 | dt: 2297.66ms | tok/sec:  3565.36 | norm: 1.02\n",
            "step729 | loss: 4.216384410858154 | dt: 2296.28ms | tok/sec:  3567.51 | norm: 0.99\n",
            "step730 | loss: 4.231316089630127 | dt: 2305.55ms | tok/sec:  3553.16 | norm: 0.84\n",
            "step731 | loss: 4.11199951171875 | dt: 2305.18ms | tok/sec:  3553.74 | norm: 0.79\n",
            "step732 | loss: 4.272443771362305 | dt: 2306.09ms | tok/sec:  3552.33 | norm: 0.88\n",
            "step733 | loss: 4.221818447113037 | dt: 2304.00ms | tok/sec:  3555.55 | norm: 0.98\n",
            "step734 | loss: 4.184690475463867 | dt: 2299.04ms | tok/sec:  3563.23 | norm: 0.94\n",
            "step735 | loss: 4.052567958831787 | dt: 2293.46ms | tok/sec:  3571.90 | norm: 0.91\n",
            "step736 | loss: 3.911104440689087 | dt: 2288.98ms | tok/sec:  3578.89 | norm: 0.87\n",
            "step737 | loss: 4.537712574005127 | dt: 2307.18ms | tok/sec:  3550.65 | norm: 1.12\n",
            "step738 | loss: 4.417442798614502 | dt: 2303.89ms | tok/sec:  3555.72 | norm: 1.00\n",
            "step739 | loss: 4.413908004760742 | dt: 2304.33ms | tok/sec:  3555.05 | norm: 1.01\n",
            "step740 | loss: 4.129517078399658 | dt: 2296.31ms | tok/sec:  3567.46 | norm: 1.10\n",
            "step741 | loss: 4.086535930633545 | dt: 2302.83ms | tok/sec:  3557.36 | norm: 1.09\n",
            "step742 | loss: 4.169131755828857 | dt: 2301.29ms | tok/sec:  3559.75 | norm: 1.03\n",
            "step743 | loss: 4.361244201660156 | dt: 2290.44ms | tok/sec:  3576.61 | norm: 0.91\n",
            "step744 | loss: 4.236090183258057 | dt: 2301.63ms | tok/sec:  3559.21 | norm: 1.08\n",
            "step745 | loss: 4.243112564086914 | dt: 2307.38ms | tok/sec:  3550.35 | norm: 0.98\n",
            "step746 | loss: 3.981714963912964 | dt: 2314.55ms | tok/sec:  3539.35 | norm: 0.96\n",
            "step747 | loss: 4.18094539642334 | dt: 2303.70ms | tok/sec:  3556.02 | norm: 1.02\n",
            "step748 | loss: 4.082976818084717 | dt: 2289.54ms | tok/sec:  3578.01 | norm: 1.02\n",
            "step749 | loss: 4.364386081695557 | dt: 2307.27ms | tok/sec:  3550.52 | norm: 0.83\n",
            "step750 | loss: 4.4420623779296875 | dt: 2313.68ms | tok/sec:  3540.67 | norm: 0.88\n",
            "step751 | loss: 4.3343400955200195 | dt: 2305.02ms | tok/sec:  3553.97 | norm: 0.90\n",
            "step752 | loss: 4.319232940673828 | dt: 2304.80ms | tok/sec:  3554.32 | norm: 0.84\n",
            "step753 | loss: 4.0748419761657715 | dt: 2305.34ms | tok/sec:  3553.50 | norm: 0.81\n",
            "step754 | loss: 4.2767181396484375 | dt: 2310.89ms | tok/sec:  3544.96 | norm: 0.86\n",
            "step755 | loss: 4.373315811157227 | dt: 2301.05ms | tok/sec:  3560.12 | norm: 0.85\n",
            "step756 | loss: 4.309667587280273 | dt: 2303.09ms | tok/sec:  3556.96 | norm: 0.93\n",
            "step757 | loss: 4.176182746887207 | dt: 2302.20ms | tok/sec:  3558.33 | norm: 0.87\n",
            "step758 | loss: 4.304083824157715 | dt: 2294.89ms | tok/sec:  3569.67 | norm: 0.80\n",
            "step759 | loss: 4.192200183868408 | dt: 2307.12ms | tok/sec:  3550.75 | norm: 0.82\n",
            "step760 | loss: 4.416082859039307 | dt: 2306.42ms | tok/sec:  3551.82 | norm: 0.87\n",
            "step761 | loss: 4.167412281036377 | dt: 2293.63ms | tok/sec:  3571.63 | norm: 0.87\n",
            "step762 | loss: 4.220929145812988 | dt: 2301.17ms | tok/sec:  3559.93 | norm: 0.97\n",
            "step763 | loss: 4.230892181396484 | dt: 2308.98ms | tok/sec:  3547.89 | norm: 0.87\n",
            "step764 | loss: 4.5756611824035645 | dt: 2307.14ms | tok/sec:  3550.71 | norm: 0.99\n",
            "step765 | loss: 4.497010231018066 | dt: 2314.38ms | tok/sec:  3539.61 | norm: 0.96\n",
            "step766 | loss: 4.724018096923828 | dt: 2294.02ms | tok/sec:  3571.02 | norm: 1.08\n",
            "step767 | loss: 4.400308132171631 | dt: 2310.13ms | tok/sec:  3546.13 | norm: 1.07\n",
            "step768 | loss: 4.438772678375244 | dt: 2307.40ms | tok/sec:  3550.31 | norm: 0.99\n",
            "step769 | loss: 4.21809196472168 | dt: 2311.43ms | tok/sec:  3544.13 | norm: 1.09\n",
            "step770 | loss: 4.190917015075684 | dt: 2301.56ms | tok/sec:  3559.32 | norm: 1.10\n",
            "step771 | loss: 4.236634254455566 | dt: 2301.95ms | tok/sec:  3558.73 | norm: 0.99\n",
            "step772 | loss: 4.078949451446533 | dt: 2309.08ms | tok/sec:  3547.73 | norm: 0.92\n",
            "step773 | loss: 4.224246978759766 | dt: 2305.68ms | tok/sec:  3552.96 | norm: 0.82\n",
            "step774 | loss: 4.180056095123291 | dt: 2306.04ms | tok/sec:  3552.40 | norm: 0.97\n",
            "step775 | loss: 4.14932918548584 | dt: 2311.19ms | tok/sec:  3544.49 | norm: 1.03\n",
            "step776 | loss: 4.001735210418701 | dt: 2310.02ms | tok/sec:  3546.29 | norm: 0.99\n",
            "step777 | loss: 3.8718132972717285 | dt: 2309.03ms | tok/sec:  3547.82 | norm: 0.88\n",
            "step778 | loss: 4.479026794433594 | dt: 2306.14ms | tok/sec:  3552.25 | norm: 1.05\n",
            "step779 | loss: 4.383927345275879 | dt: 2315.16ms | tok/sec:  3538.41 | norm: 0.96\n",
            "step780 | loss: 4.419631481170654 | dt: 2307.56ms | tok/sec:  3550.07 | norm: 0.99\n",
            "step781 | loss: 4.116662502288818 | dt: 2304.83ms | tok/sec:  3554.28 | norm: 0.97\n",
            "step782 | loss: 4.044635772705078 | dt: 2307.09ms | tok/sec:  3550.80 | norm: 0.83\n",
            "step783 | loss: 4.1413164138793945 | dt: 2311.05ms | tok/sec:  3544.71 | norm: 1.03\n",
            "step784 | loss: 4.338653564453125 | dt: 2306.98ms | tok/sec:  3550.97 | norm: 1.18\n",
            "step785 | loss: 4.163083076477051 | dt: 2307.13ms | tok/sec:  3550.72 | norm: 1.23\n",
            "step786 | loss: 4.183486461639404 | dt: 2310.26ms | tok/sec:  3545.92 | norm: 1.05\n",
            "step787 | loss: 3.916598081588745 | dt: 2302.74ms | tok/sec:  3557.51 | norm: 1.00\n",
            "step788 | loss: 4.138077259063721 | dt: 2306.42ms | tok/sec:  3551.83 | norm: 1.05\n",
            "step789 | loss: 4.033498287200928 | dt: 2304.60ms | tok/sec:  3554.63 | norm: 1.08\n",
            "step790 | loss: 4.311134338378906 | dt: 2309.46ms | tok/sec:  3547.15 | norm: 0.98\n",
            "step791 | loss: 4.382108211517334 | dt: 2305.84ms | tok/sec:  3552.71 | norm: 0.94\n",
            "step792 | loss: 4.289656639099121 | dt: 2306.13ms | tok/sec:  3552.27 | norm: 0.82\n",
            "step793 | loss: 4.292037010192871 | dt: 2306.53ms | tok/sec:  3551.65 | norm: 0.92\n",
            "step794 | loss: 4.0841755867004395 | dt: 2310.39ms | tok/sec:  3545.72 | norm: 1.20\n",
            "step795 | loss: 4.330689907073975 | dt: 2306.04ms | tok/sec:  3552.41 | norm: 1.17\n",
            "step796 | loss: 4.407301902770996 | dt: 2309.93ms | tok/sec:  3546.43 | norm: 1.09\n",
            "step797 | loss: 4.315996170043945 | dt: 2302.72ms | tok/sec:  3557.53 | norm: 0.93\n",
            "step798 | loss: 4.163267612457275 | dt: 2305.71ms | tok/sec:  3552.92 | norm: 0.88\n",
            "step799 | loss: 4.287867069244385 | dt: 2309.65ms | tok/sec:  3546.86 | norm: 0.92\n",
            "step800 | loss: 4.156505107879639 | dt: 2311.40ms | tok/sec:  3544.18 | norm: 1.02\n",
            "step801 | loss: 4.396267890930176 | dt: 2302.58ms | tok/sec:  3557.75 | norm: 1.04\n",
            "step802 | loss: 4.143618106842041 | dt: 2308.03ms | tok/sec:  3549.34 | norm: 0.94\n",
            "step803 | loss: 4.158883094787598 | dt: 2307.34ms | tok/sec:  3550.41 | norm: 0.83\n",
            "step804 | loss: 4.149499416351318 | dt: 2310.73ms | tok/sec:  3545.20 | norm: 0.88\n",
            "step805 | loss: 4.478279113769531 | dt: 2310.48ms | tok/sec:  3545.58 | norm: 1.15\n",
            "step806 | loss: 4.44082498550415 | dt: 2304.30ms | tok/sec:  3555.10 | norm: 1.12\n",
            "step807 | loss: 4.687321186065674 | dt: 2303.78ms | tok/sec:  3555.90 | norm: 1.00\n",
            "step808 | loss: 4.390868663787842 | dt: 2311.67ms | tok/sec:  3543.76 | norm: 1.01\n",
            "step809 | loss: 4.42594575881958 | dt: 2303.47ms | tok/sec:  3556.38 | norm: 0.94\n",
            "step810 | loss: 4.2142157554626465 | dt: 2307.84ms | tok/sec:  3549.65 | norm: 1.17\n",
            "step811 | loss: 4.152146339416504 | dt: 2307.54ms | tok/sec:  3550.10 | norm: 1.07\n",
            "step812 | loss: 4.1668381690979 | dt: 2306.58ms | tok/sec:  3551.58 | norm: 1.01\n",
            "step813 | loss: 4.028045177459717 | dt: 2308.33ms | tok/sec:  3548.88 | norm: 0.90\n",
            "step814 | loss: 4.1568121910095215 | dt: 2309.22ms | tok/sec:  3547.51 | norm: 0.82\n",
            "step815 | loss: 4.107815265655518 | dt: 2300.08ms | tok/sec:  3561.61 | norm: 0.94\n",
            "step816 | loss: 4.07319974899292 | dt: 2306.67ms | tok/sec:  3551.44 | norm: 1.00\n",
            "step817 | loss: 3.941514730453491 | dt: 2298.32ms | tok/sec:  3564.34 | norm: 1.02\n",
            "step818 | loss: 3.821336030960083 | dt: 2299.01ms | tok/sec:  3563.28 | norm: 0.95\n",
            "step819 | loss: 4.436516284942627 | dt: 2306.30ms | tok/sec:  3552.01 | norm: 1.07\n",
            "step820 | loss: 4.35889196395874 | dt: 2304.34ms | tok/sec:  3555.04 | norm: 0.92\n",
            "step821 | loss: 4.336177349090576 | dt: 2304.97ms | tok/sec:  3554.06 | norm: 0.81\n",
            "step822 | loss: 4.066466331481934 | dt: 2305.04ms | tok/sec:  3553.95 | norm: 0.90\n",
            "step823 | loss: 4.044369220733643 | dt: 2305.11ms | tok/sec:  3553.84 | norm: 0.85\n",
            "step824 | loss: 4.147393703460693 | dt: 2312.46ms | tok/sec:  3542.55 | norm: 0.96\n",
            "step825 | loss: 4.35451602935791 | dt: 2300.47ms | tok/sec:  3561.01 | norm: 0.99\n",
            "step826 | loss: 4.233841896057129 | dt: 2305.53ms | tok/sec:  3553.19 | norm: 1.31\n",
            "step827 | loss: 4.245798587799072 | dt: 2307.17ms | tok/sec:  3550.67 | norm: 1.26\n",
            "step828 | loss: 4.01271915435791 | dt: 2305.49ms | tok/sec:  3553.25 | norm: 1.09\n",
            "step829 | loss: 4.259291648864746 | dt: 2307.63ms | tok/sec:  3549.96 | norm: 1.07\n",
            "step830 | loss: 4.168907165527344 | dt: 2311.21ms | tok/sec:  3544.46 | norm: 1.28\n",
            "step831 | loss: 4.461110591888428 | dt: 2304.40ms | tok/sec:  3554.94 | norm: 1.19\n",
            "step832 | loss: 4.512576580047607 | dt: 2304.56ms | tok/sec:  3554.69 | norm: 1.26\n",
            "step833 | loss: 4.3961381912231445 | dt: 2310.24ms | tok/sec:  3545.95 | norm: 1.15\n",
            "step834 | loss: 4.381185531616211 | dt: 2306.26ms | tok/sec:  3552.08 | norm: 1.09\n",
            "step835 | loss: 4.123561382293701 | dt: 2306.60ms | tok/sec:  3551.55 | norm: 0.99\n",
            "step836 | loss: 4.354402542114258 | dt: 2308.08ms | tok/sec:  3549.27 | norm: 0.96\n",
            "step837 | loss: 4.421960830688477 | dt: 2298.65ms | tok/sec:  3563.84 | norm: 0.92\n",
            "step838 | loss: 4.328711986541748 | dt: 2303.86ms | tok/sec:  3555.77 | norm: 1.05\n",
            "step839 | loss: 4.1989288330078125 | dt: 2302.38ms | tok/sec:  3558.05 | norm: 1.24\n",
            "step840 | loss: 4.316659450531006 | dt: 2305.33ms | tok/sec:  3553.51 | norm: 1.11\n",
            "step841 | loss: 4.170476913452148 | dt: 2291.01ms | tok/sec:  3575.71 | norm: 0.93\n",
            "step842 | loss: 4.397782802581787 | dt: 2306.72ms | tok/sec:  3551.36 | norm: 0.87\n",
            "step843 | loss: 4.135888576507568 | dt: 2292.92ms | tok/sec:  3572.74 | norm: 0.88\n",
            "step844 | loss: 4.177240371704102 | dt: 2302.78ms | tok/sec:  3557.44 | norm: 0.86\n",
            "step845 | loss: 4.169283390045166 | dt: 2308.36ms | tok/sec:  3548.84 | norm: 0.86\n",
            "step846 | loss: 4.501537799835205 | dt: 2301.62ms | tok/sec:  3559.22 | norm: 1.07\n",
            "step847 | loss: 4.461684226989746 | dt: 2303.88ms | tok/sec:  3555.74 | norm: 0.99\n",
            "step848 | loss: 4.698957920074463 | dt: 2303.62ms | tok/sec:  3556.14 | norm: 1.07\n",
            "step849 | loss: 4.372135639190674 | dt: 2297.88ms | tok/sec:  3565.03 | norm: 0.93\n",
            "step850 | loss: 4.404295444488525 | dt: 2300.67ms | tok/sec:  3560.70 | norm: 0.80\n",
            "step851 | loss: 4.2844648361206055 | dt: 2301.67ms | tok/sec:  3559.16 | norm: 1.14\n",
            "step852 | loss: 4.235015869140625 | dt: 2303.60ms | tok/sec:  3556.17 | norm: 1.13\n",
            "step853 | loss: 4.3819804191589355 | dt: 2293.02ms | tok/sec:  3572.58 | norm: 1.45\n",
            "step854 | loss: 4.140318870544434 | dt: 2300.50ms | tok/sec:  3560.97 | norm: 1.12\n",
            "step855 | loss: 4.239299774169922 | dt: 2297.25ms | tok/sec:  3566.01 | norm: 1.04\n",
            "step856 | loss: 4.233489990234375 | dt: 2296.41ms | tok/sec:  3567.31 | norm: 1.16\n",
            "step857 | loss: 4.131597518920898 | dt: 2308.54ms | tok/sec:  3548.56 | norm: 1.09\n",
            "step858 | loss: 3.9911651611328125 | dt: 2305.45ms | tok/sec:  3553.32 | norm: 1.08\n",
            "step859 | loss: 3.891857624053955 | dt: 2303.70ms | tok/sec:  3556.02 | norm: 1.06\n",
            "step860 | loss: 4.502476215362549 | dt: 2290.47ms | tok/sec:  3576.56 | norm: 1.23\n",
            "step861 | loss: 4.448402404785156 | dt: 2306.70ms | tok/sec:  3551.40 | norm: 1.14\n",
            "step862 | loss: 4.435216426849365 | dt: 2302.01ms | tok/sec:  3558.63 | norm: 1.19\n",
            "step863 | loss: 4.128370761871338 | dt: 2306.03ms | tok/sec:  3552.43 | norm: 1.31\n",
            "step864 | loss: 4.086544036865234 | dt: 2311.97ms | tok/sec:  3543.30 | norm: 1.12\n",
            "step865 | loss: 4.161961555480957 | dt: 2305.24ms | tok/sec:  3553.65 | norm: 0.99\n",
            "step866 | loss: 4.371344566345215 | dt: 2312.28ms | tok/sec:  3542.83 | norm: 0.86\n",
            "step867 | loss: 4.247908592224121 | dt: 2301.91ms | tok/sec:  3558.79 | norm: 1.10\n",
            "step868 | loss: 4.2601518630981445 | dt: 2306.62ms | tok/sec:  3551.52 | norm: 1.10\n",
            "step869 | loss: 4.037916660308838 | dt: 2306.14ms | tok/sec:  3552.26 | norm: 1.10\n",
            "step870 | loss: 4.2286906242370605 | dt: 2306.38ms | tok/sec:  3551.89 | norm: 1.06\n",
            "step871 | loss: 4.154676914215088 | dt: 2304.55ms | tok/sec:  3554.70 | norm: 1.19\n",
            "step872 | loss: 4.428680419921875 | dt: 2311.47ms | tok/sec:  3544.07 | norm: 0.92\n",
            "step873 | loss: 4.538956165313721 | dt: 2304.98ms | tok/sec:  3554.05 | norm: 1.10\n",
            "step874 | loss: 4.421480655670166 | dt: 2305.99ms | tok/sec:  3552.49 | norm: 1.06\n",
            "step875 | loss: 4.411762237548828 | dt: 2307.32ms | tok/sec:  3550.44 | norm: 1.11\n",
            "step876 | loss: 4.149191856384277 | dt: 2306.34ms | tok/sec:  3551.95 | norm: 1.08\n",
            "step877 | loss: 4.3114118576049805 | dt: 2306.73ms | tok/sec:  3551.35 | norm: 1.02\n",
            "step878 | loss: 4.426447868347168 | dt: 2305.73ms | tok/sec:  3552.88 | norm: 1.06\n",
            "step879 | loss: 4.311849594116211 | dt: 2310.58ms | tok/sec:  3545.44 | norm: 1.05\n",
            "step880 | loss: 4.225450038909912 | dt: 2308.92ms | tok/sec:  3547.98 | norm: 0.99\n",
            "step881 | loss: 4.3151984214782715 | dt: 2304.58ms | tok/sec:  3554.66 | norm: 1.08\n",
            "step882 | loss: 4.160793781280518 | dt: 2301.68ms | tok/sec:  3559.13 | norm: 1.11\n",
            "step883 | loss: 4.352362632751465 | dt: 2311.33ms | tok/sec:  3544.28 | norm: 0.97\n",
            "step884 | loss: 4.079080581665039 | dt: 2298.04ms | tok/sec:  3564.77 | norm: 0.96\n",
            "step885 | loss: 4.135173320770264 | dt: 2289.85ms | tok/sec:  3577.53 | norm: 0.96\n",
            "step886 | loss: 4.118033409118652 | dt: 2289.27ms | tok/sec:  3578.43 | norm: 0.79\n",
            "step887 | loss: 4.459343910217285 | dt: 2299.54ms | tok/sec:  3562.45 | norm: 1.04\n",
            "step888 | loss: 4.431703567504883 | dt: 2291.10ms | tok/sec:  3575.57 | norm: 1.11\n",
            "step889 | loss: 4.659277439117432 | dt: 2302.89ms | tok/sec:  3557.28 | norm: 0.98\n",
            "step890 | loss: 4.343227863311768 | dt: 2287.65ms | tok/sec:  3580.96 | norm: 0.90\n",
            "step891 | loss: 4.381894111633301 | dt: 2287.48ms | tok/sec:  3581.23 | norm: 0.88\n",
            "step892 | loss: 4.169342994689941 | dt: 2307.53ms | tok/sec:  3550.11 | norm: 1.03\n",
            "step893 | loss: 4.138832092285156 | dt: 2294.72ms | tok/sec:  3569.94 | norm: 0.96\n",
            "step894 | loss: 4.237609386444092 | dt: 2306.74ms | tok/sec:  3551.33 | norm: 1.06\n",
            "step895 | loss: 4.034025192260742 | dt: 2300.12ms | tok/sec:  3561.55 | norm: 1.04\n",
            "step896 | loss: 4.170960426330566 | dt: 2290.07ms | tok/sec:  3577.18 | norm: 0.95\n",
            "step897 | loss: 4.188391208648682 | dt: 2306.61ms | tok/sec:  3551.53 | norm: 1.05\n",
            "step898 | loss: 4.124619960784912 | dt: 2308.89ms | tok/sec:  3548.02 | norm: 1.13\n",
            "step899 | loss: 3.9965338706970215 | dt: 2288.37ms | tok/sec:  3579.85 | norm: 1.18\n",
            "step900 | loss: 3.8672780990600586 | dt: 2286.93ms | tok/sec:  3582.10 | norm: 1.11\n",
            "step901 | loss: 4.442645072937012 | dt: 2306.62ms | tok/sec:  3551.52 | norm: 1.17\n",
            "step902 | loss: 4.416510105133057 | dt: 2301.04ms | tok/sec:  3560.12 | norm: 1.24\n",
            "step903 | loss: 4.424991607666016 | dt: 2290.76ms | tok/sec:  3576.11 | norm: 1.13\n",
            "step904 | loss: 4.127256870269775 | dt: 2305.19ms | tok/sec:  3553.72 | norm: 1.20\n",
            "step905 | loss: 4.045323848724365 | dt: 2304.88ms | tok/sec:  3554.20 | norm: 1.02\n",
            "step906 | loss: 4.1087188720703125 | dt: 2290.68ms | tok/sec:  3576.23 | norm: 1.11\n",
            "step907 | loss: 4.321613788604736 | dt: 2305.40ms | tok/sec:  3553.40 | norm: 1.18\n",
            "step908 | loss: 4.206920146942139 | dt: 2300.18ms | tok/sec:  3561.47 | norm: 1.28\n",
            "step909 | loss: 4.245839595794678 | dt: 2297.21ms | tok/sec:  3566.06 | norm: 1.15\n",
            "step910 | loss: 4.01225471496582 | dt: 2301.95ms | tok/sec:  3558.73 | norm: 1.06\n",
            "step911 | loss: 4.198150157928467 | dt: 2303.96ms | tok/sec:  3555.61 | norm: 1.00\n",
            "step912 | loss: 4.12332820892334 | dt: 2303.80ms | tok/sec:  3555.87 | norm: 1.32\n",
            "step913 | loss: 4.393115520477295 | dt: 2285.82ms | tok/sec:  3583.83 | norm: 1.23\n",
            "step914 | loss: 4.516831398010254 | dt: 2292.36ms | tok/sec:  3573.61 | norm: 1.26\n",
            "step915 | loss: 4.391524314880371 | dt: 2301.45ms | tok/sec:  3559.50 | norm: 1.20\n",
            "step916 | loss: 4.400265693664551 | dt: 2305.36ms | tok/sec:  3553.47 | norm: 1.14\n",
            "step917 | loss: 4.113411903381348 | dt: 2295.20ms | tok/sec:  3569.18 | norm: 1.13\n",
            "step918 | loss: 4.246509075164795 | dt: 2298.58ms | tok/sec:  3563.94 | norm: 1.02\n",
            "step919 | loss: 4.324458599090576 | dt: 2289.37ms | tok/sec:  3578.28 | norm: 1.01\n",
            "step920 | loss: 4.234099388122559 | dt: 2304.63ms | tok/sec:  3554.58 | norm: 1.14\n",
            "step921 | loss: 4.178236484527588 | dt: 2286.43ms | tok/sec:  3582.88 | norm: 1.51\n",
            "step922 | loss: 4.302627086639404 | dt: 2289.18ms | tok/sec:  3578.57 | norm: 1.13\n",
            "step923 | loss: 4.227274417877197 | dt: 2288.29ms | tok/sec:  3579.96 | norm: 1.27\n",
            "step924 | loss: 4.419629096984863 | dt: 2292.52ms | tok/sec:  3573.35 | norm: 1.21\n",
            "step925 | loss: 4.121997833251953 | dt: 2303.45ms | tok/sec:  3556.40 | norm: 1.10\n",
            "step926 | loss: 4.149446487426758 | dt: 2307.57ms | tok/sec:  3550.06 | norm: 1.15\n",
            "step927 | loss: 4.115288257598877 | dt: 2307.06ms | tok/sec:  3550.84 | norm: 1.12\n",
            "step928 | loss: 4.456670761108398 | dt: 2289.18ms | tok/sec:  3578.57 | norm: 1.30\n",
            "step929 | loss: 4.415912628173828 | dt: 2285.31ms | tok/sec:  3584.64 | norm: 1.12\n",
            "step930 | loss: 4.648735046386719 | dt: 2298.97ms | tok/sec:  3563.34 | norm: 1.11\n",
            "step931 | loss: 4.319321632385254 | dt: 2292.46ms | tok/sec:  3573.45 | norm: 1.05\n",
            "step932 | loss: 4.368415832519531 | dt: 2285.83ms | tok/sec:  3583.82 | norm: 0.95\n",
            "step933 | loss: 4.1338114738464355 | dt: 2303.86ms | tok/sec:  3555.77 | norm: 1.00\n",
            "step934 | loss: 4.0509867668151855 | dt: 2282.64ms | tok/sec:  3588.83 | norm: 0.95\n",
            "step935 | loss: 4.151813983917236 | dt: 2290.47ms | tok/sec:  3576.56 | norm: 0.99\n",
            "step936 | loss: 3.985325813293457 | dt: 2287.18ms | tok/sec:  3581.70 | norm: 0.91\n",
            "step937 | loss: 4.106934547424316 | dt: 2287.84ms | tok/sec:  3580.67 | norm: 0.82\n",
            "step938 | loss: 4.06360387802124 | dt: 2293.83ms | tok/sec:  3571.32 | norm: 0.90\n",
            "step939 | loss: 4.000078201293945 | dt: 2300.75ms | tok/sec:  3560.57 | norm: 0.92\n",
            "step940 | loss: 3.8728244304656982 | dt: 2302.41ms | tok/sec:  3558.02 | norm: 0.85\n",
            "step941 | loss: 3.7727253437042236 | dt: 2305.07ms | tok/sec:  3553.90 | norm: 0.89\n",
            "step942 | loss: 4.373008728027344 | dt: 2286.62ms | tok/sec:  3582.58 | norm: 1.13\n",
            "step943 | loss: 4.370673656463623 | dt: 2297.60ms | tok/sec:  3565.46 | norm: 1.09\n",
            "step944 | loss: 4.341773986816406 | dt: 2291.95ms | tok/sec:  3574.25 | norm: 1.08\n",
            "step945 | loss: 4.06616735458374 | dt: 2292.54ms | tok/sec:  3573.33 | norm: 1.13\n",
            "step946 | loss: 4.0223388671875 | dt: 2299.96ms | tok/sec:  3561.80 | norm: 1.18\n",
            "step947 | loss: 4.103090286254883 | dt: 2299.10ms | tok/sec:  3563.13 | norm: 1.11\n",
            "step948 | loss: 4.291822910308838 | dt: 2292.56ms | tok/sec:  3573.30 | norm: 1.02\n",
            "step949 | loss: 4.169439792633057 | dt: 2289.76ms | tok/sec:  3577.66 | norm: 1.21\n",
            "step950 | loss: 4.185662269592285 | dt: 2290.58ms | tok/sec:  3576.38 | norm: 1.09\n",
            "step951 | loss: 3.95711088180542 | dt: 2297.51ms | tok/sec:  3565.61 | norm: 1.17\n",
            "step952 | loss: 4.1454925537109375 | dt: 2288.85ms | tok/sec:  3579.09 | norm: 1.13\n",
            "step953 | loss: 4.0771098136901855 | dt: 2288.87ms | tok/sec:  3579.07 | norm: 1.20\n",
            "step954 | loss: 4.3535966873168945 | dt: 2297.78ms | tok/sec:  3565.18 | norm: 1.04\n",
            "step955 | loss: 4.4236836433410645 | dt: 2290.89ms | tok/sec:  3575.91 | norm: 0.98\n",
            "step956 | loss: 4.308115005493164 | dt: 2295.36ms | tok/sec:  3568.93 | norm: 1.09\n",
            "step957 | loss: 4.301920413970947 | dt: 2290.18ms | tok/sec:  3577.01 | norm: 1.18\n",
            "step958 | loss: 4.053255081176758 | dt: 2304.91ms | tok/sec:  3554.15 | norm: 1.40\n",
            "step959 | loss: 4.184039115905762 | dt: 2287.59ms | tok/sec:  3581.05 | norm: 1.18\n",
            "step960 | loss: 4.250932693481445 | dt: 2292.25ms | tok/sec:  3573.78 | norm: 1.06\n",
            "step961 | loss: 4.174666404724121 | dt: 2294.38ms | tok/sec:  3570.46 | norm: 1.11\n",
            "step962 | loss: 4.1494903564453125 | dt: 2291.82ms | tok/sec:  3574.46 | norm: 1.22\n",
            "step963 | loss: 4.20896053314209 | dt: 2285.42ms | tok/sec:  3584.46 | norm: 1.10\n",
            "step964 | loss: 4.111074924468994 | dt: 2294.94ms | tok/sec:  3569.60 | norm: 1.08\n",
            "step965 | loss: 4.308154582977295 | dt: 2296.96ms | tok/sec:  3566.46 | norm: 1.11\n",
            "step966 | loss: 4.038153171539307 | dt: 2304.37ms | tok/sec:  3554.98 | norm: 1.01\n",
            "step967 | loss: 4.099040985107422 | dt: 2304.52ms | tok/sec:  3554.75 | norm: 1.03\n",
            "step968 | loss: 4.090198516845703 | dt: 2306.99ms | tok/sec:  3550.94 | norm: 1.10\n",
            "step969 | loss: 4.4154791831970215 | dt: 2302.23ms | tok/sec:  3558.29 | norm: 1.40\n",
            "step970 | loss: 4.386600494384766 | dt: 2288.05ms | tok/sec:  3580.33 | norm: 1.43\n",
            "step971 | loss: 4.579163074493408 | dt: 2306.62ms | tok/sec:  3551.51 | norm: 1.24\n",
            "step972 | loss: 4.2793402671813965 | dt: 2306.83ms | tok/sec:  3551.20 | norm: 1.13\n",
            "step973 | loss: 4.311212062835693 | dt: 2307.55ms | tok/sec:  3550.08 | norm: 0.95\n",
            "step974 | loss: 4.092278003692627 | dt: 2296.52ms | tok/sec:  3567.14 | norm: 1.04\n",
            "step975 | loss: 4.034930229187012 | dt: 2294.53ms | tok/sec:  3570.23 | norm: 0.99\n",
            "step976 | loss: 4.108638763427734 | dt: 2287.49ms | tok/sec:  3581.22 | norm: 1.06\n",
            "step977 | loss: 3.965794324874878 | dt: 2296.01ms | tok/sec:  3567.93 | norm: 0.96\n",
            "step978 | loss: 4.086893558502197 | dt: 2295.78ms | tok/sec:  3568.29 | norm: 0.97\n",
            "step979 | loss: 4.042834758758545 | dt: 2290.54ms | tok/sec:  3576.44 | norm: 1.11\n",
            "step980 | loss: 3.9618353843688965 | dt: 2300.84ms | tok/sec:  3560.43 | norm: 0.95\n",
            "step981 | loss: 3.8429384231567383 | dt: 2292.57ms | tok/sec:  3573.28 | norm: 0.90\n",
            "step982 | loss: 3.7113852500915527 | dt: 2307.60ms | tok/sec:  3550.00 | norm: 0.87\n",
            "step983 | loss: 4.320889949798584 | dt: 2305.87ms | tok/sec:  3552.67 | norm: 1.16\n",
            "step984 | loss: 4.301190376281738 | dt: 2309.71ms | tok/sec:  3546.76 | norm: 1.26\n",
            "step985 | loss: 4.282918930053711 | dt: 2304.58ms | tok/sec:  3554.65 | norm: 1.29\n",
            "step986 | loss: 4.009130001068115 | dt: 2291.52ms | tok/sec:  3574.92 | norm: 1.26\n",
            "step987 | loss: 3.954073667526245 | dt: 2305.39ms | tok/sec:  3553.41 | norm: 0.96\n",
            "step988 | loss: 4.038329601287842 | dt: 2307.04ms | tok/sec:  3550.86 | norm: 1.17\n",
            "step989 | loss: 4.202874660491943 | dt: 2306.71ms | tok/sec:  3551.37 | norm: 1.11\n",
            "step990 | loss: 4.081968784332275 | dt: 2306.19ms | tok/sec:  3552.18 | norm: 1.30\n",
            "step991 | loss: 4.143943786621094 | dt: 2290.73ms | tok/sec:  3576.15 | norm: 1.30\n",
            "step992 | loss: 3.89019775390625 | dt: 2310.17ms | tok/sec:  3546.06 | norm: 1.11\n",
            "step993 | loss: 4.056742191314697 | dt: 2303.92ms | tok/sec:  3555.68 | norm: 0.91\n",
            "step994 | loss: 3.983905076980591 | dt: 2308.18ms | tok/sec:  3549.12 | norm: 1.10\n",
            "step995 | loss: 4.252892017364502 | dt: 2301.96ms | tok/sec:  3558.71 | norm: 0.94\n",
            "step996 | loss: 4.3056464195251465 | dt: 2309.44ms | tok/sec:  3547.18 | norm: 1.01\n",
            "step997 | loss: 4.233465194702148 | dt: 2309.75ms | tok/sec:  3546.71 | norm: 1.02\n",
            "step998 | loss: 4.243231773376465 | dt: 2306.41ms | tok/sec:  3551.83 | norm: 1.01\n",
            "step999 | loss: 4.0199174880981445 | dt: 2301.53ms | tok/sec:  3559.38 | norm: 1.02\n",
            "step1000 | loss: 4.160107612609863 | dt: 2313.44ms | tok/sec:  3541.05 | norm: 1.06\n",
            "step1001 | loss: 4.221234321594238 | dt: 2302.77ms | tok/sec:  3557.46 | norm: 1.05\n",
            "step1002 | loss: 4.126731872558594 | dt: 2304.34ms | tok/sec:  3555.03 | norm: 1.06\n",
            "step1003 | loss: 4.071261882781982 | dt: 2301.87ms | tok/sec:  3558.85 | norm: 1.11\n",
            "step1004 | loss: 4.156951904296875 | dt: 2305.91ms | tok/sec:  3552.61 | norm: 1.09\n",
            "step1005 | loss: 4.061639785766602 | dt: 2301.14ms | tok/sec:  3559.97 | norm: 1.24\n",
            "step1006 | loss: 4.239048480987549 | dt: 2288.61ms | tok/sec:  3579.46 | norm: 1.12\n",
            "step1007 | loss: 3.957841157913208 | dt: 2299.65ms | tok/sec:  3562.29 | norm: 1.06\n",
            "step1008 | loss: 4.015157222747803 | dt: 2294.59ms | tok/sec:  3570.14 | norm: 0.95\n",
            "step1009 | loss: 4.016993522644043 | dt: 2305.21ms | tok/sec:  3553.68 | norm: 0.91\n",
            "step1010 | loss: 4.326251983642578 | dt: 2290.53ms | tok/sec:  3576.47 | norm: 1.25\n",
            "step1011 | loss: 4.324209690093994 | dt: 2288.59ms | tok/sec:  3579.50 | norm: 1.40\n",
            "step1012 | loss: 4.504650592803955 | dt: 2295.62ms | tok/sec:  3568.53 | norm: 1.35\n",
            "step1013 | loss: 4.199865341186523 | dt: 2303.82ms | tok/sec:  3555.83 | norm: 1.34\n",
            "step1014 | loss: 4.255958080291748 | dt: 2308.53ms | tok/sec:  3548.58 | norm: 1.22\n",
            "step1015 | loss: 4.023671627044678 | dt: 2299.30ms | tok/sec:  3562.83 | norm: 1.35\n",
            "step1016 | loss: 3.979377269744873 | dt: 2296.73ms | tok/sec:  3566.82 | norm: 1.24\n",
            "step1017 | loss: 4.08045768737793 | dt: 2289.53ms | tok/sec:  3578.02 | norm: 1.26\n",
            "step1018 | loss: 3.9119458198547363 | dt: 2300.91ms | tok/sec:  3560.34 | norm: 1.11\n",
            "step1019 | loss: 4.031219959259033 | dt: 2306.67ms | tok/sec:  3551.44 | norm: 1.01\n",
            "step1020 | loss: 4.004566669464111 | dt: 2305.25ms | tok/sec:  3553.63 | norm: 1.14\n",
            "step1021 | loss: 3.902486801147461 | dt: 2305.31ms | tok/sec:  3553.53 | norm: 1.06\n",
            "step1022 | loss: 3.7798960208892822 | dt: 2303.56ms | tok/sec:  3556.23 | norm: 0.95\n",
            "step1023 | loss: 3.668973445892334 | dt: 2295.35ms | tok/sec:  3568.96 | norm: 0.89\n",
            "step1024 | loss: 4.239543437957764 | dt: 2299.86ms | tok/sec:  3561.95 | norm: 1.07\n",
            "step1025 | loss: 4.2455549240112305 | dt: 2298.20ms | tok/sec:  3564.52 | norm: 1.03\n",
            "step1026 | loss: 4.223167419433594 | dt: 2293.56ms | tok/sec:  3571.75 | norm: 1.02\n",
            "step1027 | loss: 3.954730749130249 | dt: 2305.42ms | tok/sec:  3553.36 | norm: 1.23\n",
            "step1028 | loss: 3.8986494541168213 | dt: 2305.37ms | tok/sec:  3553.44 | norm: 1.26\n",
            "step1029 | loss: 3.992269992828369 | dt: 2311.42ms | tok/sec:  3544.15 | norm: 1.26\n",
            "step1030 | loss: 4.1674580574035645 | dt: 2306.69ms | tok/sec:  3551.41 | norm: 1.23\n",
            "step1031 | loss: 4.042506217956543 | dt: 2307.58ms | tok/sec:  3550.04 | norm: 1.24\n",
            "step1032 | loss: 4.058654308319092 | dt: 2302.47ms | tok/sec:  3557.91 | norm: 1.07\n",
            "step1033 | loss: 3.805178642272949 | dt: 2307.28ms | tok/sec:  3550.49 | norm: 0.94\n",
            "step1034 | loss: 3.9784934520721436 | dt: 2304.46ms | tok/sec:  3554.85 | norm: 0.96\n",
            "step1035 | loss: 3.875084400177002 | dt: 2306.41ms | tok/sec:  3551.85 | norm: 1.15\n",
            "step1036 | loss: 4.180270195007324 | dt: 2299.19ms | tok/sec:  3563.00 | norm: 1.26\n",
            "step1037 | loss: 4.278735637664795 | dt: 2292.38ms | tok/sec:  3573.58 | norm: 1.39\n",
            "step1038 | loss: 4.199331760406494 | dt: 2293.58ms | tok/sec:  3571.71 | norm: 1.15\n",
            "step1039 | loss: 4.175717830657959 | dt: 2303.15ms | tok/sec:  3556.87 | norm: 1.11\n",
            "step1040 | loss: 3.9141440391540527 | dt: 2313.76ms | tok/sec:  3540.55 | norm: 0.98\n",
            "step1041 | loss: 4.074719429016113 | dt: 2305.59ms | tok/sec:  3553.10 | norm: 1.04\n",
            "step1042 | loss: 4.163700103759766 | dt: 2304.10ms | tok/sec:  3555.40 | norm: 1.05\n",
            "step1043 | loss: 4.082521915435791 | dt: 2305.04ms | tok/sec:  3553.95 | norm: 1.10\n",
            "step1044 | loss: 4.007304668426514 | dt: 2305.21ms | tok/sec:  3553.68 | norm: 1.14\n",
            "step1045 | loss: 4.095407009124756 | dt: 2290.46ms | tok/sec:  3576.57 | norm: 1.06\n",
            "step1046 | loss: 4.026165962219238 | dt: 2291.89ms | tok/sec:  3574.34 | norm: 1.19\n",
            "step1047 | loss: 4.205225467681885 | dt: 2303.73ms | tok/sec:  3555.96 | norm: 0.95\n",
            "step1048 | loss: 3.901754379272461 | dt: 2296.05ms | tok/sec:  3567.87 | norm: 1.07\n",
            "step1049 | loss: 3.9436914920806885 | dt: 2299.52ms | tok/sec:  3562.48 | norm: 1.04\n",
            "step1050 | loss: 3.9442925453186035 | dt: 2297.56ms | tok/sec:  3565.53 | norm: 1.03\n",
            "step1051 | loss: 4.289533615112305 | dt: 2292.47ms | tok/sec:  3573.43 | norm: 1.27\n",
            "step1052 | loss: 4.276508331298828 | dt: 2300.08ms | tok/sec:  3561.61 | norm: 1.10\n",
            "step1053 | loss: 4.462847709655762 | dt: 2305.50ms | tok/sec:  3553.24 | norm: 1.10\n",
            "step1054 | loss: 4.164111614227295 | dt: 2308.36ms | tok/sec:  3548.83 | norm: 1.39\n",
            "step1055 | loss: 4.2222981452941895 | dt: 2306.96ms | tok/sec:  3550.99 | norm: 1.35\n",
            "step1056 | loss: 4.007760047912598 | dt: 2303.86ms | tok/sec:  3555.78 | norm: 1.47\n",
            "step1057 | loss: 4.019925594329834 | dt: 2306.71ms | tok/sec:  3551.38 | norm: 1.58\n",
            "step1058 | loss: 4.041740417480469 | dt: 2294.69ms | tok/sec:  3569.98 | norm: 1.34\n",
            "step1059 | loss: 3.925081253051758 | dt: 2298.13ms | tok/sec:  3564.64 | norm: 1.17\n",
            "step1060 | loss: 4.017978191375732 | dt: 2301.51ms | tok/sec:  3559.41 | norm: 1.07\n",
            "step1061 | loss: 3.9878993034362793 | dt: 2289.89ms | tok/sec:  3577.46 | norm: 1.12\n",
            "step1062 | loss: 3.885335683822632 | dt: 2304.33ms | tok/sec:  3555.04 | norm: 1.18\n",
            "step1063 | loss: 3.762816905975342 | dt: 2302.85ms | tok/sec:  3557.33 | norm: 1.17\n",
            "step1064 | loss: 3.6322474479675293 | dt: 2298.15ms | tok/sec:  3564.61 | norm: 1.12\n",
            "step1065 | loss: 4.170557022094727 | dt: 2296.20ms | tok/sec:  3567.64 | norm: 1.19\n",
            "step1066 | loss: 4.176162242889404 | dt: 2305.51ms | tok/sec:  3553.22 | norm: 1.34\n",
            "step1067 | loss: 4.176558971405029 | dt: 2293.01ms | tok/sec:  3572.59 | norm: 1.26\n",
            "step1068 | loss: 3.9096126556396484 | dt: 2295.81ms | tok/sec:  3568.25 | norm: 1.24\n",
            "step1069 | loss: 3.8658242225646973 | dt: 2293.17ms | tok/sec:  3572.35 | norm: 0.98\n",
            "step1070 | loss: 3.9441280364990234 | dt: 2290.05ms | tok/sec:  3577.21 | norm: 1.05\n",
            "step1071 | loss: 4.098872184753418 | dt: 2288.79ms | tok/sec:  3579.19 | norm: 1.11\n",
            "step1072 | loss: 4.013568878173828 | dt: 2306.42ms | tok/sec:  3551.83 | norm: 1.43\n",
            "step1073 | loss: 4.03942346572876 | dt: 2289.63ms | tok/sec:  3577.88 | norm: 1.32\n",
            "step1074 | loss: 3.7954444885253906 | dt: 2293.42ms | tok/sec:  3571.96 | norm: 1.16\n",
            "step1075 | loss: 3.9728214740753174 | dt: 2302.35ms | tok/sec:  3558.10 | norm: 1.14\n",
            "step1076 | loss: 3.8866238594055176 | dt: 2305.30ms | tok/sec:  3553.54 | norm: 1.13\n",
            "step1077 | loss: 4.1600260734558105 | dt: 2286.53ms | tok/sec:  3582.72 | norm: 1.01\n",
            "step1078 | loss: 4.26984977722168 | dt: 2287.16ms | tok/sec:  3581.73 | norm: 0.99\n",
            "step1079 | loss: 4.134702205657959 | dt: 2301.22ms | tok/sec:  3559.85 | norm: 1.01\n",
            "step1080 | loss: 4.105805397033691 | dt: 2308.93ms | tok/sec:  3547.96 | norm: 1.22\n",
            "step1081 | loss: 3.861842155456543 | dt: 2285.89ms | tok/sec:  3583.72 | norm: 1.14\n",
            "step1082 | loss: 4.040215492248535 | dt: 2287.20ms | tok/sec:  3581.67 | norm: 1.29\n",
            "step1083 | loss: 4.114558696746826 | dt: 2285.03ms | tok/sec:  3585.07 | norm: 1.20\n",
            "step1084 | loss: 4.029317378997803 | dt: 2289.99ms | tok/sec:  3577.30 | norm: 1.24\n",
            "step1085 | loss: 3.9716286659240723 | dt: 2299.34ms | tok/sec:  3562.77 | norm: 1.08\n",
            "step1086 | loss: 4.027870178222656 | dt: 2288.14ms | tok/sec:  3580.20 | norm: 0.99\n",
            "step1087 | loss: 3.9653432369232178 | dt: 2305.44ms | tok/sec:  3553.33 | norm: 1.09\n",
            "step1088 | loss: 4.160100936889648 | dt: 2304.52ms | tok/sec:  3554.76 | norm: 1.07\n",
            "step1089 | loss: 3.8487067222595215 | dt: 2287.95ms | tok/sec:  3580.50 | norm: 1.09\n",
            "step1090 | loss: 3.883918046951294 | dt: 2305.54ms | tok/sec:  3553.17 | norm: 1.09\n",
            "step1091 | loss: 3.8955392837524414 | dt: 2286.17ms | tok/sec:  3583.28 | norm: 0.95\n",
            "step1092 | loss: 4.268080234527588 | dt: 2287.20ms | tok/sec:  3581.67 | norm: 1.21\n",
            "step1093 | loss: 4.2289581298828125 | dt: 2293.06ms | tok/sec:  3572.51 | norm: 1.15\n",
            "step1094 | loss: 4.44915771484375 | dt: 2300.90ms | tok/sec:  3560.35 | norm: 1.28\n",
            "step1095 | loss: 4.14562463760376 | dt: 2285.69ms | tok/sec:  3584.04 | norm: 1.13\n",
            "step1096 | loss: 4.185483455657959 | dt: 2288.43ms | tok/sec:  3579.75 | norm: 1.01\n",
            "step1097 | loss: 3.9718148708343506 | dt: 2298.65ms | tok/sec:  3563.83 | norm: 1.17\n",
            "step1098 | loss: 3.9470505714416504 | dt: 2297.28ms | tok/sec:  3565.95 | norm: 1.13\n",
            "step1099 | loss: 3.959804058074951 | dt: 2285.33ms | tok/sec:  3584.60 | norm: 1.17\n",
            "step1100 | loss: 3.806194543838501 | dt: 2282.37ms | tok/sec:  3589.26 | norm: 1.18\n",
            "step1101 | loss: 3.9238593578338623 | dt: 2294.11ms | tok/sec:  3570.88 | norm: 1.09\n",
            "step1102 | loss: 3.8657281398773193 | dt: 2297.03ms | tok/sec:  3566.34 | norm: 1.16\n",
            "step1103 | loss: 3.788282871246338 | dt: 2291.97ms | tok/sec:  3574.22 | norm: 1.10\n",
            "step1104 | loss: 3.671229362487793 | dt: 2290.35ms | tok/sec:  3576.74 | norm: 1.04\n",
            "step1105 | loss: 3.5720741748809814 | dt: 2287.75ms | tok/sec:  3580.82 | norm: 1.06\n",
            "step1106 | loss: 4.161736965179443 | dt: 2286.14ms | tok/sec:  3583.33 | norm: 1.37\n",
            "step1107 | loss: 4.119817733764648 | dt: 2284.15ms | tok/sec:  3586.46 | norm: 1.24\n",
            "step1108 | loss: 4.08652925491333 | dt: 2289.28ms | tok/sec:  3578.42 | norm: 1.10\n",
            "step1109 | loss: 3.820319890975952 | dt: 2295.27ms | tok/sec:  3569.07 | norm: 1.17\n",
            "step1110 | loss: 3.7584388256073 | dt: 2294.00ms | tok/sec:  3571.05 | norm: 1.09\n",
            "step1111 | loss: 3.833756446838379 | dt: 2287.75ms | tok/sec:  3580.81 | norm: 1.07\n",
            "step1112 | loss: 4.01695442199707 | dt: 2286.84ms | tok/sec:  3582.24 | norm: 1.20\n",
            "step1113 | loss: 3.9521751403808594 | dt: 2284.97ms | tok/sec:  3585.17 | norm: 1.29\n",
            "step1114 | loss: 3.9593920707702637 | dt: 2279.79ms | tok/sec:  3593.31 | norm: 1.13\n",
            "step1115 | loss: 3.7085812091827393 | dt: 2294.40ms | tok/sec:  3570.43 | norm: 0.95\n",
            "step1116 | loss: 3.886707305908203 | dt: 2287.56ms | tok/sec:  3581.11 | norm: 0.97\n",
            "step1117 | loss: 3.7882473468780518 | dt: 2284.56ms | tok/sec:  3585.81 | norm: 1.16\n",
            "step1118 | loss: 4.048989295959473 | dt: 2288.69ms | tok/sec:  3579.34 | norm: 1.22\n",
            "step1119 | loss: 4.139927864074707 | dt: 2300.39ms | tok/sec:  3561.13 | norm: 1.08\n",
            "step1120 | loss: 4.039453983306885 | dt: 2305.91ms | tok/sec:  3552.61 | norm: 1.09\n",
            "step1121 | loss: 4.050309181213379 | dt: 2287.92ms | tok/sec:  3580.54 | norm: 1.05\n",
            "step1122 | loss: 3.8179385662078857 | dt: 2287.65ms | tok/sec:  3580.97 | norm: 1.07\n",
            "step1123 | loss: 3.991732120513916 | dt: 2302.35ms | tok/sec:  3558.11 | norm: 1.15\n",
            "step1124 | loss: 4.057784557342529 | dt: 2286.25ms | tok/sec:  3583.16 | norm: 1.16\n",
            "step1125 | loss: 3.959817409515381 | dt: 2288.49ms | tok/sec:  3579.66 | norm: 0.98\n",
            "step1126 | loss: 3.8809282779693604 | dt: 2285.30ms | tok/sec:  3584.65 | norm: 1.05\n",
            "step1127 | loss: 3.942901372909546 | dt: 2282.35ms | tok/sec:  3589.29 | norm: 1.09\n",
            "step1128 | loss: 3.8920376300811768 | dt: 2292.73ms | tok/sec:  3573.03 | norm: 1.22\n",
            "step1129 | loss: 4.082817554473877 | dt: 2294.94ms | tok/sec:  3569.59 | norm: 1.23\n",
            "step1130 | loss: 3.8119475841522217 | dt: 2286.07ms | tok/sec:  3583.44 | norm: 1.17\n",
            "step1131 | loss: 3.8493189811706543 | dt: 2281.25ms | tok/sec:  3591.01 | norm: 1.09\n",
            "step1132 | loss: 3.862016439437866 | dt: 2297.22ms | tok/sec:  3566.05 | norm: 1.05\n",
            "step1133 | loss: 4.185514450073242 | dt: 2279.62ms | tok/sec:  3593.59 | norm: 1.36\n",
            "step1134 | loss: 4.188313961029053 | dt: 2290.07ms | tok/sec:  3577.18 | norm: 1.45\n",
            "step1135 | loss: 4.3712263107299805 | dt: 2292.41ms | tok/sec:  3573.53 | norm: 1.35\n",
            "step1136 | loss: 4.082266807556152 | dt: 2291.80ms | tok/sec:  3574.49 | norm: 1.38\n",
            "step1137 | loss: 4.134967803955078 | dt: 2291.96ms | tok/sec:  3574.24 | norm: 1.27\n",
            "step1138 | loss: 3.9239213466644287 | dt: 2295.21ms | tok/sec:  3569.16 | norm: 1.22\n",
            "step1139 | loss: 3.8810698986053467 | dt: 2285.89ms | tok/sec:  3583.72 | norm: 1.10\n",
            "step1140 | loss: 3.902182102203369 | dt: 2286.25ms | tok/sec:  3583.16 | norm: 1.16\n",
            "step1141 | loss: 3.7451446056365967 | dt: 2303.44ms | tok/sec:  3556.42 | norm: 1.01\n",
            "step1142 | loss: 3.8736519813537598 | dt: 2286.72ms | tok/sec:  3582.43 | norm: 1.04\n",
            "step1143 | loss: 3.798888683319092 | dt: 2287.02ms | tok/sec:  3581.96 | norm: 1.10\n",
            "step1144 | loss: 3.7280850410461426 | dt: 2283.89ms | tok/sec:  3586.86 | norm: 1.07\n",
            "step1145 | loss: 3.6098451614379883 | dt: 2300.94ms | tok/sec:  3560.28 | norm: 1.12\n",
            "step1146 | loss: 3.5068142414093018 | dt: 2292.45ms | tok/sec:  3573.47 | norm: 1.09\n",
            "step1147 | loss: 4.171079158782959 | dt: 2286.61ms | tok/sec:  3582.60 | norm: 1.57\n",
            "step1148 | loss: 4.15413236618042 | dt: 2284.59ms | tok/sec:  3585.77 | norm: 1.69\n",
            "step1149 | loss: 4.071777820587158 | dt: 2285.92ms | tok/sec:  3583.68 | norm: 1.33\n",
            "step1150 | loss: 3.8024532794952393 | dt: 2298.17ms | tok/sec:  3564.58 | norm: 1.31\n",
            "step1151 | loss: 3.7437376976013184 | dt: 2293.83ms | tok/sec:  3571.32 | norm: 1.09\n",
            "step1152 | loss: 3.809713363647461 | dt: 2286.19ms | tok/sec:  3583.26 | norm: 1.05\n",
            "step1153 | loss: 3.9788339138031006 | dt: 2286.98ms | tok/sec:  3582.01 | norm: 1.02\n",
            "step1154 | loss: 3.9226326942443848 | dt: 2287.15ms | tok/sec:  3581.76 | norm: 1.39\n",
            "step1155 | loss: 3.932199478149414 | dt: 2302.59ms | tok/sec:  3557.73 | norm: 1.53\n",
            "step1156 | loss: 3.7232038974761963 | dt: 2291.29ms | tok/sec:  3575.28 | norm: 1.58\n",
            "step1157 | loss: 3.875840663909912 | dt: 2288.79ms | tok/sec:  3579.18 | norm: 1.44\n",
            "step1158 | loss: 3.7859926223754883 | dt: 2305.58ms | tok/sec:  3553.12 | norm: 1.34\n",
            "step1159 | loss: 4.035125255584717 | dt: 2289.75ms | tok/sec:  3577.68 | norm: 1.16\n",
            "step1160 | loss: 4.066216945648193 | dt: 2292.93ms | tok/sec:  3572.72 | norm: 1.05\n",
            "step1161 | loss: 3.983293294906616 | dt: 2302.97ms | tok/sec:  3557.14 | norm: 1.25\n",
            "step1162 | loss: 4.00643253326416 | dt: 2291.53ms | tok/sec:  3574.90 | norm: 1.24\n",
            "step1163 | loss: 3.7820539474487305 | dt: 2304.46ms | tok/sec:  3554.85 | norm: 1.18\n",
            "step1164 | loss: 3.9976155757904053 | dt: 2292.42ms | tok/sec:  3573.52 | norm: 1.42\n",
            "step1165 | loss: 4.055527210235596 | dt: 2298.85ms | tok/sec:  3563.52 | norm: 1.30\n",
            "step1166 | loss: 3.9361517429351807 | dt: 2293.91ms | tok/sec:  3571.19 | norm: 1.31\n",
            "step1167 | loss: 3.86781907081604 | dt: 2299.71ms | tok/sec:  3562.18 | norm: 1.14\n",
            "step1168 | loss: 3.9603328704833984 | dt: 2309.05ms | tok/sec:  3547.78 | norm: 1.33\n",
            "step1169 | loss: 3.8760201930999756 | dt: 2302.30ms | tok/sec:  3558.19 | norm: 1.31\n",
            "step1170 | loss: 4.063068866729736 | dt: 2301.99ms | tok/sec:  3558.67 | norm: 1.17\n",
            "step1171 | loss: 3.768983840942383 | dt: 2286.52ms | tok/sec:  3582.73 | norm: 1.20\n",
            "step1172 | loss: 3.8241820335388184 | dt: 2290.50ms | tok/sec:  3576.52 | norm: 1.16\n",
            "step1173 | loss: 3.8074452877044678 | dt: 2301.88ms | tok/sec:  3558.83 | norm: 1.15\n",
            "step1174 | loss: 4.1157426834106445 | dt: 2293.47ms | tok/sec:  3571.88 | norm: 1.25\n",
            "step1175 | loss: 4.13050651550293 | dt: 2298.20ms | tok/sec:  3564.52 | norm: 1.24\n",
            "step1176 | loss: 4.290125370025635 | dt: 2286.83ms | tok/sec:  3582.25 | norm: 1.28\n",
            "step1177 | loss: 4.003389358520508 | dt: 2307.75ms | tok/sec:  3549.78 | norm: 1.26\n",
            "step1178 | loss: 4.080326080322266 | dt: 2286.46ms | tok/sec:  3582.82 | norm: 1.32\n",
            "step1179 | loss: 3.8401288986206055 | dt: 2309.34ms | tok/sec:  3547.33 | norm: 1.39\n",
            "step1180 | loss: 3.833240509033203 | dt: 2301.70ms | tok/sec:  3559.11 | norm: 1.38\n",
            "step1181 | loss: 3.8566741943359375 | dt: 2306.31ms | tok/sec:  3551.99 | norm: 1.33\n",
            "step1182 | loss: 3.7265443801879883 | dt: 2306.18ms | tok/sec:  3552.19 | norm: 1.37\n",
            "step1183 | loss: 3.876784324645996 | dt: 2306.10ms | tok/sec:  3552.32 | norm: 1.31\n",
            "step1184 | loss: 3.7860612869262695 | dt: 2293.91ms | tok/sec:  3571.20 | norm: 1.23\n",
            "step1185 | loss: 3.700456380844116 | dt: 2299.97ms | tok/sec:  3561.78 | norm: 1.17\n",
            "step1186 | loss: 3.5659594535827637 | dt: 2306.90ms | tok/sec:  3551.09 | norm: 1.13\n",
            "step1187 | loss: 3.4351603984832764 | dt: 2291.75ms | tok/sec:  3574.56 | norm: 1.07\n",
            "step1188 | loss: 4.093329906463623 | dt: 2309.79ms | tok/sec:  3546.64 | norm: 1.29\n",
            "step1189 | loss: 4.117168426513672 | dt: 2308.14ms | tok/sec:  3549.18 | norm: 1.56\n",
            "step1190 | loss: 4.079810619354248 | dt: 2308.80ms | tok/sec:  3548.16 | norm: 1.52\n",
            "step1191 | loss: 3.801225185394287 | dt: 2303.56ms | tok/sec:  3556.24 | norm: 1.38\n",
            "step1192 | loss: 3.713824510574341 | dt: 2290.13ms | tok/sec:  3577.09 | norm: 1.21\n",
            "step1193 | loss: 3.796468734741211 | dt: 2295.77ms | tok/sec:  3568.30 | norm: 1.27\n",
            "step1194 | loss: 3.9654219150543213 | dt: 2299.06ms | tok/sec:  3563.20 | norm: 1.25\n",
            "step1195 | loss: 3.891479730606079 | dt: 2305.72ms | tok/sec:  3552.90 | norm: 1.42\n",
            "step1196 | loss: 3.90262508392334 | dt: 2306.23ms | tok/sec:  3552.12 | norm: 1.24\n",
            "step1197 | loss: 3.687480926513672 | dt: 2288.69ms | tok/sec:  3579.34 | norm: 1.17\n",
            "step1198 | loss: 3.8393783569335938 | dt: 2295.64ms | tok/sec:  3568.51 | norm: 1.34\n",
            "step1199 | loss: 3.7813966274261475 | dt: 2296.72ms | tok/sec:  3566.82 | norm: 1.67\n",
            "step1200 | loss: 4.043402194976807 | dt: 2298.34ms | tok/sec:  3564.31 | norm: 1.73\n",
            "step1201 | loss: 4.071904182434082 | dt: 2292.04ms | tok/sec:  3574.11 | norm: 1.61\n",
            "step1202 | loss: 4.008413791656494 | dt: 2306.44ms | tok/sec:  3551.79 | norm: 1.38\n",
            "step1203 | loss: 4.001833438873291 | dt: 2305.71ms | tok/sec:  3552.91 | norm: 1.26\n",
            "step1204 | loss: 3.743757963180542 | dt: 2305.51ms | tok/sec:  3553.23 | norm: 1.10\n",
            "step1205 | loss: 3.949115753173828 | dt: 2307.98ms | tok/sec:  3549.43 | norm: 1.25\n",
            "step1206 | loss: 3.9996306896209717 | dt: 2288.39ms | tok/sec:  3579.81 | norm: 1.40\n",
            "step1207 | loss: 3.905381441116333 | dt: 2293.26ms | tok/sec:  3572.21 | norm: 1.33\n",
            "step1208 | loss: 3.824136972427368 | dt: 2297.42ms | tok/sec:  3565.73 | norm: 1.35\n",
            "step1209 | loss: 3.935302495956421 | dt: 2303.82ms | tok/sec:  3555.83 | norm: 1.35\n",
            "step1210 | loss: 3.863481044769287 | dt: 2299.88ms | tok/sec:  3561.92 | norm: 1.32\n",
            "step1211 | loss: 4.041208744049072 | dt: 2299.21ms | tok/sec:  3562.96 | norm: 1.34\n",
            "step1212 | loss: 3.7678210735321045 | dt: 2306.83ms | tok/sec:  3551.19 | norm: 1.38\n",
            "step1213 | loss: 3.7915570735931396 | dt: 2299.97ms | tok/sec:  3561.79 | norm: 1.28\n",
            "step1214 | loss: 3.7893662452697754 | dt: 2298.41ms | tok/sec:  3564.20 | norm: 1.46\n",
            "step1215 | loss: 4.075750350952148 | dt: 2296.00ms | tok/sec:  3567.94 | norm: 1.61\n",
            "step1216 | loss: 4.073053359985352 | dt: 2288.89ms | tok/sec:  3579.03 | norm: 1.41\n",
            "step1217 | loss: 4.223034858703613 | dt: 2293.00ms | tok/sec:  3572.62 | norm: 1.28\n",
            "step1218 | loss: 3.9162039756774902 | dt: 2295.03ms | tok/sec:  3569.46 | norm: 1.59\n",
            "step1219 | loss: 4.007511615753174 | dt: 2294.05ms | tok/sec:  3570.98 | norm: 1.14\n",
            "step1220 | loss: 3.7898316383361816 | dt: 2296.42ms | tok/sec:  3567.29 | norm: 1.46\n",
            "step1221 | loss: 3.8242123126983643 | dt: 2300.03ms | tok/sec:  3561.69 | norm: 1.46\n",
            "step1222 | loss: 3.809171676635742 | dt: 2306.83ms | tok/sec:  3551.20 | norm: 1.42\n",
            "step1223 | loss: 3.6978096961975098 | dt: 2296.03ms | tok/sec:  3567.90 | norm: 1.32\n",
            "step1224 | loss: 3.815431594848633 | dt: 2295.42ms | tok/sec:  3568.84 | norm: 1.28\n",
            "step1225 | loss: 3.7361083030700684 | dt: 2291.74ms | tok/sec:  3574.58 | norm: 1.69\n",
            "step1226 | loss: 3.658917188644409 | dt: 2306.23ms | tok/sec:  3552.11 | norm: 1.31\n",
            "step1227 | loss: 3.5382401943206787 | dt: 2299.62ms | tok/sec:  3562.33 | norm: 1.60\n",
            "step1228 | loss: 3.428929567337036 | dt: 2304.63ms | tok/sec:  3554.59 | norm: 1.34\n",
            "step1229 | loss: 4.059868812561035 | dt: 2291.18ms | tok/sec:  3575.45 | norm: 1.49\n",
            "step1230 | loss: 4.079405784606934 | dt: 2301.74ms | tok/sec:  3559.04 | norm: 1.50\n",
            "step1231 | loss: 4.034646034240723 | dt: 2307.19ms | tok/sec:  3550.65 | norm: 1.30\n",
            "step1232 | loss: 3.7186028957366943 | dt: 2285.35ms | tok/sec:  3584.56 | norm: 1.11\n",
            "step1233 | loss: 3.6320958137512207 | dt: 2287.53ms | tok/sec:  3581.15 | norm: 1.04\n",
            "step1234 | loss: 3.7204220294952393 | dt: 2290.42ms | tok/sec:  3576.63 | norm: 1.30\n",
            "step1235 | loss: 3.897794008255005 | dt: 2303.19ms | tok/sec:  3556.81 | norm: 1.38\n",
            "step1236 | loss: 3.8561437129974365 | dt: 2288.76ms | tok/sec:  3579.23 | norm: 1.76\n",
            "step1237 | loss: 3.8990557193756104 | dt: 2297.85ms | tok/sec:  3565.07 | norm: 1.64\n",
            "step1238 | loss: 3.644740343093872 | dt: 2293.14ms | tok/sec:  3572.40 | norm: 1.36\n",
            "step1239 | loss: 3.822950839996338 | dt: 2302.48ms | tok/sec:  3557.91 | norm: 1.25\n",
            "step1240 | loss: 3.762291431427002 | dt: 2306.93ms | tok/sec:  3551.03 | norm: 1.30\n",
            "step1241 | loss: 3.996936798095703 | dt: 2294.56ms | tok/sec:  3570.19 | norm: 1.20\n",
            "step1242 | loss: 4.012481212615967 | dt: 2294.64ms | tok/sec:  3570.06 | norm: 1.26\n",
            "step1243 | loss: 3.926661491394043 | dt: 2308.82ms | tok/sec:  3548.13 | norm: 1.37\n",
            "step1244 | loss: 3.9430835247039795 | dt: 2291.43ms | tok/sec:  3575.06 | norm: 1.47\n",
            "step1245 | loss: 3.6789042949676514 | dt: 2292.72ms | tok/sec:  3573.05 | norm: 1.44\n",
            "step1246 | loss: 3.861140251159668 | dt: 2298.06ms | tok/sec:  3564.75 | norm: 1.32\n",
            "step1247 | loss: 3.952007293701172 | dt: 2304.99ms | tok/sec:  3554.04 | norm: 1.31\n",
            "step1248 | loss: 3.855875015258789 | dt: 2291.15ms | tok/sec:  3575.50 | norm: 1.19\n",
            "step1249 | loss: 3.7874441146850586 | dt: 2295.61ms | tok/sec:  3568.55 | norm: 1.35\n",
            "step1250 | loss: 3.8948397636413574 | dt: 2303.53ms | tok/sec:  3556.29 | norm: 1.47\n",
            "step1251 | loss: 3.8214943408966064 | dt: 2287.38ms | tok/sec:  3581.38 | norm: 1.69\n",
            "step1252 | loss: 4.051941394805908 | dt: 2298.24ms | tok/sec:  3564.47 | norm: 1.70\n",
            "step1253 | loss: 3.7637650966644287 | dt: 2294.68ms | tok/sec:  3570.00 | norm: 1.55\n",
            "step1254 | loss: 3.764747142791748 | dt: 2305.92ms | tok/sec:  3552.59 | norm: 1.38\n",
            "step1255 | loss: 3.758535146713257 | dt: 2306.29ms | tok/sec:  3552.02 | norm: 1.14\n",
            "step1256 | loss: 4.037665367126465 | dt: 2284.42ms | tok/sec:  3586.02 | norm: 1.32\n",
            "step1257 | loss: 4.041743278503418 | dt: 2305.20ms | tok/sec:  3553.70 | norm: 1.31\n",
            "step1258 | loss: 4.188201427459717 | dt: 2297.66ms | tok/sec:  3565.37 | norm: 1.54\n",
            "step1259 | loss: 3.9344866275787354 | dt: 2292.28ms | tok/sec:  3573.73 | norm: 1.59\n",
            "step1260 | loss: 3.9865520000457764 | dt: 2285.96ms | tok/sec:  3583.61 | norm: 1.64\n",
            "step1261 | loss: 3.763274908065796 | dt: 2288.30ms | tok/sec:  3579.95 | norm: 1.56\n",
            "step1262 | loss: 3.7507870197296143 | dt: 2305.99ms | tok/sec:  3552.49 | norm: 1.36\n",
            "step1263 | loss: 3.713242530822754 | dt: 2294.59ms | tok/sec:  3570.14 | norm: 1.15\n",
            "step1264 | loss: 3.608741521835327 | dt: 2307.09ms | tok/sec:  3550.80 | norm: 1.09\n",
            "step1265 | loss: 3.761918783187866 | dt: 2298.01ms | tok/sec:  3564.82 | norm: 1.18\n",
            "step1266 | loss: 3.7057435512542725 | dt: 2305.68ms | tok/sec:  3552.96 | norm: 1.40\n",
            "step1267 | loss: 3.621546506881714 | dt: 2286.75ms | tok/sec:  3582.37 | norm: 1.35\n",
            "step1268 | loss: 3.5110528469085693 | dt: 2296.61ms | tok/sec:  3566.99 | norm: 1.42\n",
            "step1269 | loss: 3.3776731491088867 | dt: 2296.87ms | tok/sec:  3566.59 | norm: 1.32\n",
            "step1270 | loss: 3.9771933555603027 | dt: 2287.84ms | tok/sec:  3580.66 | norm: 1.43\n",
            "step1271 | loss: 3.9430158138275146 | dt: 2295.93ms | tok/sec:  3568.05 | norm: 1.36\n",
            "step1272 | loss: 3.872340202331543 | dt: 2300.84ms | tok/sec:  3560.44 | norm: 1.26\n",
            "step1273 | loss: 3.5753955841064453 | dt: 2296.99ms | tok/sec:  3566.40 | norm: 1.11\n",
            "step1274 | loss: 3.5267744064331055 | dt: 2291.92ms | tok/sec:  3574.29 | norm: 1.10\n",
            "step1275 | loss: 3.6210505962371826 | dt: 2288.62ms | tok/sec:  3579.45 | norm: 1.06\n",
            "step1276 | loss: 3.838883876800537 | dt: 2299.91ms | tok/sec:  3561.88 | norm: 1.30\n",
            "step1277 | loss: 3.8057098388671875 | dt: 2292.02ms | tok/sec:  3574.14 | norm: 1.67\n",
            "step1278 | loss: 3.8321456909179688 | dt: 2290.84ms | tok/sec:  3575.98 | norm: 1.39\n",
            "step1279 | loss: 3.5705337524414062 | dt: 2286.97ms | tok/sec:  3582.03 | norm: 1.30\n",
            "step1280 | loss: 3.7203803062438965 | dt: 2306.56ms | tok/sec:  3551.62 | norm: 1.24\n",
            "step1281 | loss: 3.658263683319092 | dt: 2287.82ms | tok/sec:  3580.71 | norm: 1.46\n",
            "step1282 | loss: 3.9243762493133545 | dt: 2286.70ms | tok/sec:  3582.45 | norm: 1.44\n",
            "step1283 | loss: 3.962127447128296 | dt: 2285.99ms | tok/sec:  3583.58 | norm: 1.49\n",
            "step1284 | loss: 3.8988749980926514 | dt: 2308.38ms | tok/sec:  3548.82 | norm: 1.43\n",
            "step1285 | loss: 3.8722026348114014 | dt: 2291.99ms | tok/sec:  3574.19 | norm: 1.16\n",
            "step1286 | loss: 3.6178603172302246 | dt: 2296.11ms | tok/sec:  3567.78 | norm: 1.11\n",
            "step1287 | loss: 3.806271553039551 | dt: 2283.20ms | tok/sec:  3587.94 | norm: 1.19\n",
            "step1288 | loss: 3.8848695755004883 | dt: 2295.11ms | tok/sec:  3569.33 | norm: 1.32\n",
            "step1289 | loss: 3.7970516681671143 | dt: 2303.18ms | tok/sec:  3556.82 | norm: 1.22\n",
            "step1290 | loss: 3.741222858428955 | dt: 2307.75ms | tok/sec:  3549.77 | norm: 1.37\n",
            "step1291 | loss: 3.829171657562256 | dt: 2293.72ms | tok/sec:  3571.49 | norm: 1.22\n",
            "step1292 | loss: 3.784080982208252 | dt: 2296.30ms | tok/sec:  3567.48 | norm: 1.55\n",
            "step1293 | loss: 3.969602584838867 | dt: 2307.11ms | tok/sec:  3550.77 | norm: 1.27\n",
            "step1294 | loss: 3.685267448425293 | dt: 2298.44ms | tok/sec:  3564.16 | norm: 1.34\n",
            "step1295 | loss: 3.703754425048828 | dt: 2292.89ms | tok/sec:  3572.78 | norm: 1.34\n",
            "step1296 | loss: 3.6701555252075195 | dt: 2304.88ms | tok/sec:  3554.20 | norm: 1.27\n",
            "step1297 | loss: 3.9693353176116943 | dt: 2296.77ms | tok/sec:  3566.75 | norm: 1.59\n",
            "step1298 | loss: 3.984670400619507 | dt: 2294.07ms | tok/sec:  3570.95 | norm: 1.57\n",
            "step1299 | loss: 4.168631076812744 | dt: 2285.59ms | tok/sec:  3584.19 | norm: 1.51\n",
            "step1300 | loss: 3.9381885528564453 | dt: 2304.76ms | tok/sec:  3554.38 | norm: 1.53\n",
            "step1301 | loss: 3.987877368927002 | dt: 2285.48ms | tok/sec:  3584.37 | norm: 1.45\n",
            "step1302 | loss: 3.815579891204834 | dt: 2291.32ms | tok/sec:  3575.23 | norm: 1.79\n",
            "step1303 | loss: 3.7555055618286133 | dt: 2300.97ms | tok/sec:  3560.24 | norm: 1.75\n",
            "step1304 | loss: 3.7648513317108154 | dt: 2287.47ms | tok/sec:  3581.25 | norm: 1.83\n",
            "step1305 | loss: 3.6316077709198 | dt: 2291.13ms | tok/sec:  3575.54 | norm: 1.74\n",
            "step1306 | loss: 3.7423501014709473 | dt: 2280.69ms | tok/sec:  3591.90 | norm: 1.61\n",
            "step1307 | loss: 3.6729652881622314 | dt: 2299.52ms | tok/sec:  3562.48 | norm: 1.64\n",
            "step1308 | loss: 3.5383849143981934 | dt: 2300.43ms | tok/sec:  3561.07 | norm: 1.48\n",
            "step1309 | loss: 3.45054292678833 | dt: 2292.15ms | tok/sec:  3573.93 | norm: 1.30\n",
            "step1310 | loss: 3.351855754852295 | dt: 2288.96ms | tok/sec:  3578.92 | norm: 1.45\n",
            "step1311 | loss: 3.970926284790039 | dt: 2290.74ms | tok/sec:  3576.14 | norm: 1.73\n",
            "step1312 | loss: 3.946113348007202 | dt: 2291.73ms | tok/sec:  3574.59 | norm: 1.95\n",
            "step1313 | loss: 3.902977466583252 | dt: 2298.64ms | tok/sec:  3563.84 | norm: 1.66\n",
            "step1314 | loss: 3.604379177093506 | dt: 2304.57ms | tok/sec:  3554.68 | norm: 1.50\n",
            "step1315 | loss: 3.5254688262939453 | dt: 2286.79ms | tok/sec:  3582.32 | norm: 1.31\n",
            "step1316 | loss: 3.5935611724853516 | dt: 2290.59ms | tok/sec:  3576.38 | norm: 1.21\n",
            "step1317 | loss: 3.8078627586364746 | dt: 2299.72ms | tok/sec:  3562.17 | norm: 1.13\n",
            "step1318 | loss: 3.764014720916748 | dt: 2294.26ms | tok/sec:  3570.64 | norm: 1.22\n",
            "step1319 | loss: 3.769526481628418 | dt: 2292.38ms | tok/sec:  3573.58 | norm: 1.24\n",
            "step1320 | loss: 3.5433993339538574 | dt: 2287.73ms | tok/sec:  3580.84 | norm: 1.32\n",
            "step1321 | loss: 3.700951337814331 | dt: 2295.20ms | tok/sec:  3569.19 | norm: 1.57\n",
            "step1322 | loss: 3.6477370262145996 | dt: 2303.77ms | tok/sec:  3555.90 | norm: 1.69\n",
            "step1323 | loss: 3.9032199382781982 | dt: 2299.69ms | tok/sec:  3562.21 | norm: 1.59\n",
            "step1324 | loss: 3.981825590133667 | dt: 2292.56ms | tok/sec:  3573.29 | norm: 1.51\n",
            "step1325 | loss: 3.8944289684295654 | dt: 2298.51ms | tok/sec:  3564.05 | norm: 1.41\n",
            "step1326 | loss: 3.852375030517578 | dt: 2299.68ms | tok/sec:  3562.23 | norm: 1.44\n",
            "step1327 | loss: 3.6261401176452637 | dt: 2289.91ms | tok/sec:  3577.43 | norm: 1.25\n",
            "step1328 | loss: 3.773357629776001 | dt: 2292.00ms | tok/sec:  3574.17 | norm: 1.34\n",
            "step1329 | loss: 3.864375114440918 | dt: 2288.76ms | tok/sec:  3579.22 | norm: 1.33\n",
            "step1330 | loss: 3.730461597442627 | dt: 2290.78ms | tok/sec:  3576.08 | norm: 1.22\n",
            "step1331 | loss: 3.6889612674713135 | dt: 2298.15ms | tok/sec:  3564.60 | norm: 1.33\n",
            "step1332 | loss: 3.7511777877807617 | dt: 2310.45ms | tok/sec:  3545.63 | norm: 1.17\n",
            "step1333 | loss: 3.743227481842041 | dt: 2304.90ms | tok/sec:  3554.17 | norm: 1.58\n",
            "step1334 | loss: 3.8829421997070312 | dt: 2298.44ms | tok/sec:  3564.15 | norm: 1.32\n",
            "step1335 | loss: 3.6441526412963867 | dt: 2293.85ms | tok/sec:  3571.29 | norm: 1.45\n",
            "step1336 | loss: 3.680720329284668 | dt: 2297.67ms | tok/sec:  3565.36 | norm: 1.40\n",
            "step1337 | loss: 3.630650758743286 | dt: 2297.71ms | tok/sec:  3565.28 | norm: 1.30\n",
            "step1338 | loss: 3.9919826984405518 | dt: 2288.69ms | tok/sec:  3579.34 | norm: 1.48\n",
            "step1339 | loss: 3.9802937507629395 | dt: 2301.64ms | tok/sec:  3559.21 | norm: 1.46\n",
            "step1340 | loss: 4.095240592956543 | dt: 2294.58ms | tok/sec:  3570.15 | norm: 1.35\n",
            "step1341 | loss: 3.8449721336364746 | dt: 2289.04ms | tok/sec:  3578.79 | norm: 1.26\n",
            "step1342 | loss: 3.899721622467041 | dt: 2301.68ms | tok/sec:  3559.13 | norm: 1.40\n",
            "step1343 | loss: 3.7135672569274902 | dt: 2300.35ms | tok/sec:  3561.20 | norm: 1.46\n",
            "step1344 | loss: 3.703273057937622 | dt: 2294.34ms | tok/sec:  3570.52 | norm: 1.58\n",
            "step1345 | loss: 3.7371222972869873 | dt: 2287.00ms | tok/sec:  3581.98 | norm: 1.85\n",
            "step1346 | loss: 3.592846393585205 | dt: 2306.82ms | tok/sec:  3551.22 | norm: 1.39\n",
            "step1347 | loss: 3.699155807495117 | dt: 2290.08ms | tok/sec:  3577.17 | norm: 1.47\n",
            "step1348 | loss: 3.7173423767089844 | dt: 2301.40ms | tok/sec:  3559.57 | norm: 1.72\n",
            "step1349 | loss: 3.57629656791687 | dt: 2287.22ms | tok/sec:  3581.64 | norm: 1.48\n",
            "step1350 | loss: 3.48974609375 | dt: 2305.58ms | tok/sec:  3553.12 | norm: 1.66\n",
            "step1351 | loss: 3.3733513355255127 | dt: 2292.84ms | tok/sec:  3572.86 | norm: 1.61\n",
            "step1352 | loss: 3.9081547260284424 | dt: 2290.39ms | tok/sec:  3576.68 | norm: 1.93\n",
            "step1353 | loss: 3.9588918685913086 | dt: 2292.82ms | tok/sec:  3572.90 | norm: 1.67\n",
            "step1354 | loss: 3.8868141174316406 | dt: 2308.75ms | tok/sec:  3548.25 | norm: 1.49\n",
            "step1355 | loss: 3.584408760070801 | dt: 2286.57ms | tok/sec:  3582.66 | norm: 1.41\n",
            "step1356 | loss: 3.507158041000366 | dt: 2304.30ms | tok/sec:  3555.10 | norm: 1.41\n",
            "step1357 | loss: 3.553190231323242 | dt: 2287.20ms | tok/sec:  3581.67 | norm: 1.36\n",
            "step1358 | loss: 3.7746493816375732 | dt: 2291.75ms | tok/sec:  3574.56 | norm: 1.56\n",
            "step1359 | loss: 3.802590847015381 | dt: 2298.40ms | tok/sec:  3564.22 | norm: 1.95\n",
            "step1360 | loss: 3.815227508544922 | dt: 2305.28ms | tok/sec:  3553.58 | norm: 1.91\n",
            "step1361 | loss: 3.533637046813965 | dt: 2292.49ms | tok/sec:  3573.41 | norm: 1.58\n",
            "step1362 | loss: 3.707414150238037 | dt: 2300.49ms | tok/sec:  3560.98 | norm: 1.51\n",
            "step1363 | loss: 3.628081798553467 | dt: 2302.32ms | tok/sec:  3558.15 | norm: 1.46\n",
            "step1364 | loss: 3.816436290740967 | dt: 2298.42ms | tok/sec:  3564.18 | norm: 1.25\n",
            "step1365 | loss: 3.8848443031311035 | dt: 2296.97ms | tok/sec:  3566.44 | norm: 1.35\n",
            "step1366 | loss: 3.818798542022705 | dt: 2298.30ms | tok/sec:  3564.38 | norm: 1.72\n",
            "step1367 | loss: 3.849630117416382 | dt: 2294.77ms | tok/sec:  3569.86 | norm: 2.04\n",
            "step1368 | loss: 3.6323189735412598 | dt: 2303.93ms | tok/sec:  3555.66 | norm: 1.83\n",
            "step1369 | loss: 3.8107500076293945 | dt: 2293.19ms | tok/sec:  3572.32 | norm: 2.06\n",
            "step1370 | loss: 3.8617451190948486 | dt: 2299.09ms | tok/sec:  3563.15 | norm: 1.94\n",
            "step1371 | loss: 3.7454423904418945 | dt: 2289.84ms | tok/sec:  3577.54 | norm: 1.51\n",
            "step1372 | loss: 3.663705825805664 | dt: 2305.55ms | tok/sec:  3553.16 | norm: 1.28\n",
            "step1373 | loss: 3.7044320106506348 | dt: 2286.28ms | tok/sec:  3583.12 | norm: 1.39\n",
            "step1374 | loss: 3.7588329315185547 | dt: 2285.42ms | tok/sec:  3584.46 | norm: 1.52\n",
            "step1375 | loss: 3.9160962104797363 | dt: 2301.13ms | tok/sec:  3559.99 | norm: 1.67\n",
            "step1376 | loss: 3.6511008739471436 | dt: 2289.31ms | tok/sec:  3578.37 | norm: 1.79\n",
            "step1377 | loss: 3.67150616645813 | dt: 2303.27ms | tok/sec:  3556.68 | norm: 1.51\n",
            "step1378 | loss: 3.618619203567505 | dt: 2299.24ms | tok/sec:  3562.92 | norm: 1.39\n",
            "step1379 | loss: 3.921882390975952 | dt: 2297.13ms | tok/sec:  3566.19 | norm: 1.52\n",
            "step1380 | loss: 3.9174270629882812 | dt: 2301.66ms | tok/sec:  3559.17 | norm: 1.37\n",
            "step1381 | loss: 4.025840759277344 | dt: 2287.89ms | tok/sec:  3580.59 | norm: 1.47\n",
            "step1382 | loss: 3.7704553604125977 | dt: 2288.57ms | tok/sec:  3579.53 | norm: 1.38\n",
            "step1383 | loss: 3.838535785675049 | dt: 2303.96ms | tok/sec:  3555.62 | norm: 1.29\n",
            "step1384 | loss: 3.6629412174224854 | dt: 2290.51ms | tok/sec:  3576.49 | norm: 1.61\n",
            "step1385 | loss: 3.679929256439209 | dt: 2299.98ms | tok/sec:  3561.77 | norm: 1.46\n",
            "step1386 | loss: 3.7033579349517822 | dt: 2296.22ms | tok/sec:  3567.60 | norm: 1.60\n",
            "step1387 | loss: 3.5214366912841797 | dt: 2299.36ms | tok/sec:  3562.72 | norm: 1.37\n",
            "step1388 | loss: 3.6488218307495117 | dt: 2293.48ms | tok/sec:  3571.86 | norm: 1.28\n",
            "step1389 | loss: 3.634427785873413 | dt: 2305.78ms | tok/sec:  3552.81 | norm: 1.61\n",
            "step1390 | loss: 3.5048768520355225 | dt: 2302.81ms | tok/sec:  3557.39 | norm: 1.33\n",
            "step1391 | loss: 3.39990234375 | dt: 2290.99ms | tok/sec:  3575.74 | norm: 1.37\n",
            "step1392 | loss: 3.2821667194366455 | dt: 2297.08ms | tok/sec:  3566.26 | norm: 1.28\n",
            "step1393 | loss: 3.8922736644744873 | dt: 2295.28ms | tok/sec:  3569.07 | norm: 2.08\n",
            "step1394 | loss: 3.896909475326538 | dt: 2284.57ms | tok/sec:  3585.79 | norm: 1.89\n",
            "step1395 | loss: 3.82743239402771 | dt: 2295.31ms | tok/sec:  3569.02 | norm: 1.79\n",
            "step1396 | loss: 3.55949330329895 | dt: 2291.99ms | tok/sec:  3574.18 | norm: 1.77\n",
            "step1397 | loss: 3.4715607166290283 | dt: 2298.62ms | tok/sec:  3563.87 | norm: 1.57\n",
            "step1398 | loss: 3.5099074840545654 | dt: 2307.16ms | tok/sec:  3550.69 | norm: 1.31\n",
            "step1399 | loss: 3.729627847671509 | dt: 2285.89ms | tok/sec:  3583.72 | norm: 1.62\n",
            "step1400 | loss: 3.722094774246216 | dt: 2306.18ms | tok/sec:  3552.20 | norm: 1.61\n",
            "step1401 | loss: 3.740328550338745 | dt: 2305.74ms | tok/sec:  3552.87 | norm: 1.51\n",
            "step1402 | loss: 3.474191188812256 | dt: 2285.66ms | tok/sec:  3584.09 | norm: 1.41\n",
            "step1403 | loss: 3.6011040210723877 | dt: 2300.45ms | tok/sec:  3561.05 | norm: 1.37\n",
            "step1404 | loss: 3.548501491546631 | dt: 2299.10ms | tok/sec:  3563.14 | norm: 1.50\n",
            "step1405 | loss: 3.745310068130493 | dt: 2296.35ms | tok/sec:  3567.39 | norm: 1.48\n",
            "step1406 | loss: 3.8263959884643555 | dt: 2292.25ms | tok/sec:  3573.79 | norm: 1.79\n",
            "step1407 | loss: 3.7561492919921875 | dt: 2286.30ms | tok/sec:  3583.08 | norm: 1.48\n",
            "step1408 | loss: 3.782921552658081 | dt: 2286.43ms | tok/sec:  3582.87 | norm: 1.53\n",
            "step1409 | loss: 3.5645952224731445 | dt: 2305.87ms | tok/sec:  3552.68 | norm: 1.43\n",
            "step1410 | loss: 3.7431342601776123 | dt: 2296.58ms | tok/sec:  3567.04 | norm: 1.60\n",
            "step1411 | loss: 3.784344434738159 | dt: 2287.42ms | tok/sec:  3581.32 | norm: 1.41\n",
            "step1412 | loss: 3.6802728176116943 | dt: 2298.14ms | tok/sec:  3564.62 | norm: 1.61\n",
            "step1413 | loss: 3.5743248462677 | dt: 2303.23ms | tok/sec:  3556.75 | norm: 1.49\n",
            "step1414 | loss: 3.6396195888519287 | dt: 2301.35ms | tok/sec:  3559.65 | norm: 1.69\n",
            "step1415 | loss: 3.6511380672454834 | dt: 2294.99ms | tok/sec:  3569.52 | norm: 1.56\n",
            "step1416 | loss: 3.8209943771362305 | dt: 2291.90ms | tok/sec:  3574.33 | norm: 1.51\n",
            "step1417 | loss: 3.5691263675689697 | dt: 2286.56ms | tok/sec:  3582.67 | norm: 1.44\n",
            "step1418 | loss: 3.573568105697632 | dt: 2293.22ms | tok/sec:  3572.28 | norm: 1.54\n",
            "step1419 | loss: 3.544646739959717 | dt: 2277.54ms | tok/sec:  3596.87 | norm: 1.41\n",
            "step1420 | loss: 3.847149133682251 | dt: 2289.11ms | tok/sec:  3578.69 | norm: 1.68\n",
            "step1421 | loss: 3.876295566558838 | dt: 2290.62ms | tok/sec:  3576.33 | norm: 1.65\n",
            "step1422 | loss: 3.9446370601654053 | dt: 2292.11ms | tok/sec:  3573.99 | norm: 1.49\n",
            "step1423 | loss: 3.6845791339874268 | dt: 2291.12ms | tok/sec:  3575.54 | norm: 1.49\n",
            "step1424 | loss: 3.7180891036987305 | dt: 2285.95ms | tok/sec:  3583.64 | norm: 1.33\n",
            "step1425 | loss: 3.516186237335205 | dt: 2279.44ms | tok/sec:  3593.86 | norm: 1.40\n",
            "step1426 | loss: 3.5636990070343018 | dt: 2299.14ms | tok/sec:  3563.07 | norm: 1.34\n",
            "step1427 | loss: 3.549811840057373 | dt: 2281.29ms | tok/sec:  3590.96 | norm: 1.52\n",
            "step1428 | loss: 3.4276978969573975 | dt: 2291.15ms | tok/sec:  3575.50 | norm: 1.41\n",
            "step1429 | loss: 3.5588369369506836 | dt: 2289.20ms | tok/sec:  3578.55 | norm: 1.50\n",
            "step1430 | loss: 3.551962375640869 | dt: 2289.77ms | tok/sec:  3577.65 | norm: 1.52\n",
            "step1431 | loss: 3.416729688644409 | dt: 2291.85ms | tok/sec:  3574.41 | norm: 1.57\n",
            "step1432 | loss: 3.2947778701782227 | dt: 2296.78ms | tok/sec:  3566.74 | norm: 1.42\n",
            "step1433 | loss: 3.1780004501342773 | dt: 2286.47ms | tok/sec:  3582.81 | norm: 1.27\n",
            "step1434 | loss: 3.8242435455322266 | dt: 2287.50ms | tok/sec:  3581.21 | norm: 1.78\n",
            "step1435 | loss: 3.7782883644104004 | dt: 2289.46ms | tok/sec:  3578.13 | norm: 1.46\n",
            "step1436 | loss: 3.714005947113037 | dt: 2289.49ms | tok/sec:  3578.09 | norm: 1.34\n",
            "step1437 | loss: 3.4391257762908936 | dt: 2297.05ms | tok/sec:  3566.31 | norm: 1.26\n",
            "step1438 | loss: 3.361569881439209 | dt: 2285.49ms | tok/sec:  3584.35 | norm: 1.26\n",
            "step1439 | loss: 3.395249843597412 | dt: 2280.76ms | tok/sec:  3591.78 | norm: 1.25\n",
            "step1440 | loss: 3.623137950897217 | dt: 2286.67ms | tok/sec:  3582.49 | norm: 1.27\n",
            "step1441 | loss: 3.606142520904541 | dt: 2287.29ms | tok/sec:  3581.54 | norm: 1.66\n",
            "step1442 | loss: 3.6405277252197266 | dt: 2292.74ms | tok/sec:  3573.01 | norm: 1.54\n",
            "step1443 | loss: 3.3923490047454834 | dt: 2282.15ms | tok/sec:  3589.60 | norm: 1.52\n",
            "step1444 | loss: 3.549335479736328 | dt: 2284.96ms | tok/sec:  3585.18 | norm: 1.50\n",
            "step1445 | loss: 3.523176431655884 | dt: 2293.36ms | tok/sec:  3572.05 | norm: 1.58\n",
            "step1446 | loss: 3.736757755279541 | dt: 2277.22ms | tok/sec:  3597.37 | norm: 1.62\n",
            "step1447 | loss: 3.8190581798553467 | dt: 2295.47ms | tok/sec:  3568.76 | norm: 1.54\n",
            "step1448 | loss: 3.681431293487549 | dt: 2289.77ms | tok/sec:  3577.65 | norm: 1.53\n",
            "step1449 | loss: 3.6866369247436523 | dt: 2293.81ms | tok/sec:  3571.35 | norm: 1.31\n",
            "step1450 | loss: 3.435274124145508 | dt: 2292.33ms | tok/sec:  3573.66 | norm: 1.27\n",
            "step1451 | loss: 3.634756326675415 | dt: 2288.58ms | tok/sec:  3579.51 | norm: 1.48\n",
            "step1452 | loss: 3.6913750171661377 | dt: 2292.71ms | tok/sec:  3573.06 | norm: 1.37\n",
            "step1453 | loss: 3.5961036682128906 | dt: 2289.04ms | tok/sec:  3578.80 | norm: 1.49\n",
            "step1454 | loss: 3.4890646934509277 | dt: 2295.19ms | tok/sec:  3569.20 | norm: 1.36\n",
            "step1455 | loss: 3.58492374420166 | dt: 2295.22ms | tok/sec:  3569.16 | norm: 1.34\n",
            "step1456 | loss: 3.5925514698028564 | dt: 2291.44ms | tok/sec:  3575.05 | norm: 1.69\n",
            "step1457 | loss: 3.7459208965301514 | dt: 2288.43ms | tok/sec:  3579.74 | norm: 1.45\n",
            "step1458 | loss: 3.4946725368499756 | dt: 2307.12ms | tok/sec:  3550.75 | norm: 1.48\n",
            "step1459 | loss: 3.5048866271972656 | dt: 2290.03ms | tok/sec:  3577.24 | norm: 1.37\n",
            "step1460 | loss: 3.477473735809326 | dt: 2292.38ms | tok/sec:  3573.57 | norm: 1.43\n",
            "step1461 | loss: 3.7856554985046387 | dt: 2294.14ms | tok/sec:  3570.84 | norm: 1.57\n",
            "step1462 | loss: 3.7855677604675293 | dt: 2280.73ms | tok/sec:  3591.83 | norm: 1.52\n",
            "step1463 | loss: 3.8902149200439453 | dt: 2291.05ms | tok/sec:  3575.66 | norm: 1.56\n",
            "step1464 | loss: 3.647987127304077 | dt: 2290.54ms | tok/sec:  3576.45 | norm: 1.68\n",
            "step1465 | loss: 3.672950506210327 | dt: 2278.83ms | tok/sec:  3594.82 | norm: 1.59\n",
            "step1466 | loss: 3.518836498260498 | dt: 2286.76ms | tok/sec:  3582.36 | norm: 1.87\n",
            "step1467 | loss: 3.5661468505859375 | dt: 2290.44ms | tok/sec:  3576.61 | norm: 1.90\n",
            "step1468 | loss: 3.5512003898620605 | dt: 2280.42ms | tok/sec:  3592.32 | norm: 1.66\n",
            "step1469 | loss: 3.4219913482666016 | dt: 2296.25ms | tok/sec:  3567.55 | norm: 1.51\n",
            "step1470 | loss: 3.5296266078948975 | dt: 2284.45ms | tok/sec:  3585.98 | norm: 1.38\n",
            "step1471 | loss: 3.4771690368652344 | dt: 2284.12ms | tok/sec:  3586.50 | norm: 1.39\n",
            "step1472 | loss: 3.3483939170837402 | dt: 2284.67ms | tok/sec:  3585.64 | norm: 1.23\n",
            "step1473 | loss: 3.2428739070892334 | dt: 2287.85ms | tok/sec:  3580.65 | norm: 1.38\n",
            "step1474 | loss: 3.1247308254241943 | dt: 2286.84ms | tok/sec:  3582.24 | norm: 1.23\n",
            "step1475 | loss: 3.7578611373901367 | dt: 2284.41ms | tok/sec:  3586.04 | norm: 1.60\n",
            "step1476 | loss: 3.787814140319824 | dt: 2286.84ms | tok/sec:  3582.23 | norm: 1.80\n",
            "step1477 | loss: 3.7152860164642334 | dt: 2283.98ms | tok/sec:  3586.72 | norm: 1.77\n",
            "step1478 | loss: 3.460331439971924 | dt: 2279.31ms | tok/sec:  3594.07 | norm: 1.58\n",
            "step1479 | loss: 3.3652236461639404 | dt: 2285.53ms | tok/sec:  3584.29 | norm: 1.47\n",
            "step1480 | loss: 3.3975722789764404 | dt: 2290.99ms | tok/sec:  3575.75 | norm: 10.88\n",
            "step1481 | loss: 3.638091802597046 | dt: 2292.75ms | tok/sec:  3573.00 | norm: 1.73\n",
            "step1482 | loss: 3.6619081497192383 | dt: 2291.49ms | tok/sec:  3574.97 | norm: 1.89\n",
            "step1483 | loss: 3.6754627227783203 | dt: 2280.01ms | tok/sec:  3592.96 | norm: 1.76\n",
            "step1484 | loss: 3.4023969173431396 | dt: 2287.37ms | tok/sec:  3581.40 | norm: 1.43\n",
            "step1485 | loss: 3.510687828063965 | dt: 2295.05ms | tok/sec:  3569.43 | norm: 1.36\n",
            "step1486 | loss: 3.437392234802246 | dt: 2282.98ms | tok/sec:  3588.29 | norm: 1.39\n",
            "step1487 | loss: 3.6563422679901123 | dt: 2288.13ms | tok/sec:  3580.21 | norm: 1.47\n",
            "step1488 | loss: 3.734797954559326 | dt: 2289.67ms | tok/sec:  3577.81 | norm: 2.06\n",
            "step1489 | loss: 3.625800848007202 | dt: 2280.78ms | tok/sec:  3591.76 | norm: 1.59\n",
            "step1490 | loss: 3.661350727081299 | dt: 2288.98ms | tok/sec:  3578.89 | norm: 1.67\n",
            "step1491 | loss: 3.464388370513916 | dt: 2285.24ms | tok/sec:  3584.75 | norm: 1.58\n",
            "step1492 | loss: 3.6597938537597656 | dt: 2287.94ms | tok/sec:  3580.52 | norm: 2.59\n",
            "step1493 | loss: 3.729435920715332 | dt: 2278.34ms | tok/sec:  3595.60 | norm: 1.69\n",
            "step1494 | loss: 3.6183605194091797 | dt: 2286.34ms | tok/sec:  3583.02 | norm: 1.50\n",
            "step1495 | loss: 3.4951882362365723 | dt: 2284.61ms | tok/sec:  3585.73 | norm: 1.58\n",
            "step1496 | loss: 3.54797625541687 | dt: 2287.13ms | tok/sec:  3581.79 | norm: 1.44\n",
            "step1497 | loss: 3.5675196647644043 | dt: 2288.70ms | tok/sec:  3579.32 | norm: 1.48\n",
            "step1498 | loss: 3.6982505321502686 | dt: 2293.17ms | tok/sec:  3572.35 | norm: 1.45\n",
            "step1499 | loss: 3.4702091217041016 | dt: 2284.96ms | tok/sec:  3585.19 | norm: 1.53\n",
            "step1500 | loss: 3.4971275329589844 | dt: 2277.23ms | tok/sec:  3597.35 | norm: 1.51\n",
            "step1501 | loss: 3.4389474391937256 | dt: 2276.91ms | tok/sec:  3597.86 | norm: 1.31\n",
            "step1502 | loss: 3.7371037006378174 | dt: 2293.01ms | tok/sec:  3572.60 | norm: 1.58\n",
            "step1503 | loss: 3.7205562591552734 | dt: 2292.51ms | tok/sec:  3573.37 | norm: 1.38\n",
            "step1504 | loss: 3.8506481647491455 | dt: 2282.74ms | tok/sec:  3588.67 | norm: 1.60\n",
            "step1505 | loss: 3.606733560562134 | dt: 2299.94ms | tok/sec:  3561.83 | norm: 1.55\n",
            "step1506 | loss: 3.614680290222168 | dt: 2279.19ms | tok/sec:  3594.26 | norm: 1.48\n",
            "step1507 | loss: 3.4602229595184326 | dt: 2281.09ms | tok/sec:  3591.26 | norm: 1.73\n",
            "step1508 | loss: 3.5047523975372314 | dt: 2296.01ms | tok/sec:  3567.93 | norm: 1.55\n",
            "step1509 | loss: 3.457247257232666 | dt: 2288.22ms | tok/sec:  3580.08 | norm: 1.63\n",
            "step1510 | loss: 3.3327224254608154 | dt: 2277.13ms | tok/sec:  3597.52 | norm: 1.48\n",
            "step1511 | loss: 3.4327635765075684 | dt: 2285.35ms | tok/sec:  3584.56 | norm: 1.38\n",
            "step1512 | loss: 3.3739683628082275 | dt: 2285.11ms | tok/sec:  3584.95 | norm: 1.39\n",
            "step1513 | loss: 3.254302978515625 | dt: 2287.63ms | tok/sec:  3581.01 | norm: 1.46\n",
            "step1514 | loss: 3.1603829860687256 | dt: 2293.61ms | tok/sec:  3571.66 | norm: 1.42\n",
            "step1515 | loss: 3.081867218017578 | dt: 2289.44ms | tok/sec:  3578.17 | norm: 2.04\n",
            "step1516 | loss: 3.7336673736572266 | dt: 2286.28ms | tok/sec:  3583.11 | norm: 1.89\n",
            "step1517 | loss: 3.7391512393951416 | dt: 2286.96ms | tok/sec:  3582.05 | norm: 1.82\n",
            "step1518 | loss: 3.684488296508789 | dt: 2287.40ms | tok/sec:  3581.35 | norm: 1.94\n",
            "step1519 | loss: 3.401247978210449 | dt: 2294.78ms | tok/sec:  3569.83 | norm: 1.70\n",
            "step1520 | loss: 3.3196423053741455 | dt: 2297.66ms | tok/sec:  3565.37 | norm: 1.56\n",
            "step1521 | loss: 3.4811556339263916 | dt: 2294.63ms | tok/sec:  3570.07 | norm: 1.51\n",
            "step1522 | loss: 3.570032835006714 | dt: 2284.30ms | tok/sec:  3586.22 | norm: 1.80\n",
            "step1523 | loss: 3.5089664459228516 | dt: 2285.05ms | tok/sec:  3585.04 | norm: 1.54\n",
            "step1524 | loss: 3.5339672565460205 | dt: 2280.53ms | tok/sec:  3592.14 | norm: 1.51\n",
            "step1525 | loss: 3.283371925354004 | dt: 2285.97ms | tok/sec:  3583.59 | norm: 1.34\n",
            "step1526 | loss: 3.380939483642578 | dt: 2294.96ms | tok/sec:  3569.56 | norm: 1.45\n",
            "step1527 | loss: 3.3378636837005615 | dt: 2285.07ms | tok/sec:  3585.01 | norm: 1.59\n",
            "step1528 | loss: 3.584366798400879 | dt: 2283.78ms | tok/sec:  3587.03 | norm: 1.56\n",
            "step1529 | loss: 3.708688259124756 | dt: 2282.94ms | tok/sec:  3588.35 | norm: 1.69\n",
            "step1530 | loss: 3.56048583984375 | dt: 2284.65ms | tok/sec:  3585.67 | norm: 1.65\n",
            "step1531 | loss: 3.5805246829986572 | dt: 2290.38ms | tok/sec:  3576.69 | norm: 1.38\n",
            "step1532 | loss: 3.359168529510498 | dt: 2281.93ms | tok/sec:  3589.94 | norm: 1.34\n",
            "step1533 | loss: 3.6227684020996094 | dt: 2288.23ms | tok/sec:  3580.06 | norm: 1.89\n",
            "step1534 | loss: 3.6015522480010986 | dt: 2288.29ms | tok/sec:  3579.96 | norm: 1.63\n",
            "step1535 | loss: 3.5385472774505615 | dt: 2286.28ms | tok/sec:  3583.11 | norm: 1.72\n",
            "step1536 | loss: 3.4531288146972656 | dt: 2284.69ms | tok/sec:  3585.61 | norm: 1.67\n",
            "step1537 | loss: 3.5114359855651855 | dt: 2278.21ms | tok/sec:  3595.81 | norm: 1.57\n",
            "step1538 | loss: 3.4977729320526123 | dt: 2297.69ms | tok/sec:  3565.32 | norm: 1.50\n",
            "step1539 | loss: 3.648597478866577 | dt: 2280.64ms | tok/sec:  3591.98 | norm: 1.68\n",
            "step1540 | loss: 3.433628559112549 | dt: 2282.46ms | tok/sec:  3589.11 | norm: 1.56\n",
            "step1541 | loss: 3.421759605407715 | dt: 2286.81ms | tok/sec:  3582.29 | norm: 1.64\n",
            "step1542 | loss: 3.3767356872558594 | dt: 2292.76ms | tok/sec:  3572.99 | norm: 1.47\n",
            "step1543 | loss: 3.630310297012329 | dt: 2288.83ms | tok/sec:  3579.13 | norm: 1.56\n",
            "step1544 | loss: 3.660659074783325 | dt: 2290.78ms | tok/sec:  3576.07 | norm: 1.59\n",
            "step1545 | loss: 3.8062007427215576 | dt: 2287.25ms | tok/sec:  3581.60 | norm: 1.64\n",
            "step1546 | loss: 3.5465312004089355 | dt: 2290.70ms | tok/sec:  3576.20 | norm: 1.49\n",
            "step1547 | loss: 3.5871050357818604 | dt: 2288.75ms | tok/sec:  3579.25 | norm: 1.67\n",
            "step1548 | loss: 3.389737129211426 | dt: 2294.33ms | tok/sec:  3570.54 | norm: 1.61\n",
            "step1549 | loss: 3.3917222023010254 | dt: 2292.45ms | tok/sec:  3573.48 | norm: 1.47\n",
            "step1550 | loss: 3.345670223236084 | dt: 2287.24ms | tok/sec:  3581.61 | norm: 1.53\n",
            "step1551 | loss: 3.2330663204193115 | dt: 2295.03ms | tok/sec:  3569.45 | norm: 1.33\n",
            "step1552 | loss: 3.344459056854248 | dt: 2280.87ms | tok/sec:  3591.62 | norm: 1.46\n",
            "step1553 | loss: 3.3057384490966797 | dt: 2281.78ms | tok/sec:  3590.18 | norm: 1.65\n",
            "step1554 | loss: 3.2081477642059326 | dt: 2287.44ms | tok/sec:  3581.29 | norm: 1.72\n",
            "step1555 | loss: 3.1176517009735107 | dt: 2295.30ms | tok/sec:  3569.03 | norm: 1.50\n",
            "step1556 | loss: 3.0346813201904297 | dt: 2285.03ms | tok/sec:  3585.08 | norm: 1.82\n",
            "step1557 | loss: 3.6171138286590576 | dt: 2281.79ms | tok/sec:  3590.17 | norm: 1.54\n",
            "step1558 | loss: 3.651745557785034 | dt: 2287.82ms | tok/sec:  3580.70 | norm: 1.90\n",
            "step1559 | loss: 3.5960946083068848 | dt: 2283.80ms | tok/sec:  3587.01 | norm: 2.13\n",
            "step1560 | loss: 3.3779940605163574 | dt: 2284.40ms | tok/sec:  3586.06 | norm: 1.95\n",
            "step1561 | loss: 3.2930378913879395 | dt: 2282.32ms | tok/sec:  3589.33 | norm: 1.59\n",
            "step1562 | loss: 3.4163668155670166 | dt: 2289.71ms | tok/sec:  3577.74 | norm: 1.66\n",
            "step1563 | loss: 3.564175605773926 | dt: 2288.24ms | tok/sec:  3580.05 | norm: 1.78\n",
            "step1564 | loss: 3.5087127685546875 | dt: 2281.90ms | tok/sec:  3589.99 | norm: 1.96\n",
            "step1565 | loss: 3.5216853618621826 | dt: 2293.68ms | tok/sec:  3571.55 | norm: 2.09\n",
            "step1566 | loss: 3.2925987243652344 | dt: 2283.07ms | tok/sec:  3588.15 | norm: 1.79\n",
            "step1567 | loss: 3.379606246948242 | dt: 2287.00ms | tok/sec:  3581.98 | norm: 1.65\n",
            "step1568 | loss: 3.361137866973877 | dt: 2281.36ms | tok/sec:  3590.83 | norm: 2.08\n",
            "step1569 | loss: 3.5687782764434814 | dt: 2291.03ms | tok/sec:  3575.68 | norm: 1.75\n",
            "step1570 | loss: 3.66670823097229 | dt: 2296.68ms | tok/sec:  3566.89 | norm: 1.80\n",
            "step1571 | loss: 3.5517802238464355 | dt: 2278.96ms | tok/sec:  3594.62 | norm: 1.64\n",
            "step1572 | loss: 3.5332939624786377 | dt: 2284.21ms | tok/sec:  3586.37 | norm: 1.67\n",
            "step1573 | loss: 3.327962875366211 | dt: 2289.14ms | tok/sec:  3578.64 | norm: 1.73\n",
            "step1574 | loss: 3.5937917232513428 | dt: 2289.56ms | tok/sec:  3577.98 | norm: 2.25\n",
            "step1575 | loss: 3.5628795623779297 | dt: 2285.44ms | tok/sec:  3584.42 | norm: 1.77\n",
            "step1576 | loss: 3.5036840438842773 | dt: 2290.85ms | tok/sec:  3575.96 | norm: 1.79\n",
            "step1577 | loss: 3.4328370094299316 | dt: 2288.46ms | tok/sec:  3579.70 | norm: 1.65\n",
            "step1578 | loss: 3.5131380558013916 | dt: 2293.75ms | tok/sec:  3571.44 | norm: 1.72\n",
            "step1579 | loss: 3.4689109325408936 | dt: 2285.95ms | tok/sec:  3583.63 | norm: 1.78\n",
            "step1580 | loss: 3.6116859912872314 | dt: 2287.29ms | tok/sec:  3581.53 | norm: 2.19\n",
            "step1581 | loss: 3.3634121417999268 | dt: 2286.09ms | tok/sec:  3583.42 | norm: 1.51\n",
            "step1582 | loss: 3.3876383304595947 | dt: 2285.01ms | tok/sec:  3585.11 | norm: 1.44\n",
            "step1583 | loss: 3.343799352645874 | dt: 2281.46ms | tok/sec:  3590.68 | norm: 1.45\n",
            "step1584 | loss: 3.654656410217285 | dt: 2285.56ms | tok/sec:  3584.24 | norm: 1.79\n",
            "step1585 | loss: 3.700711727142334 | dt: 2284.50ms | tok/sec:  3585.91 | norm: 2.15\n",
            "step1586 | loss: 3.7819414138793945 | dt: 2290.99ms | tok/sec:  3575.75 | norm: 1.93\n",
            "step1587 | loss: 3.5167360305786133 | dt: 2284.25ms | tok/sec:  3586.30 | norm: 1.96\n",
            "step1588 | loss: 3.5701749324798584 | dt: 2280.13ms | tok/sec:  3592.77 | norm: 1.70\n",
            "step1589 | loss: 3.396127223968506 | dt: 2287.56ms | tok/sec:  3581.11 | norm: 1.80\n",
            "step1590 | loss: 3.4153051376342773 | dt: 2280.89ms | tok/sec:  3591.58 | norm: 1.66\n",
            "step1591 | loss: 3.407261848449707 | dt: 2286.32ms | tok/sec:  3583.05 | norm: 1.81\n",
            "step1592 | loss: 3.2551839351654053 | dt: 2287.74ms | tok/sec:  3580.83 | norm: 1.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)\n",
        "torch.save(model.state_dict(), 'GPT2_TinyShakespeare.pt')"
      ],
      "metadata": {
        "id": "CdmkcxE0qCNG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}